{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "# Spark 시작하기\n",
    "\n",
    "* Last Updated on 20200901TUE1000 20190909MON1100 20181112_20180430_20171203_20160412\n",
    "* 설치\n",
    "    * Pyspark 3.0 2020\n",
    "    * Spark2.2 for Hadoop2.7 (Python 3.6x)\n",
    "    * Spark2.0 for Hadoop2.7 (Python 2.7x)\n",
    "    * Spark1.6 for Hadoop2.6\n",
    "\n",
    "## S.1 학습내용\n",
    "\n",
    "### S.1.1 목표\n",
    "\n",
    "* Spark를 설치할 수 있다.\n",
    "* SparkSession을 생성하여 Spark를 사용할 수 있다.\n",
    "* Spark의 설정을 변경할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### S.1.2 목차\n",
    "\n",
    "* S.2 Why Spark?\n",
    "* S.2.1 빅데이터 처리\n",
    "* S.2.2 Hadoop과 Spark\n",
    "* S.2.3 Spark 프레임워크\n",
    "* S.3 Notebook 설치\n",
    "* S.4 pyspark 설치\n",
    "* S.5 Spark 설치\n",
    "* S.5.1 사전 설치\n",
    "* S.5.2 설치\n",
    "* S.5.3 경로설정\n",
    "* S.6 실행\n",
    "* S.6.1 일괄실행\n",
    "* S.6.2 interactive shell\n",
    "* S.6.3 오류\n",
    "* S.7 ipython shell로 Spark사용하기\n",
    "* S.8 Jupyter Notebook으로 Spark사용하기\n",
    "* 문제 S-1: Pyspark 시작하기\n",
    "* 문제 S-2: Spark 시작하기\n",
    "* 문제 S-3: Spark standalone cluster 구성하기\n",
    "* 문제 S-4: MongoDB를 사용하기 위한 jar를 추가하는 설정 변경하기\n",
    "* 문제 S-5: sqlite를 사용하기 위한 jar를 추가하는 설정 변경하기\n",
    "* S.9 설정\n",
    "* S.9.1 필요한 jar를 추가\n",
    "* S.9.2 log\n",
    "* S.10 spark-sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## S.2 Why Spark?\n",
    "\n",
    "### S.2.1 빅데이터 처리\n",
    "\n",
    "* 빅데이터는 그 명칭에서 알 수 있듯이, **규모가 크다**는 특징이 있다. 과거에는 메가바이트 정도의 크기라고 했다면, 저장용량도 커지면서 그 규모도 그 몇 천배 이상으로 증가했다.\n",
    "* 또한 데이터가 발생하는 **출처가 다양**하다. 종전에는 '숫자'로 된 재무제표, 시계열 데이터 등 정량적 데이터, 이른바 'Hard Data'가 많았다면, 빅데이터의 경우는 그렇지 않다. SNS에 남겨지는 소문, 평판과 같은 'Soft Data'가 유용한 의미를 제공하기도 한다. 또한 동영상, 센서 데이터, 또는 핸드폰에서 발생하는 다양한 디지털 지문 등 매우 다양한 형태로 발생한다. 발생하는 출처가 다양하므로, 그 데이터 형식이 **비구조적**일 수 밖에 없다. \n",
    "* **그 규모가 방대해지고, 여러 저장소에 데이터가 분산**되어 있을 수 있다.\n",
    "* 그러면 당연히 데이터의 처리도 나누어서 수행되어야 한다. 따라서 이러한 분산작업을 제어하는 기능이 필요하게 된다. 대량의 데이터를 여러 컴퓨터에 나누어 처리하고, 집계하는 **맵리듀스 MapReduce**와 같은 알고리즘이 많이 사용되고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 이와 같은 데이터처리는 새로 등장한 유행어인가? 그렇지 않다. 거래를 처리하는 OLTP OnLine Transaction Processing과 구분하여, 분석을 위한 **OLAP OnLine Analytical Processing**이 있다.\n",
    "이런 OLAP 작업으로 데이터 웨어하우스 Data Warehouse, 데이터 마이닝 Data Mining이 있다.\n",
    "또한 데이터웨어하우스를 구성하기 위해 **ETL (Extract, Transfrom and Load)** 작업이 필요하다.\n",
    "**Extract**이란 다양한 입력에서 데이터 추출 (HDFS, files, JSON, DB, ...), **Transform**는 데이터 변환, **Load**는 다양한 형식의 저장 및 예측, 분류, 추천모델 등 지원하는 작업을 말한다.\n",
    "\n",
    "빅데이터는 이런 과거의 작업에 비해, 특히 새로운 유형, 예를 들면 텍스트를 분석하려고 하며, 이런 데이터를 수집하고, 통계 및 머신러닝 기법을 통해 분석하는 것도 매우 중요한 작업의 하나이다. 이런 다양한 출처에서 실시간 발생하는 대량의 데이터에서 의미있는 통찰력이 있는 '집단지성'을 분석해내는 것이 '빅데이터'인 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.2.2 Hadoop과 Spark\n",
    "\n",
    "Hadoop은 데이터가 대량으로 증가하면서, 이를 처리하기 위한 자바 기반의 분산 컴퓨팅 프레임워크로 개발되어 쓰이고 있다.\n",
    "Hadoop은 데이터를 **수집**하는 목적으로 주로 사용된다.\n",
    "* Hadoop 자체의 파일시스템 HDFS (Hadoop Distributed File System)를 제공하고 있고,\n",
    "* HDFS에서 데이터를 읽어 **맵리듀스 MapReduce** 작업으로 데이터를 추출, 가공할 수 있다. MapReduce는 Hadoop 이전에도 이미 여러 언어로 개발되어 사용되고 있었지만, 분산환경에서 많이 쓰이는 오픈소스로 Hadoop의 MapReduce가 많이 사용되고 있다.\n",
    "* 기계학습을 하려면, 데이터를 HDFS로 올려 놓고, 자바 라이브러리인 Mahout 또는 Spark의 Machine Learning 라이브러리를 사용해야 한다.\n",
    "\n",
    "최근에는 Spark의 사용이 늘어나고 있어서 Hadoop과 비교되고 있다.\n",
    "반면 Spark는 수집한 데이터를 **분석**하는 용도로 사용된다.\n",
    "* 자체 파일 시스템이 없고, RDD를 통해 Hadoop의 파일시스템 HDFS를 사용할 수 있다.\n",
    "* Spark는 Machine Learning 라이브러리를 가지고 있다.\n",
    "* Hadoop과 달리 메모리에서 처리하기 때문에 빠르다 (pipeline). 빅데이터를 빠르게 Map Reduce 할 수 있다.\n",
    "\n",
    "구분 | Spark | Hadoop\n",
    "-------|-------|-------\n",
    "사용 목적 | 데이터 분석 | 데이터 수집\n",
    "파일 시스템 | 자체 파일 시스템이 없슴. hdfs, db, csv등을 사용 | hdfs\n",
    "속도 | 파이프라인을 사용하므로 빠름 | 보다 느림"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.2.3 Spark 프레임워크\n",
    "\n",
    "Spark는 2009년 처음 등장하고, 10여년 넘게 버전업을 하면서 널리 이용되고 있다.\n",
    "현재 처음에 관여했던 개발자가 **Databricks**를 설립해서 관리하고 있다.\n",
    "* 2009년 UC Berkeley에서 Matei Zaharia가 박사학위 과정에서 개뱔\n",
    "* 2010년 BSD 라이센스 오픈소스로 전환.\n",
    "* 2013년 Apache 2.0 license로 전환\n",
    "* 2020년 6월 18일 3.0 발표"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 기능\n",
    "\n",
    "Spark는 실시간 대용량 데이터 처리를 위한 클러스터 컴퓨팅 프레임워크로서, API를 사용해서 데이터를 읽고, 변환, 통계처리, 기계학습, 그래프분석을 할 수 있다.\n",
    "**Scala**로 개발되어 jvm에서 실행한다. 그러나 Scala, Java, Python, R 여러 언어를 섞어서 할 수 있는 polyglot 환경을 제공한다.\n",
    "쉘 환경에서 **REPL (Read Eval Print Loop)**이 가능해서 배우기 쉽고 편리하다. Standalone으로 시작할 수 있고, 클러스터 환경에서 실행할 수도 있다.\n",
    "아래 표에서 보듯이 **Spark Core**가 분산작업에 필요한 바탕이 되고, 그 위 **Sql, streaming, mllib, graphx**를 제공한다.\n",
    "    \n",
    "구분 | 구성 | 설명\n",
    "-------|-------|-------\n",
    "Spark engine | Spark Core | 작업배분, 입출력 등 분산작업에 필요한 기능\n",
    "Spark Applicaiton Frameworks | Spark SQL | DataFrames\n",
    "| Spark Streaming | 실시간 처리\n",
    "| MLlib | 머신러닝 (참조 scikit-learn)\n",
    "|GraphX | 그래프 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 아키텍쳐\n",
    "\n",
    "Spark는 Master-Slave 아키텍처에 따라, 중앙에 하나의 Driver가 있고, 여러 Worker가 분산되어 있고 이를 통해서 작업이 실행된다.\n",
    "* Spark Driver: Driver는 자바 프로세스로서 Scala, Java, Python 언어이든 main() 함수가 실행된다. SparkSession을 만들고, 데이터 처리하는 과정의 상태관리도 하고, 데이터 결과 값을 돌려받는다.\n",
    "* Cluster Manager: 클러스터 노드 간 작업일정을 관리하고, CPU나 메모리등 필요한 자원을 배분한다. Cluster Manager의 종류는:\n",
    "    * Standalone\n",
    "    * YARN\n",
    "    * Mesos\n",
    "    * Kubernetes\n",
    "* Worker는 Slave에서 시행, Executor를 가지고 있고, Task를 실행한다.\n",
    "* Executor는 Taks를 실행하고, 그 결과를 Driver에 반환\n",
    "\n",
    "에를 들어, ```spark-submit --master <Spark master URL> --driver-memory 5g --executor-memory 7g --executor-cores 2 testSpark.py```라고 작업이 시작되었다고 하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'iplantuml'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-3db08b9b1ba5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0miplantuml\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'iplantuml'"
     ]
    }
   ],
   "source": [
    "import iplantuml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Cell magic `%%plantuml` not found.\n"
     ]
    }
   ],
   "source": [
    "%%plantuml --jar\n",
    "@startuml\n",
    "node Driver\n",
    "node Cluster_Manager {\n",
    "    node Master\n",
    "}\n",
    "node Cluster_Worker_1 {\n",
    "    node Executor_1_1 {\n",
    "        rectangle Task_1_1_1\n",
    "    }\n",
    "}\n",
    "node Cluster_Worker_2 {\n",
    "    node Executor_2_1 {\n",
    "        rectangle Task_2_1_1\n",
    "    }\n",
    "}\n",
    "node Cluster_Worker_n {\n",
    "    node Executor_n_1 {\n",
    "        rectangle Task_n_1_1\n",
    "    }\n",
    "}\n",
    "Driver --> Cluster_Manager : (1) Driver는 Cluster Manager에게\\nExecutor를 실행하는데 필요한\\n 컴퓨팅 자원을 요청한다.\n",
    "Driver --> Executor_1_1 : (3) Driver는 필요한 Task가 무엇인지 정하고,\\n 작업 계획을 하고,\\n Executor에 Task를 할당\n",
    "Executor_1_1 --> Driver : (4) Executor는 작업을 실행하고, \\n종료하면 그 결과를 Driver에게 반환\n",
    "Cluster_Manager -down-> Cluster_Worker_1\n",
    "Cluster_Manager -down-> Cluster_Worker_2\n",
    "Cluster_Manager -down-> Cluster_Worker_n : (2) Cluster Manager가 Driver를 대신해\\n Executor를 실행한다.\n",
    "Driver --> Driver : (5) 모든 작업이 종료되면, Driver의 main() 함수가 종료, sparkSession.stop()이 호출된다.\\n (6) Spark는 Cluster Manager에게 할당되었던 자원을 회수한다.\n",
    "@enduml "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S.3 Notebook 설치\n",
    "\n",
    "pyspark은 기본으로 제공되는 Python Shell에서 사용하면 불편한다.\n",
    "Jupyter Notebook에서 실행하면 보다 편리하다.\n",
    "\n",
    "### 윈도우, 리눅스, 맥 설치\n",
    "\n",
    "* 윈도우 운영체제에서 쉬운 방법은 Anaconda를 설치하는 방법이다 (http://docs.continuum.io/anaconda/install) 설치하기 전, 자신의 컴퓨터 운영체제와 32비트, 64비트 지원을 확인하고 적합한 Anaconda를 설치하도록 한다. 자신의 시스템의 사양을 모르면 제어판의 시스템으로 가면 확인할 수 있다. Python은 3.x를 지원하는 버전으로 한다. 설치하고 나면, 윈도우에서 Anaconda를 선택하여 실행한다. Anaconda에는 명령창을 제공하는데, 여기서 pip와 같은 conda 명령을 사용할 수 있다.\n",
    "* 리눅스, 맥에서는 Python 3을 설치하고 ```pip3 install notebook``` 하면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anaconda에서 시작 디렉토리를 변경하려면\n",
    "\n",
    "* Anaconda 명령창에서 ```jupyter notebook –generate-config```를 타이핑하고 엔터를 누른다.\n",
    "* 그러면 ```C:\\Users\\username\\.jupyter\\jupyter_notebook_config.py```가 생성된다.\n",
    "* 이 파일을 열고 ```#c.NotebookApp.notebook_dir = ''``` 이 부분을 ```c.NotebookApp.notebook_dir = '/path/to/home/dir/'```로 변경해 주면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 디렉토리 구성\n",
    "\n",
    "* 최상위 디렉토리 'Code'를 먼저 만든다. 문서 디렉토리가 있듯이, 개발자는 소스코드를 'Code'에 저장하는 것이 좋다.\n",
    "* 'Code' 아래 **프로젝트 디렉토리**를 만든다. 학번으로 하는 편이 좋다.\n",
    "* **프로젝트 디렉토리** 아래 'src' 디렉토리를 만든다.\n",
    "    * 'data'는 csv, txt 등 파일 저장\n",
    "    * 'src'는 **소스코드 파일**들을 저장\n",
    "\n",
    "```python\n",
    "C:\\Users\\G305\\Code\\201711111\\ 리눅스는 '/home/user/Code/201711111\n",
    "         |\n",
    "         |\n",
    "         d0_intro.ipynb\n",
    "         |\n",
    "         ---> data\\\n",
    "         |\n",
    "         ---> src\\\n",
    "                |\n",
    "                ---> Hello.py\n",
    "         |\n",
    "         |---> spark-warehouse Spark 실행하면 자동으로 생성\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S.4 pyspark 설치\n",
    "\n",
    "Python 2는 지원이 되지 않게 되었다. 이제 Python 3을 사용한다.\n",
    "\n",
    "PySpark 버전 2.2부터는 Pyspark를 PyPi에서 설치할 수 있다.\n",
    "\n",
    "```python\n",
    "pip install pyspark\n",
    "```\n",
    "\n",
    "pyspark만 설치하게 되면, standalone으로만 사용할 수 있다.\n",
    "완전한 Spark는 다음 장에서 조금 어렵지만, 다음 장에서 해보자.\n",
    "\n",
    "필요한 jar, py4j를 자동으로 같이 설치한다.\n",
    "필요하다면, 아래와 같이 설치해주면 된다.\n",
    "\n",
    "```python\n",
    "pip install py4j\n",
    "``` \n",
    "\n",
    "Python 2와 3이 같이 설치되어 있는 경우에는, 아래에서 보듯이 ```PYSPARK_PYTHON=python3```, ```PYSPARK_DRIVER_PYTHON=python3```이라고 경로 설정을 해주어야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jsl@jsl-smu:~$ export PYSPARK_PYTHON=python3\n",
    "jsl@jsl-smu:~$ pyspark \n",
    "Python 3.6.9 (default, Apr 18 2020, 01:56:04) \n",
    "[GCC 8.4.0] on linux\n",
    "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
    "20/08/26 07:03:13 WARN Utils: Your hostname, jsl-smu resolves to a loopback address: 127.0.1.1; using 117.16.44.45 instead (on interface eth0)\n",
    "20/08/26 07:03:13 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
    "WARNING: An illegal reflective access operation has occurred\n",
    "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/jsl/.local/lib/python3.6/site-packages/pyspark/jars/spark-unsafe_2.12-3.0.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
    "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
    "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
    "WARNING: All illegal access operations will be denied in a future release\n",
    "20/08/26 07:03:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
    "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
    "Setting default log level to \"WARN\".\n",
    "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
    "Welcome to\n",
    "      ____              __\n",
    "     / __/__  ___ _____/ /__\n",
    "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.0.0\n",
    "      /_/\n",
    "\n",
    "Using Python version 3.6.9 (default, Apr 18 2020 01:56:04)\n",
    "SparkSession available as 'spark'.\n",
    ">>> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0.0\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "print (pyspark.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.5 Spark 설치\n",
    "\n",
    "### S.5.1 사전 설치\n",
    "\n",
    "* Spark는 프로그래밍 언어를 다양하게 선택하여 사용할 수 있다.\n",
    "* 사용하는 언어에 따라 다르겠지만 Spark 배포시기에 버전을 맞추어 Java, Scala, Python이 미리 설치되어 있어야 한다.\n",
    "* Pyspark를 사용하려면 Java와 Python을 반드시 설치해 놓아야 한다.\n",
    "\n",
    "구분 | Pyspark를 사용하려면 | 설명\n",
    "----|-----|-----\n",
    "Java | 필수 | Java SE Development Kit을 설치. Spark 3은 Java 11 지원. (Spark 2에서는 Java 7+, 8이 무난)\n",
    "Python | 필수 | Spark 3.0은 Python 2 지원 중지, Python 3 필요 (Spark 2에서는 Python 2.6+/3/4+)\n",
    "Scala | 선택 (Scala사용하는 경우 필수) | Spark 3.0은 Scala 2.12 (Spark 2에서는 Scala 2.11.x)<br>리눅스 ```sudo apt install scala```<br>윈도우 scala-2.xx.x.msi 다운로드, 설치 및 환경설정 https://www.scala-lang.org/download/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.5.2 설치\n",
    "\n",
    "앞에서는 pyspark만을 설치하였다.\n",
    "여기서는 클러스터 방식으로 사용하거나 다른 언어 Java, Scala를 사용하려면\n",
    "\n",
    "#### **방법 1: prebuilt 바이너리를 내려 받아, 압축 풀고 저장**\n",
    "* [Spark 다운로드](https://spark.apache.org/downloads.html)\n",
    "* 하둡 HDFS를 사용할 수 있도록 하둡의 버전을 정해서 설치파일을 내려받고 압축을 풀어준다\n",
    "(Hadoop없는 prebuilt를 설치하면 오류. 아래 참조)\n",
    "* spark 2.0 hadoop2.7\n",
    "```python\n",
    "tar -xvzf spark-2.0.0-bin-hadoop2.7.tgz\n",
    "```\n",
    "\n",
    "#### 방법 2: source code를 github에서 내려받아서 컴파일\n",
    "이 방법은 컴파일을 할 수 있다면, 시도해도 좋지만 오류가 발생하면 이에 대처할 수 있는 능력이 있어야 한다.\n",
    "\n",
    "#### 방법 3: apt로 설치하는 경우\n",
    "\n",
    "Ubuntu 16.04에서 제공되기 시작했다. 그러나 이 방법으로 설치하면 spark 실행에 필요한 shell을 설치하지 않아서, 모든 기능을 사용할 수 없다는 단점이 있다.\n",
    "    \n",
    "```python\n",
    "sudo apt install spark\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.5.3 경로설정\n",
    "\n",
    "* SPARK가 설치된 디렉토리에 맞게 설정한다.\n",
    "\n",
    "구분 | 리눅스 | 윈도우\n",
    "-----|-----|-----\n",
    "**SPARK_HOME** | ```export SPARK_HOME=$HOME/Downloads/spark-1.6.0-bin-hadoop2.6``` | setx SPARK_HOME \"C:\\Users\\jsl\\Downloads\\spark-1.6.0-bin-hadoop2.6\"\n",
    "**JAVA_HOME** | apt 설치한 경우 별도 설정x | setx JAVA_HOME \"C:\\Program Files\\Java\\jdk1.8.0\"\n",
    "**HADOOP_HOME** | 별도 설정x| - 윈도우에서는 Hadoop을 직접 내려받아 설치하거나, **'winutils.exe'**를 설치해야 한다 (https://github.com/steveloughran/winutils/)<br>- %HADOOP_HOME%\\bin을 만들고 그 아래 'winutils.exe'를 복사해 놓거나<br> 또는SPARK_HOME과 동일하게 설정하면 된다. ```setx HADOOP_HOME %SPARK_HOME%```<br>- Hadoop이 사용하는 ```C:/tmp/hive``` 폴더를 생성하고, 명령창에서 **```winutils chmod 777 c:\\tmp\\hive```**해서 권한을 부여함\n",
    "**PATH** | ```export PATH=$SPARK_HOME/bin:$PATH``` | setx PATH \"%PATH%;%SPARK_HOME%\\bin;%JAVA_HOME%\\bin\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* ipython과 ipython notebook을 사용하기 위해서는 추가 경로를 설정한다. 아래와 같이 하면 명령 프롬프트에서 ```$ pyspark```라고 실행하면 notebook이 실행된다.\n",
    "\n",
    "구분 | 리눅스 | 윈도우\n",
    "-----|-----|-----\n",
    "python | ```export PYSPARK_PYTHON=ipython``` | ```sex PYSPARK_PYTHON ~/anaconda/bin/python```\n",
    "ipython 사용 | ```export PYSPARK_DRIVER_PYTHON=ipython``` | ```setx PYSPARK_DRIVER_PYTHON ~/anaconda/bin/ipython```\n",
    "ipython notebook 사용 | ```export PYSPARK_DRIVER_PYTHON_OPTS=notebook``` | ```setx PYSPARK_DRIVER_PYTHON_OPTS notebook```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 경로의 설정이 올바르게 되었는지 확인한다.\n",
    "* java, python 버전을 확인한다. java, javac가 다른 경우 일치 시킨다.\n",
    "\n",
    "리눅스 | 윈도우 \n",
    "-----|-----\n",
    "update-alternatives --config java | 관리자 권한으로 단말을 열어서 \"Run as administrator\"<br>```cd C:\\ProgramData\\Oracle\\Java\\javapath```<br>```mklink java.exe \"c:\\Program Files\\Java\\jdk1.8.0_152\\bin\\java.exe\"```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "java version \"9.0.4\"\r\n",
      "Java(TM) SE Runtime Environment (build 9.0.4+11)\r\n",
      "Java HotSpot(TM) 64-Bit Server VM (build 9.0.4+11, mixed mode)\r\n"
     ]
    }
   ],
   "source": [
    "!java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "javac 9.0.4\r\n"
     ]
    }
   ],
   "source": [
    "!javac -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scala code runner version 2.11.6 -- Copyright 2002-2013, LAMP/EPFL\r\n"
     ]
    }
   ],
   "source": [
    "!scala -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 2.7.12\r\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 윈도우에서는 ```!where python```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/python\r\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## S.6 실행\n",
    "\n",
    "* Spark는 **batch, streaming, iterative, interactive 4가지 방식**으로 실행할 수 있다.\n",
    "\n",
    "### S.6.1 일괄실행\n",
    "\n",
    "* Spark 프로그램을 일괄 실행\n",
    "* Python 프로그램 test.py를 **spark-submit**하는 경우:\n",
    "\n",
    "```python\n",
    "spark-submit test.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.6.2 insteractive shell\n",
    "\n",
    "Spark를 설치하면 Scala, Python Shell을 사용할 수 있다. REPL (the Read-Eval-Print-Loop)이 가능한 대화형 Shell이다.\n",
    "\n",
    "#### Scala\n",
    "Scala를 실행하려면, spark-shell이라고 타이핑하고 <Enter>를 누른다.\n",
    "\n",
    "```python\n",
    "spark-3.0.0-bin-hadoop2.7/bin/bin$ spark-shell\n",
    "scala>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### python\n",
    "\n",
    "spark설치 디렉토리 아래 bin으로 가서 pyspark를 실행하면, spark, sc는 기본 제공된다.\n",
    "(윈도우는 **관리자권한'Run as administator'**으로 열어야 한다. 아니면 오류.)\n",
    "\n",
    "```python\n",
    "spark-2.0.0-bin-hadoop2.7/bin$ ./pyspark \n",
    "Python 2.7.12 (default, Nov 19 2016, 06:48:10) \n",
    "[GCC 5.4.0 20160609] on linux2\n",
    "\n",
    "Welcome to\n",
    "      ____              __\n",
    "     / __/__  ___ _____/ /__\n",
    "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.0.0\n",
    "      /_/\n",
    "\n",
    "Using Python version 2.7.12 (default, Nov 19 2016 06:48:10)\n",
    "SparkSession available as 'spark'.\n",
    ">>> sc.version\n",
    "u'2.0.0'\n",
    ">>> text=sc.textFile(\"derby.log\")\n",
    ">>>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S.6.3 오류\n",
    "\n",
    "#### **Hadoop 없는 버전**을 내려 받아 설치하면 오류 발생\n",
    "이런 오류는 prebuilt with hadoop을 설치하면 된다. 예를 들어, Hadoop 없는 'spark-x.x.x'을 설치하고, 'spark-shell'를 실행하면, 오류가 발생하게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 'Failed to find Spark assembly spark' 오류\n",
    "sbt를 다음과 같이 설치한다.\n",
    "\n",
    "```python\n",
    "download tar from http://www.scala-sbt.org/\n",
    "tar -xvf sbt.tar\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### java.net.BindException: Cannot assign requested address: Service **'sparkDriver' failed** after 16 retries\n",
    "host 주소가 할당되지 않은 경우 오류가 발생한다.\n",
    "\n",
    "```python\n",
    "$ sudo hostname -s 127.0.0.1\n",
    "```\n",
    "\n",
    "spark-env.sh를 편집해서 Local IP를 설정한다.\n",
    "```python\n",
    "SPARK_LOCAL_IP=127.0.0.1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### SparkContext: Error initializing SparkContext\n",
    "* /etc/hostname이 지금 설정과 다른 경우의 오류\n",
    "* 변경하려면\n",
    "\n",
    "```python\n",
    "$ scutil --set HostName jsl.com\n",
    "```\n",
    "\n",
    "* 확인하려면   \n",
    "```python\n",
    "$ hostname\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 'cannot import name accumulators'\n",
    "* PYTHONPATH가 올바르게 설정되지 않아서 발생하는 오류\n",
    "* sys.path()를 사용해 경로를 추가한다.\n",
    "\n",
    "#### 사용하는 포트를 해제하지 않고 실행하는 경우 java.net.BindException: Address already in use\n",
    "* 포트를 해제한다.\n",
    "```python\n",
    "ps -aux | grep spark\n",
    "kill -9 3370\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.7 ipython shell로 Spark사용하기\n",
    "\n",
    "pyspark는 Python에서 지원하는 shell을 사용한다. 보다 사용하기 편리한 ipython shell로 변경하여 실행하려면:\n",
    "\n",
    "### **방법 1: 단순한 방식**\n",
    "\n",
    "'PYSPARK_DRIVER_PYTHON=ipython'라고 설정하고 명령창에서 아래와 같이 pyspark를 실행한다.\n",
    "(윈도우에서는 명령창을 관리자권한으로 열어 pyspark를 실행한다. 명령창에서 설정을 한 후에는 닫은 후 다시 열어야 한다.)\n",
    "\n",
    "```python\n",
    "> export PYSPARK_DRIVER_PYTHON=ipython # 경로를 설정\n",
    "> ./bin/pyspark                        # 실행\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 방법2: profile로 환경 설정하기\n",
    "* ipython profile 생성하고, ipython shell 실행\n",
    "\n",
    "```python\n",
    "> ipython console --profile=pyspark # 아래 ipython profile 생성 후 실행\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* ipython profile 생성\n",
    "\n",
    "```python\n",
    "> ipython profile create pyspark # ipython profile 생성\n",
    "> vim ~/.ipython/profile_pyspark/startup/00-pyspark-setup.py # profile startup 파일 생성\n",
    "import os\n",
    "import sys\n",
    "if \"SPARK_HOME\" not in os.environ:\n",
    "    os.environ[\"SPARK_HOME\"]=os.path.join(os.environ['HOME'],'Downloads','spark-2.0.0-bin-hadoop2.7')\n",
    "os.environ[\"PYLIB\"]=os.path.join(os.environ[\"SPARK_HOME\"],'python','lib')\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'py4j-0.10.1-src.zip'))\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'pyspark.zip'))\n",
    "#sys.path.insert(0,os.path.join(SPARK_HOME,\"python\")\n",
    "execfile(os.path.join(os.environ[\"SPARK_HOME\"],'python/pyspark/shell.py'))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## S.8 Jupyter Notebook으로 Spark사용하기\n",
    "\n",
    "### **방법1: spark/bin/pyspark를 사용해서 Jupyter Notebook.**\n",
    "* 명령창에서 다음 명령어, 노트북 뜨고 나면 spark, sc는 기본 지원 (윈도우는 관리자권한으로 명령창을 열어야 한다)\n",
    "* Spark 2.0이후 IPYTHON_OPTS 명령어 지원 하지 않음\n",
    "\n",
    "```python\n",
    "> cd ~/Downloads/spark-2.0.0-bin-hadoop2.7/bin/\n",
    "> export PYSPARK_DRIVER_PYTHON=ipython\n",
    "> ./pyspark # ipython이 뜸\n",
    "> export PYSPARK_DRIVER_PYTHON_OPTS='notebook'\n",
    "> ./pyspark # 노트북이 뜸\n",
    "```\n",
    "\n",
    "* 다음 명령어는 오류 (IPTYHON_OPTS를 사용했던 경우) -> 7077 can not be reached\n",
    "```python\n",
    "MASTER=\"spark://127.0.0.1:7077\" SPARK_EXECUTOR_MEMORY=\"6G\" IPYTHON_OPTS=\"notebook\" ~/Downloads/spark-1.6.0-bin-hadoop2.6/bin/pyspark\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### **방법 2: IPython Notebook을 실행하고, 설정을 변경한다. 강의자료는 이 기준으로 만들어져 있다.**\n",
    "* PyPi에서 pyspark를 설치해서 사용하는 경우이다.\n",
    "* SPARK_HOME 및 PYTHONPATH를 설정한다.\n",
    "* Spark 2.0+를 사용하는 경우, SparkSession을 생성한다.\n",
    "\n",
    "설정 항목 | 설명\n",
    "----------|----------\n",
    "SPARK_HOME | Spark를 설치한 자신의 경로로 수정한다.\n",
    "PYTHONPATH | sys.path.insert()를 사용하여 PYTHONPATH를 수정한다. pyspark.zip, py4j-0.10.1-src.zip를 추가\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 방법 3: findspark 라이브러리를 설치\n",
    "\n",
    "* 설치 디렉토리를 설정하면, findspark이 필요한 PYTHONPATH를 설정해 준다.\n",
    "\n",
    "```python\n",
    "pip install findspark\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "home=os.path.expanduser(\"~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(os.path.join(home,\"Downloads\",\"spark-2.0.0-bin-hadoop2.7\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jsl/Downloads/spark-2.0.0-bin-hadoop2.7'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "myConf=pyspark.SparkConf()\n",
    "spark = pyspark.sql.SparkSession.builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"myApp\")\\\n",
    "    .config(conf=myConf)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName=\"myAppName\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "print spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SparkSession을 중지할 경우 stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 방법 4: ipython notebook kernel을 설정 (mac ok 20160902 fri)\n",
    "    * 'kernel.json' 작성 후, ipython notebook을 실행하면 kernel에 PySpark가 추가된다.\n",
    "```python        \n",
    "> mkdir -p ~/.ipython/kernels/pyspark\n",
    "> vim ~/.ipython/kernels/pyspark/kernel.json\n",
    "```\n",
    "\n",
    "    * kernel.json은 아래 내용을 수정해서 저장\n",
    "```python\n",
    "{\n",
    "    \"display_name\": \"pySpark (Spark 1.6.0) with graphFrames\",\n",
    "    \"language\": \"python\",\n",
    "    \"argv\": [\n",
    "        \"/usr/bin/python2.7\",\n",
    "        \"-m\",\n",
    "        \"ipykernel\",\n",
    "        \"-f\",\n",
    "        \"{connection_file}\"\n",
    "    ],  \n",
    "    \"env\": {\n",
    "        \"SPARK_HOME\": \"/Users/media/Downloads/spark-1.6.0-bin-hadoop2.6\",\n",
    "        \"PYTHONPATH\": \"/Users/media/Downloads/spark-1.6.0-bin-hadoop2.6/python:/Users/media/Downloads/spark-1.6.0-bin-hadoop2.6/python/lib/py4j-0.9-src.zip\",\n",
    "        \"PYTHONSTARTUP\": \"/Users/media/Downloads/spark-1.6.0-bin-hadoop2.6/python/pyspark/shell.py\",\n",
    "        \"PYSPARK_SUBMIT_ARGS\": \"--packages graphframes:graphframes:0.1.0-spark1.6 --master spark://127.0.0.1:7077 pyspark-shell\",\n",
    "        \"SPARK_DRIVER_MEMORY\":\"10G\"\n",
    "     }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제 S-1: Pyspark 시작하기\n",
    "\n",
    "### (1) 명령창에서 pyspark 실행\n",
    "\n",
    "* 명령창에서 ```pyspark```를 타이핑하고 엔터를 하면 ```Spark```가 실행하면서 메시지가 출력되고 Python Shell 프롬프트 ```>>>```가 뜬다.\n",
    "출력되는 메시지에 Python 버전 3.x와 Spark 버전 3.0.0이 올바르게 출력이 되는지 확인한다.\n",
    "* 프롬프트 ```>>>```에서 ```spark.version```을 출력한다.\n",
    "\n",
    "아래와 같이 출력된 화면을 캡쳐해서 제출\n",
    "\n",
    "```python\n",
    "> pyspark<Enter>\n",
    "\n",
    "      ____              __\n",
    "     / __/__  ___ _____/ /__\n",
    "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.0.0\n",
    "      /_/\n",
    "\n",
    "Using Python version 3.6.9 (default, Apr 18 2020 01:56:04)\n",
    "SparkSession available as 'spark'.Python \n",
    ">>> spark.version\n",
    "'3.0.0'\n",
    "```\n",
    "\n",
    "### (2) 노트북에서 pyspark 실행\n",
    "\n",
    "주피터 노트북을 실행한 후, 셀에서 아래 명령어를 실행하여 spark를 생성하고, 그 버전을 출력한 화면캡쳐를 제출하세요.\n",
    "\n",
    "```python\n",
    "import pyspark\n",
    "myConf=pyspark.SparkConf()\n",
    "spark = pyspark.sql.SparkSession.builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"myApp\")\\\n",
    "    .config(conf=myConf)\\\n",
    "    .getOrCreate()\n",
    "```\n",
    "\n",
    "```python\n",
    "spark.version\n",
    "'3.0.0'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 문제 S-2: Spark 시작하기\n",
    "\n",
    "Spark prebuilt를 다운로드해서 설치했다면, pyspark를 설치한 것과는 실행하는 방법이 다르다.\n",
    "아래와 같이, 환경변수 SPARK_HOME 설치 경로 및 PYTHONPATH에 추가하는 경로는 자신의 것으로 수정해야 한다.\n",
    "\n",
    "* 필요하지 않은 라이브러리의 경로를 제거하려면:\n",
    "```python\n",
    "sys.path.remove('/home/jsl/Downloads/spark-1.6.0-bin-hadoop2.6/python/lib/py4j-0.9-src.zip')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys \n",
    "#os.environ[\"SPARK_HOME\"]=os.path.join(os.environ['HOME'],'Downloads','spark-2.0.0-bin-hadoop2.7')\n",
    "os.environ[\"SPARK_HOME\"]=os.path.join(os.environ['HOME'],'Downloads','spark-3.0.0-bin-hadoop2.7')\n",
    "os.environ[\"PYLIB\"]=os.path.join(os.environ[\"SPARK_HOME\"],'python','lib')\n",
    "#sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'py4j-0.10.1-src.zip'))\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'py4j-0.10.9-src.zip'))\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'pyspark.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jsl/Downloads/spark-3.0.0-bin-hadoop2.7/python/lib/pyspark.zip\n",
      "/home/jsl/Downloads/spark-3.0.0-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip\n",
      "/usr/lib/python36.zip\n",
      "/tmp/spark-208d631f-e7b2-420c-bb5b-ac5d9d4f8e1e/userFiles-78992114-2542-4185-b3c3-ac77b4dacc12\n",
      "/usr/lib/python3.6\n",
      "/usr/lib/python3.6/lib-dynload\n",
      "\n",
      "/home/jsl/.local/lib/python3.6/site-packages\n",
      "/usr/local/lib/python3.6/dist-packages\n",
      "/usr/lib/python3/dist-packages\n",
      "/home/jsl/.local/lib/python3.6/site-packages/IPython/extensions\n",
      "/home/jsl/.ipython\n"
     ]
    }
   ],
   "source": [
    "for i in sys.path:\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 참고로 Spark 1.6은 다음과 같이 **SparkContext**를 생성해서 사용한다. Spark 2.0은 **SparkSession**으로 통합되었다.\n",
    "\n",
    "```python\n",
    "import pyspark\n",
    "conf=pyspark.SparkConf()\n",
    "conf = pyspark.SparkConf().setAppName(\"myAppName\")\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "myConf=pyspark.SparkConf()\n",
    "spark = pyspark.sql.SparkSession.builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"myApp\")\\\n",
    "    .config(conf=myConf)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0.0'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 문제 S-3: Spark standalone cluster 구성하기\n",
    "\n",
    "클러스터를 구성하지 않으면, 단독형standalone으로 실행한다. (curl로 7077, 8080확인해도 없음 -> NO cluster!)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 클러스터 환경 구성\n",
    "* Client: spark shell, pyspark shell\n",
    "* Cluster\n",
    "    * Cluster 1: 마스터\n",
    "        * Spark master / Spark worker\n",
    "        * HDFS namenode / datanode\n",
    "    * Cluster n: 작업노드\n",
    "        * Spark worker\n",
    "        * HDFS datanode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Spark-Standalone의 master-slave 실행\n",
    "\n",
    "#### 단계1 JAVA_HOME을 설정\n",
    "* JAVA_HOME을 설정한다.\n",
    "    * automatically set symlink to java binary /usr/bin/java\n",
    "    * JAVA_HOME을 설정하려면 /etc/environment에 하는 것이 좋다.\n",
    "```python\n",
    "> echo $JAVA_HOME\n",
    "> update-alternatives --config java\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 단계2: master 실행\n",
    "* SPARK_HOME은 /home/jsl/Downloads/spark-1.6.0-bin-hadoop2.6/\n",
    "* $SPARK_HOME/conf/spark-env.sh에 master ip설정\n",
    "* spark://IPADRESS_OF_YOUR_MASTER_SYSTEM:7077\n",
    "* 기본 port는 7077 (web UI는 localhost:8080)\n",
    "\n",
    "```python\n",
    "SPARK_MASTER_IP=\n",
    "```\n",
    "\n",
    "* 실행\n",
    "\n",
    "```python\n",
    "> sh $SPARK_HOME/sbin/start-master.sh\n",
    "```\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 단계3: slave 실행 (worker라고 함)\n",
    "\n",
    "```python\n",
    "> sh $SPARK_HOME/sbin//start-slave.sh spark://IPADRESS_OF_YOUR_MASTER_SYSTEM:7077\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 쉘 명령어\n",
    "\n",
    "sbin디렉토리의 shell | 설명\n",
    "----------|----------\n",
    "start-master.sh, stop-master.sh | 마스터를 시작 (종료)\n",
    "start-slaves.sh, stop-slaves.sh | 각 노드의 슬레이브를 시작 (종료)\n",
    "start-all.sh, stop-all.sh | 마스터, 슬레이브를 모두 시작 (종료)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.9 설정\n",
    "\n",
    "Spark 설정은:\n",
    "* 프로그램 내부에서 SparkConf()를 사용해서 하거나,\n",
    "* 프로그램 외부에서도 할 수 있다. conf/ 디렉토리에 주어진 spark-defaults.conf, spark-env.sh, log4j.properties 등 설정파일의 환경변수를 변경해서 할 수 있다\n",
    "    * conf/ 아닌 디렉토리를 사용하려면 'SPARK_CONF_DIR'를 수정하면 된다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.9.1 필요한 jar를 추가\n",
    "\n",
    "Scala, Java에서 하는 것 보다 Python에서 jar를 추가하는 것은 쉽지 않다.\n",
    "또한 SparkContext를 생성하고 난 후, 동적으로 jar를 추가할 수 없다.\n",
    "\n",
    "\n",
    "#### S.9.1.1 설정 파일\n",
    "\n",
    "* 단계 1: Spark Package에서 jar url 검색 https://spark-packages.org/\n",
    "    * Spark Package의 명칭은 maven에서 명명하는 방식에 따라, 다음을 콜론(:)으로 합쳐서 사용한다.\n",
    "\n",
    "구분 | 설명\n",
    "-----|-----\n",
    "groupId | 패키지 명칭.\n",
    "artifactId | jar 명칭. 확장자는 생략한다.\n",
    "version | 버전 정보, x.x.x\n",
    "\n",
    "```python\n",
    "org.mongodb.spark:mongo-spark-connector_2.10:2.0.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 단계 2: 'spark-default.conf' 설정 파일의 수정\n",
    "\n",
    "    * ```--jars```는 컴마로 구분된 jar파일을 적어주고, driver와 executor의 classpath에 포함된다.\n",
    "    * ```--packages```는 컴마로 구분된 패키지를 적어주고, 로컬이나 central maven repo에서 찾게된다.\n",
    "\n",
    "구분 | 설명\n",
    "----------|----------\n",
    "spark 패키지 | spark.jars.package의 property를 수정한다. 여러 packages는 컴마로 분리\n",
    "jar | driver, executor 설정. 와일드카드 '*'를 사용할 수 있다.<br>spark.driver.extraClassPath /path/to/my.jar<br>spark.executor.extraClassPath /path/to/my.jar\n",
    "\n",
    "* 단계 3: 실행\n",
    "    * 설정을 해 놓으면 'pyspark', 'submit-spark' 모두 그 설정을 읽어 실행한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### S.9.1.2 다른 방법\n",
    "\n",
    "* jar를 내려 받아서 shell에서 실행하는 방법도 있다.\n",
    "    * https://spark-packages.org/에 가서 Spark 패키지 해당하는 jar를 내려 받고,\n",
    "    * 디렉토리를 만든 후 복사하고, symlink를 만들어 이름을 줄일 수 있다.\n",
    "```python\n",
    "cd ~/Downloads/spark-1.6.0-bin-hadoop2.6/lib\n",
    "ln -s graphframes-0.1.0-spark1.6.jar graphframes.jar\n",
    "```\n",
    "\n",
    "* [ok] pyspark --jars\n",
    "\n",
    "```python\n",
    "$ bin/pyspark --py-files lib/graphframes.jar --jars lib/graphframes.jar \n",
    "Welcome to\n",
    "      ____              __\n",
    "     / __/__  ___ _____/ /__\n",
    "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "   /__ / .__/\\_,_/_/ /_/\\_\\   version 1.6.0\n",
    "      /_/\n",
    "\n",
    "Using Python version 2.7.12 (default, Jul  1 2016 15:12:24)\n",
    "SparkContext available as sc, HiveContext available as sqlContext.\n",
    ">>> from graphframes import *\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* [ok] spark-submit --packages\n",
    "```python\n",
    "./bin/spark-submit src/ds_spark_dataframe.py \\\n",
    "   --packages graphframes:graphframes:0.1.0-spark1.6\n",
    "```\n",
    "\n",
    "* [x] addJar()는 사용할 수 없다.\n",
    "```python\n",
    "SparkContext.addJar(...) method\n",
    "SparkContext.addFile(...) method\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* [nok] os.environ - submit_args\n",
    "    * IPyton Notebook에서 class-not-found errors. python kernel이 일단 실행되고 나면, 동적으로 추가하는 것이 가능하지 않다. (subprocess.Popen은 env값을 가져올 수 없다?)\n",
    "```python\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages graphframes:graphframes:0.2.0-spark2.0-s_2.11'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 문제 S-3: MongoDB를 사용하기 위한 jar를 추가하는 설정 변경하기\n",
    "\n",
    "* 참고 MongoDB의 Spark 연결 설명 https://docs.mongodb.com/spark-connector/\n",
    "* 우선, MongoDB가 설치되어 서버가 실행되어 있어야 한다. 올바른 jar 버전을 사용하기 위해 MongoDB의 버전을 확인한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "db version v2.6.10\r\n",
      "2017-04-15T06:05:02.080+0900 git version: nogitversion\r\n",
      "2017-04-15T06:05:02.080+0900 OpenSSL version: OpenSSL 1.0.2g  1 Mar 2016\r\n"
     ]
    }
   ],
   "source": [
    "!mongod --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 단계 1: Spark Package 설치 확인 https://spark-packages.org/\n",
    "    * mongo-spark를 검색하여, 자신의 Spark Version, Scala Version을 확인해서 맞는 jar를 선택한다.\n",
    "        * mongo-spark-connector_2.10을 사용하려면 Scala 2.10.x이 필요하다.\n",
    "        * scala 버전이 2.11이라 mongo-spark-connector_2.11을 넣어야 하지만, 2.10으로 ok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 단계 2: 설정파일 conf/spark-defaults.conf 수정\n",
    "    * MongoDB<3.2인 경우 spark.mongodb.input.partitioner가 필요하다.\n",
    "```python\n",
    "$vim conf/spark-defaults.conf \n",
    "spark.jars.packages=org.mongodb.spark:mongo-spark-connector_2.10:1.1.0\n",
    "spark.mongodb.input.partitioner=MongoPaginateBySizePartitioner\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 단계 3: pyspark에서 SparkConf \n",
    "```python\n",
    "spark = pyspark.sql.SparkSession.builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"myApp\")\\\n",
    "    .config(\"spark.mongodb.input.uri\", \"mongodb://127.0.0.1/myDB.ds_spark_df_mongo\") \\\n",
    "    .config(\"spark.mongodb.output.uri\", \"mongodb://127.0.0.1/myDB.ds_spark_df_mongo\") \\\n",
    "    .getOrCreate()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 단계 4: 실행\n",
    "    * pyspark로 사용할 경우\n",
    "\n",
    "```python\n",
    "./bin/pyspark --conf \"spark.mongodb.input.uri=mongodb://127.0.0.1/test.myCollection?readPreference=primaryPreferred\" \\\n",
    "              --conf \"spark.mongodb.output.uri=mongodb://127.0.0.1/test.myCollection\" \\\n",
    "              --packages org.mongodb.spark:mongo-spark-connector_2.10:1.1.0\n",
    "```\n",
    "\n",
    "* 오류 DefaultMongoPartitioner: MongoDB version < 3.2 detected. 설정에 추가\n",
    "```python\n",
    "spark.mongodb.input.partitioner=MongoPaginateBySizePartitioner\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graphframes:graphframes:0.4.0-spark2.0-s_2.11,org.mongodb.spark:mongo-spark-connector_2.10:2.0.0,com.databricks:spark-csv_2.11:1.5.0\n"
     ]
    }
   ],
   "source": [
    "print spark.conf.get('spark.jars.packages')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 문제 S-4: sqlite를 사용하기 위한 jar를 추가하는 설정 변경하기\n",
    "\n",
    "* sqlite와 같이 Spark 패키지가 없는 경우, jar를 다운로드하고 'spark.driver.extraClassPath'에 추가한다.\n",
    "\n",
    "* 단계 1: jar 다운로드\n",
    "    * 따라서 jar를 다운로드 받아서, 추가한다. \n",
    "* 단계 2: 설정파일 conf/spark-defaults.conf 수정\n",
    "    * 여러 jar를 추가할 경우, 리눅스는 ':' 윈도우 ';'로 분리하여 추가한다.\n",
    "    * 이전 버전과의 호환성에 필요한 경우 'spark.executor.extraClassPath'도 추가할 수 있다.\n",
    "\n",
    "```python\n",
    "$vim conf/spark-defaults.conf \n",
    "spark.driver.extraClassPath /home/jsl/Code/git/bb/jsl/pyds/lib/sqlite-jdbc-3.14.2.jar\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jsl/Code/git/bb/jsl/pyds/lib/sqlite-jdbc-3.14.2.jar\n"
     ]
    }
   ],
   "source": [
    "print spark.conf.get('spark.driver.extraClassPath')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### S.9.2 log\n",
    "\n",
    "* log4j.properties를 사용해서 변경한다.\n",
    "\n",
    "```python\n",
    "> cp conf/log4j.properties.template conf/log4j.properties\n",
    "> vim conf/log4j.properties\n",
    "log4j.rootCategory=ERROR, console\n",
    "```\n",
    "\n",
    "* log level\n",
    "    * ALL, DEBUG, ERROR, FATAL, INFO, OFF, TRACE, WARN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.10 spark-sklearn\n",
    "\n",
    "sklearn은 데이터분석에 많이 사용되는 패키지이다.\n",
    "Spark에서 sklearn 패키지를 그대로 사용한다면 보다 많은 기계학습 라이브러리를 사용할 수 있게 한다.\n",
    "\n",
    "* [spark-sklearn](https://github.com/databricks/spark-sklearn)\n",
    "```python\n",
    "pip install spark-sklearn\n",
    "```\n",
    "\n",
    "* spark-shell\n",
    "```python\n",
    "$SPARK_HOME/bin/spark-shell --packages databricks:spark-sklearn:0.2.0\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
