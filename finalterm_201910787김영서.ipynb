{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 휴먼지능정보공학과 201910787 김영서"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 문제 1:서울시 지하철 호선별 역별 유/무임 승하차 인원 분석\n",
    "'서울시 지하철 호선별 역별 유/무임 승하차 인원 정보'를 csv파일로 내려받아서 분석하세요\n",
    "\n",
    "(https://data.seoul.go.kr/dataList/OA-12251/S/1/datasetView.do?tab=S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "home=os.path.expanduser(\"~\")\n",
    "os.environ[\"PYSPARK_PYTHON\"]=\"C:\\\\Users\\\\zaqxs\\\\Anaconda2\\\\envs\\\\py35\\\\python.exe\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"]=\"C:\\\\Users\\\\zaqxs\\\\Anaconda2\\\\envs\\\\py35\\\\python.exe\"\n",
    "os.environ[\"PYLIB\"]=os.path.join(os.environ[\"SPARK_HOME\"],'python','lib')\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'py4j-0.10.9-src.zip'))\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'pyspark.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "myConf=pyspark.SparkConf()\n",
    "spark = pyspark.sql.SparkSession\\\n",
    "    .builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"myApp\")\\\n",
    "    .config(conf=myConf)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문제 1-1의 답\n",
    "'서울시 지하철 호선별 역별 유_무임 승하차 인원 정보.csv'를 데이터프레임으로 읽고, 스키마를 출력<br>\n",
    "(한글이 깨질 경우에는 엑셀에서 UTF로 저장하거나, 직접 csv파일을 수정하거나 우회적인 방법을 통해서 해결, 약간의 감점)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "subway = spark.read.format('com.databricks.spark.csv')\\\n",
    "    .options(header='true', inferschema='true').load('data/서울시 지하철 호선별 역별 유_무임 승하차 인원 정보.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- 사용월: integer (nullable = true)\n",
      " |-- 호선명: string (nullable = true)\n",
      " |-- 지하철역: string (nullable = true)\n",
      " |-- 유임승차인원: integer (nullable = true)\n",
      " |-- 무임승차인원: integer (nullable = true)\n",
      " |-- 유임하차인원: integer (nullable = true)\n",
      " |-- 무임하차인원: integer (nullable = true)\n",
      " |-- 작업일자: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subway.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문제 1-2의 답\n",
    "건수를 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55171"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subway.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문제 1-3의 답\n",
    "'호선명'별 '유임승차인원'의 순위를 매기고, 그 결과를 30줄 출력 (마지막 'rank' 컬럼)\n",
    "\n",
    "```\n",
    "+------+------+--------+------------+------------+------------+------------+--------+----+\n",
    "|사용월|호선명|지하철역|유임승차인원|무임승차인원|유임하차인원|무임하차인원|작업일자|rank|\n",
    "+------+------+--------+------------+------------+------------+------------+--------+----+\n",
    "|201605|일산선|    화정|      515925|      139163|      535460|      142410|20160608|   1|\n",
    "|201512|일산선|    화정|      508044|      136734|      529106|      139931|20160108|   2|\n",
    "|201603|일산선|    화정|      502144|      140596|      526030|      144088|20160408|   3|\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "should sort by at least one column",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-f43e254c1360>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0msubway_3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubway\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"유임승차인원\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"호선명\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36msort\u001b[1;34m(self, *cols, **kwargs)\u001b[0m\n\u001b[0;32m   1403\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Bob'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Alice'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1404\u001b[0m         \"\"\"\n\u001b[1;32m-> 1405\u001b[1;33m         \u001b[0mjdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sort_cols\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1406\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1407\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36m_sort_cols\u001b[1;34m(self, cols, kwargs)\u001b[0m\n\u001b[0;32m   1429\u001b[0m         \"\"\"\n\u001b[0;32m   1430\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcols\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1431\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"should sort by at least one column\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1432\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1433\u001b[0m             \u001b[0mcols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcols\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: should sort by at least one column"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "subway_3 = subway.sort_values(by=\"유임승차인원\", ascending=False).groupby(\"호선명\").head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문제 1-4의 답\n",
    "*  '호선명'별 '유임승차인원'이 순위가 가장 높은 경우만 30건 출력 (마지막 'rank' 컬럼이 모두 1)\n",
    "\n",
    "```\n",
    "\n",
    "+------+--------------+--------------------+------------+------------+------------+------------+--------+----+\n",
    "\n",
    "|사용월|        호선명|            지하철역|유임승차인원|무임승차인원|유임하차인원|무임하차인원|작업일자|rank|\n",
    "\n",
    "+------+--------------+--------------------+------------+------------+------------+------------+--------+----+\n",
    "\n",
    "|201605|        일산선|                화정|      515925|      139163|      535460|      142410|20160608|   1|\n",
    "\n",
    "|201905|        장항선|                아산|      109243|       12901|       93737|       11808|20190603|   1|\n",
    "\n",
    "|201512|        경부선|              영등포|     1439049|      280623|     1545631|      280969|20160108|   1|\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문제 1-5의 답\n",
    "*  '호선명'별 '유임승차인원'의 zscore를 계산하고 출력. 윈도우 함수를 사용하여야 한다.\n",
    "\n",
    "```\n",
    "\n",
    "+------+------------+------------------+------------------+--------------------+\n",
    "\n",
    "|호선명|유임승차인원|        meanByLine|          sdByLine|              zscore|\n",
    "\n",
    "+------+------------+------------------+------------------+--------------------+\n",
    "\n",
    "|일산선|      237714|231630.21716649432|127730.48519577553| 0.04762984203951724|\n",
    "\n",
    "|일산선|      363144|231630.21716649432|127730.48519577553|  1.0296193788971475|\n",
    "\n",
    "|일산선|      294566|231630.21716649432|127730.48519577553|  0.4927232738295993|\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 문제 2:회귀분석\n",
    "sklearn의 make_regression함수를 다음과 같이 사용하여 데이터를 생성하고, 문제를 푸세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 생성([식-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "X, y, coef = make_regression(n_samples = 200,\n",
    "                            n_features = 4,\n",
    "                            n_informative = 3,\n",
    "                            n_targets = 1,\n",
    "                            noise = 0.0,\n",
    "                            coef = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문제 2-1의 답\n",
    "gradient 방법으로 회귀식을 계산하고, 계수를 출력.<br>\n",
    "또한 [식-1]의 'coef'를 출력하고, gradient방법에서 도출된 계수와 일치하는지 비교하세요.<br>\n",
    "주의: 반복이 많으면 감점 (예를 들어, 1000 이상이면 많다고 간주할 것임)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "alpha = 0.02\n",
    "theta=np.array(np.ones([X.shape[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(x,y,theta,alpha,numIterations):\n",
    "    for i in range(numIterations):\n",
    "        #i+=1\n",
    "        h=np.dot(x,theta)\n",
    "        error=h-y\n",
    "        cost=np.sum((h-y)**2)/len(x) #full MSE\n",
    "        gradient=np.dot(x.T,error)*2/len(x) #w0,w1\n",
    "        theta-=alpha*gradient\n",
    "        if i%100==0:\n",
    "            print(\"Iteration {0} | theta{1} Cost {2:.5f}\".format(i,theta,cost))\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**계수 출력**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 | theta[2.50010749 3.36146554 1.1692096  3.72523866] Cost 9065.64546\n",
      "Iteration 100 | theta[42.36685972 51.65031675  0.35911404 65.5072738 ] Cost 2.82336\n",
      "Iteration 200 | theta[4.36741009e+01 5.19755354e+01 1.48224623e-02 6.64221573e+01] Cost 0.00271\n",
      "Iteration 300 | theta[4.37235592e+01 5.19723322e+01 5.68656642e-04 6.64373979e+01] Cost 0.00000\n",
      "Iteration 400 | theta[4.37254831e+01 5.19720505e+01 2.19257772e-05 6.64377042e+01] Cost 0.00000\n",
      "Iteration 500 | theta[4.37255582e+01 5.19720380e+01 8.50744408e-07 6.64377120e+01] Cost 0.00000\n",
      "Iteration 600 | theta[4.37255611e+01 5.19720375e+01 3.31046710e-08 6.64377122e+01] Cost 0.00000\n",
      "Iteration 700 | theta[4.37255612e+01 5.19720375e+01 1.28960022e-09 6.64377122e+01] Cost 0.00000\n"
     ]
    }
   ],
   "source": [
    "w4,w3,w2,w1=gradientDescent(X, y, theta, alpha, 800) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**비교**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 결과:      43.72556121  51.97203751  0.00000000  66.43771223\n",
      "[식-1의] coef:  43.72556121  51.97203751  0.00000000  66.43771223\n"
     ]
    }
   ],
   "source": [
    "print(\"예측 결과:      %.8f  %.8f  %.8f  %.8f\"%(w4,w3,w2,w1))\n",
    "print(\"[식-1의] coef:  %.8f  %.8f  %.8f  %.8f\"%(coef[0],coef[1],coef[2],coef[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문제 2-2의 답\n",
    "스파크 데이터프레임을 생성 (이후 계속 스파크로 푸세요)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pdDf=pd.DataFrame(X)\n",
    "pdDf['y']=y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "Df=spark.createDataFrame(pdDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "Df = Df.withColumnRenamed(\"0\", \"x0\")\n",
    "Df = Df.withColumnRenamed(\"1\", \"x1\")\n",
    "Df = Df.withColumnRenamed(\"2\", \"x2\")\n",
    "Df = Df.withColumnRenamed(\"3\", \"x3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+--------------------+--------------------+-------------------+\n",
      "|                  x0|                 x1|                  x2|                  x3|                  y|\n",
      "+--------------------+-------------------+--------------------+--------------------+-------------------+\n",
      "| -0.5397017192382667| 0.8329027547714076| -0.5100548423652854| -1.0662451781197946| -51.14999766625296|\n",
      "|-0.00339115150069...|-1.1316601186140154| -0.5892035094858212| -0.9676262804386547|-123.24983850094662|\n",
      "| -1.4816785384136577|0.18075465755420211| -1.4567734774332153|-0.21502567111289722| -69.67885144696598|\n",
      "|  3.5150546124925293|0.38313853181543467|-0.08827401307422125| -1.6337495471987837|  65.06764348909356|\n",
      "| 0.29853695010243364| 2.0183587461393824|  0.0979203116012738|  1.0599796258082859|  188.3745334938853|\n",
      "+--------------------+-------------------+--------------------+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문제 2-3의 답\n",
    "* 훈련, 테스트 데이터를 6:4 비율로 분리하고,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainDf, testDf) = Df.randomSplit([0.6, 0.4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문제 2-4의 답\n",
    "* 회귀모델링하고 계수와 절편 출력. \n",
    "다음과 같이 실제와 예측을 10줄 출력.\n",
    "\n",
    "\n",
    "```\n",
    "+-------------------+-------------------+\n",
    "|                  y|         prediction|\n",
    "+-------------------+-------------------+\n",
    "|-109.97900025087853|-109.80947358240276|\n",
    "|-145.43579783438383|-144.69608022770103|\n",
    "|-133.86831015463093|-133.02142763705754|\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "va = VectorAssembler(inputCols = [\"x0\",\"x1\",\"x2\",\"x3\"],\n",
    "                                   outputCol = 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression(featuresCol = 'features', labelCol='y', maxIter=10, regParam=0.3, elasticNetParam=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[va,lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(trainDf)\n",
    "modelTrainDf = model.transform(trainDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**계수와 절편 출력**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [43.38175548926913,51.639773309331225,0.0,66.10944308055264]\n",
      "Intercept: -0.024\n"
     ]
    }
   ],
   "source": [
    "print(\"Coefficients: {}\".format(model.stages[-1].coefficients))\n",
    "print(\"Intercept: {:.3f}\".format(model.stages[-1].intercept))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**실제와 예측 출력**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>pridiction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-51.149998</td>\n",
       "      <td>-51.149998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-123.249839</td>\n",
       "      <td>-123.249839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-69.678851</td>\n",
       "      <td>-69.678851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>65.067644</td>\n",
       "      <td>65.067644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>188.374533</td>\n",
       "      <td>188.374533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-109.866574</td>\n",
       "      <td>-109.866574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10.629199</td>\n",
       "      <td>10.629199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>44.292317</td>\n",
       "      <td>44.292317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-79.198037</td>\n",
       "      <td>-79.198037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-62.956010</td>\n",
       "      <td>-62.956010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            y  pridiction\n",
       "0  -51.149998  -51.149998\n",
       "1 -123.249839 -123.249839\n",
       "2  -69.678851  -69.678851\n",
       "3   65.067644   65.067644\n",
       "4  188.374533  188.374533\n",
       "5 -109.866574 -109.866574\n",
       "6   10.629199   10.629199\n",
       "7   44.292317   44.292317\n",
       "8  -79.198037  -79.198037\n",
       "9  -62.956010  -62.956010"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Df.select('y','prediction').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문제 2-5의 답\n",
    "* 테스트 데이터에 대해 R2를 구해서 출력하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelTestDf=model.transform(testDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator=RegressionEvaluator(labelCol=\"y\", predictionCol=\"prediction\", metricName=\"rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse=evaluator.evaluate(modelTestDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Root Mean Squared Error (RMSE) on test data: 0.52657348'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"Root Mean Squared Error (RMSE) on test data: {rmse:.8f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator=RegressionEvaluator(labelCol=\"y\", predictionCol=\"prediction\", metricName=\"r2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'r2: 0.99998153'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"r2: {evaluator.evaluate(modelTestDf):.8f}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 문제 3:텍스트 분류"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문제 3-1의 답\n",
    "데이터프레임을 생성하세요.<br>\n",
    "영화리뷰를 파일로 작성하자. ipynb에서 직접 하거나 편리한 에디터를 사용함.\n",
    "\n",
    "- 리뷰는 한 줄씩 저장하며, 각각의 긍정, 부정을 구분하여 레이블을 지정하고 함께 저장함.\n",
    "\n",
    "- 파일로 작성하지 못하면, 이후 문제를 포기하지 말고 리스트로 만들거나 자신이 할 수 있는 방법을 통해서 만들어도 됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "revDf = spark\\\n",
    "    .read\\\n",
    "    .options(header='false', inferschema='true', delimiter='\\t')\\\n",
    "    .csv(os.path.join('data', 'movie_review.txt'))  # 0=부정, 1=긍정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "revDf = revDf.withColumnRenamed(\"_c0\", \"review\")\n",
    "revDf = revDf.withColumnRenamed(\"_c1\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+-----+\n",
      "|                            review|label|\n",
      "+----------------------------------+-----+\n",
      "|고구마 먹은 영화 유치해서 못보겠다|    0|\n",
      "|노잼이네요 왜 배우들이 출연했는...|    0|\n",
      "| 제발 뻘짓 좀 그만하셨으면 이건...|    0|\n",
      "|         스토리가 매우 매우 아쉽다|    0|\n",
      "|             영화 약간 이해가 안됨|    0|\n",
      "+----------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "revDf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "tokenizer = Tokenizer(inputCol=\"review\", outputCol=\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokDf = tokenizer.transform(revDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+-----+------------------------------+\n",
      "|                            review|label|                         words|\n",
      "+----------------------------------+-----+------------------------------+\n",
      "|고구마 먹은 영화 유치해서 못보겠다|    0|[고구마, 먹은, 영화, 유치해...|\n",
      "|노잼이네요 왜 배우들이 출연했는...|    0|[노잼이네요, 왜, 배우들이, ...|\n",
      "| 제발 뻘짓 좀 그만하셨으면 이건...|    0|[제발, 뻘짓, 좀, 그만하셨으...|\n",
      "|         스토리가 매우 매우 아쉽다|    0|[스토리가, 매우, 매우, 아쉽다]|\n",
      "|             영화 약간 이해가 안됨|    0|    [영화, 약간, 이해가, 안됨]|\n",
      "+----------------------------------+-----+------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokDf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문제 3-2의 답\n",
    "불용어 제거, 불필요 단어 정리\n",
    "\n",
    "- 느낌표, 점, '...', 'ㅋㅋㅋ', 'ㅠ', 'ㄹㅇ' 불필요한 단어 제거\n",
    "- 불용어 제거: '그', '수', '본', '등', '때', '걸', '다', '게', '날','짱', '거', '중', '더', '넘', '두', '달', '올', '뻔', '단', '개', '듯'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StopWordsRemover_ae5b40ad3278"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "stop = StopWordsRemover(inputCol=\"words\", outputCol=\"nostops\")\n",
    "\n",
    "_mystopwords=['!', '.', ',', u'ㅋㅋㅋ', u'ㅠ', u'ㄹㅇ', u'그', u'수', u'본', u'등', u'때', u'걸', u'다', u'게', u'날', u'짱', u'거', u'중', u'더', u'넘', u'두', u'달', u'올', u'뻔', u'단', u'개', u'듯']\n",
    "stop.setStopWords(_mystopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!/./,/ㅋㅋㅋ/ㅠ/ㄹㅇ/그/수/본/등/때/걸/다/게/날/짱/거/중/더/넘/두/달/올/뻔/단/개/듯/"
     ]
    }
   ],
   "source": [
    "for e in stop.getStopWords():\n",
    "    print (e, end=\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+-----+--------------------------------+--------------------------------+\n",
      "|                            review|label|                           words|                         nostops|\n",
      "+----------------------------------+-----+--------------------------------+--------------------------------+\n",
      "|고구마 먹은 영화 유치해서 못보겠다|    0|  [고구마, 먹은, 영화, 유치해...|  [고구마, 먹은, 영화, 유치해...|\n",
      "|노잼이네요 왜 배우들이 출연했는...|    0|  [노잼이네요, 왜, 배우들이, ...|  [노잼이네요, 왜, 배우들이, ...|\n",
      "| 제발 뻘짓 좀 그만하셨으면 이건...|    0|  [제발, 뻘짓, 좀, 그만하셨으...|  [제발, 뻘짓, 좀, 그만하셨으...|\n",
      "|         스토리가 매우 매우 아쉽다|    0|  [스토리가, 매우, 매우, 아쉽다]|  [스토리가, 매우, 매우, 아쉽다]|\n",
      "|             영화 약간 이해가 안됨|    0|      [영화, 약간, 이해가, 안됨]|      [영화, 약간, 이해가, 안됨]|\n",
      "|좋아하는 배우들인데 최근 영화라...|    0| [좋아하는, 배우들인데, 최근,...| [좋아하는, 배우들인데, 최근,...|\n",
      "| 그지같네 도대체 이 따위 영화를...|    0|  [그지같네, 도대체, 이, 따위...|  [그지같네, 도대체, 이, 따위...|\n",
      "|  스크린보다 시계를 더 많이 봄....|    0|  [스크린보다, 시계를, 더, 많...|  [스크린보다, 시계를, 많이, ...|\n",
      "| 이때까지 본 영화중에 제일 최악...|    0|  [이때까지, 본, 영화중에, 제...|  [이때까지, 영화중에, 제일, ...|\n",
      "|    나만 짜증.. 줄거리 한줄평.....|    0|    [나만, 짜증.., 줄거리, 한...|    [나만, 짜증.., 줄거리, 한...|\n",
      "|로맨틱 코미디로 방향을 잡았으면...| null|  [로맨틱, 코미디로, 방향을, ...|  [로맨틱, 코미디로, 방향을, ...|\n",
      "| 똥 싸다가 막힌 느낌 안보는거를...|    0|    [똥, 싸다가, 막힌, 느낌, ...|    [똥, 싸다가, 막힌, 느낌, ...|\n",
      "|  어우 핵노잼 영화가 왜 이렇게 ...|    0|   [어우, 핵노잼, 영화가, 왜,...|   [어우, 핵노잼, 영화가, 왜,...|\n",
      "| 솔직히 재미없음 감독이 뭘 말할...| null|  [솔직히, 재미없음, 감독이, ...|  [솔직히, 재미없음, 감독이, ...|\n",
      "|   뭘 말하고싶은지 모르겠다 아쉽다|    0| [뭘, 말하고싶은지, 모르겠다,...| [뭘, 말하고싶은지, 모르겠다,...|\n",
      "| 새벽에 케이블에서 볼거 없어서 ...| null|  [새벽에, 케이블에서, 볼거, ...|  [새벽에, 케이블에서, 볼거, ...|\n",
      "|         핵노잼 억지 착한척 오졌다|    0|  [핵노잼, 억지, 착한척, 오졌다]|  [핵노잼, 억지, 착한척, 오졌다]|\n",
      "|오그라드는 대사는 기본에 옵션으...|    0| [오그라드는, 대사는, 기본에,...| [오그라드는, 대사는, 기본에,...|\n",
      "| 역대급 호화캐스팅인데 스토리, ...|    0|[역대급, 호화캐스팅인데, 스토...|[역대급, 호화캐스팅인데, 스토...|\n",
      "|  스토리 개 말도 안되는 억지 고...|    0|   [스토리, 개, 말도, 안되는,...|  [스토리, 말도, 안되는, 억지...|\n",
      "+----------------------------------+-----+--------------------------------+--------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stopDf=stop.transform(tokDf) #불용어 제거 된 컬럼\n",
    "stopDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['고구마', '먹은', '영화', '유치해서', '못보겠다']\n",
      "['노잼이네요', '왜', '배우들이', '출연했는지', '이해가', '잘', '안됨']\n",
      "['제발', '뻘짓', '좀', '그만하셨으면', '이건', '여러모로', '낭비다.']\n",
      "['스토리가', '매우', '매우', '아쉽다']\n",
      "['영화', '약간', '이해가', '안됨']\n"
     ]
    }
   ],
   "source": [
    "for r in stopDf.select(\"nostops\").take(5):\n",
    "    for e in r:\n",
    "        print (e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문제 3-3의 답\n",
    "TF-IDF를 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "hashTF = HashingTF(inputCol=\"nostops\", outputCol=\"features\")\n",
    "hashDf = hashTF.transform(stopDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = IDF(inputCol=\"features\", outputCol=\"idf\")\n",
    "\n",
    "idfModel = idf.fit(hashDf)\n",
    "idfDf = idfModel.transform(hashDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(nostops=['고구마', '먹은', '영화', '유치해서', '못보겠다'], idf=SparseVector(262144, {114067: 3.1781, 145746: 1.9253, 202865: 2.7726, 232307: 2.7726, 243237: 2.7726}))\n",
      "Row(nostops=['노잼이네요', '왜', '배우들이', '출연했는지', '이해가', '잘', '안됨'], idf=SparseVector(262144, {14123: 3.1781, 77545: 3.1781, 89636: 2.7726, 102360: 2.7726, 123815: 2.4849, 221856: 2.7726, 252373: 2.2618}))\n",
      "Row(nostops=['제발', '뻘짓', '좀', '그만하셨으면', '이건', '여러모로', '낭비다.'], idf=SparseVector(262144, {45298: 2.7726, 104569: 3.1781, 159598: 3.1781, 160982: 3.1781, 165201: 3.1781, 196501: 3.1781, 249777: 2.7726}))\n",
      "Row(nostops=['스토리가', '매우', '매우', '아쉽다'], idf=SparseVector(262144, {8121: 2.2618, 118336: 6.3561, 181130: 3.1781}))\n",
      "Row(nostops=['영화', '약간', '이해가', '안됨'], idf=SparseVector(262144, {35471: 3.1781, 102360: 2.7726, 145746: 1.9253, 221856: 2.7726}))\n"
     ]
    }
   ],
   "source": [
    "for e in idfDf.select(\"nostops\",\"idf\").take(5):\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+--------------------+--------------------+\n",
      "|                       nostops|            features|                 idf|\n",
      "+------------------------------+--------------------+--------------------+\n",
      "|[고구마, 먹은, 영화, 유치해...|(262144,[114067,1...|(262144,[114067,1...|\n",
      "|[노잼이네요, 왜, 배우들이, ...|(262144,[14123,77...|(262144,[14123,77...|\n",
      "|[제발, 뻘짓, 좀, 그만하셨으...|(262144,[45298,10...|(262144,[45298,10...|\n",
      "+------------------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idfDf.select(\"nostops\", \"features\",\"idf\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문제 3-4의 답\n",
    "* 베이지안 모델을 적용해서 분류 (alpha는 1.0으로 설정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "nb=NaiveBayes(featuresCol='features', labelCol='label', modelType='multinomial', predictionCol='prediction',smoothing=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=nb.fit(hashDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=model.transform(hashDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.select('label','review','prediction').show(45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문제 3-5의 답\n",
    "* 훈련, 테스트 데이터를 반반씩 분리하고, 훈련 후 정확성을 계산하고 출력."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainDf, testDf) = hashDf.randomSplit([0.5, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2=nb.fit(trainDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_train=model2.transform(trainDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_train.select('label','review','prediction').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator=BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.evaluate(predictions_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문제 3-6의 답\n",
    "* 앞서 훈련한 베이지안 모델로 다음 문장을 긍정, 부정으로 분류하세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
