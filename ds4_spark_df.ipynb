{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark DataFrame\n",
    "\n",
    "* Last updated 20201013TUE1400 20191029_20170606_20170221_20161125\n",
    "\n",
    "## S.1 학습내용\n",
    "\n",
    "### S.1.1 목표\n",
    "\n",
    "* Spark DataFrame을 생성하고, API를 사용할 수 있다.\n",
    "* Spark SQL을 사용하여 데이터를 조회할 수 있다.\n",
    "* Spark 데이터를 MongoDB에 쓰고, 읽을 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### S.1.2 목차\n",
    "* S.2 Jupyter Notebook에서 SparkSession 생성\n",
    "* S.3 DataFrame: 특징, Schema, DataFrame의 API\n",
    "* S.4 DataFrame 생성\n",
    "* S.4.1 schema에서 생성하기\n",
    "* S.4.2 RDD에서 생성하기\n",
    "* S.4.3 Pandas\n",
    "* S.4.4 csv 파일 읽기\n",
    "* S.4.5 tsv 파일 읽기\n",
    "* S.4.6 JSON 파일 읽기\n",
    "* 문제: 월드컵 데이터 JSON\n",
    "* S.4.7 Parquet 파일 읽기, 쓰기\n",
    "* S.5 DataFrame API 사용해 보기\n",
    "    * 빈 DataFrame 생성\n",
    "    * range\n",
    "    * 컬럼 추가 withColumn, 컬럼 삭제 Drop\n",
    "    * 사용자정의 함수 udf\n",
    "    * 컬럼명 변경 withColumnRenamed\n",
    "    * 그래프\n",
    "    * 컬럼 조회 select\n",
    "    * filter\n",
    "    * regexp_replace 컬럼의 내용 변경\n",
    "    * groupBy\n",
    "    * F 함수\n",
    "    * 행 추가\n",
    "    * partition\n",
    "    * 통계 요약 describe\n",
    "    * 결측값\n",
    "* 문제: 년별 분기별 대여건수\n",
    "* S.6 Spark SQL\n",
    "* 문제 S-1: 네트워크에 불법적으로 침입하는 사용자의 분석\n",
    "* 문제 S-2: Twitter JSON 데이터 읽기\n",
    "* 문제 S-3: 뉴욕에서 출생한 신생아 분석\n",
    "* 문제 S-4: 우버 택시의 운행기록 분석\n",
    "* 문제 S-5: JDBC를 사용해서 데이터 읽기\n",
    "* S.7 MongoDB Spark connector\n",
    "* S.7.1 설정\n",
    "* S.7.2 uri \n",
    "* S.7.3 MongoDB Python API \n",
    "* S.7.4 연습으로 쓰기, 읽기\n",
    "* S.8 spark-submit\n",
    "* S.8.1 간단한 작업\n",
    "* S.8.2 MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## S.2 Jupyter Notebook에서 SparkSession 생성\n",
    "\n",
    "### Spark를 설치한 경우\n",
    "\n",
    "Spark를 설치했다면, Spark가 설치된 디렉토리를 SPARK_HOME으로 설정하고, 실행에 필요한 'py4j-0.10.1-src.zip', 'pyspark.zip' 라이브러리를 추가해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "home=os.path.expanduser(\"~\")\n",
    "os.environ[\"PYSPARK_PYTHON\"]=\"C:\\\\Users\\\\zaqxs\\\\Anaconda2\\\\envs\\\\py35\\\\python.exe\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"]=\"C:\\\\Users\\\\zaqxs\\\\Anaconda2\\\\envs\\\\py35\\\\python.exe\"\n",
    "os.environ[\"PYLIB\"]=os.path.join(os.environ[\"SPARK_HOME\"],'python','lib')\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'py4j-0.10.9-src.zip'))\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'pyspark.zip'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "그러나 pyspark만을 설치한 경우에는, 위와 같이 별도로 경로와 라이브러리를 추가하지 않아도 된다.\n",
    "다만 Python 2, 3 여러 버전이 설치된 경우에는 아래와 같이 그 경로를 설정해 주어야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "myConf=pyspark.SparkConf()\n",
    "spark = pyspark.sql.SparkSession\\\n",
    "    .builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"myApp\")\\\n",
    "    .config(conf=myConf)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\zaqxs\\\\Code\\\\201910787\\\\s-master'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.3 DataFrame\n",
    "\n",
    "### 특징\n",
    "\n",
    "DataFrame은 **행, 열로 구조화**된 데이터구조이다.\n",
    "관계형데이터베이스 RDB의 테이블이나 엑셀 쉬트sheet와 비슷하다.\n",
    "또는 Pandas 또는 R을 사용해 보았다면 거기서 제공되는 DataFrame과 유사하다.\n",
    "Apache Spark 1.0에서는 **SchemaRDD**라는 명칭으로 시험적으로 제공되었다. 이름에서 보듯이 RDD에 스키마를 얹어서 만든 개념이다.\n",
    "그러나 Spark의 DataFrame은 **대용량 데이터를 처리하기 위해 만들어진 프레임워크로 분산**해서 사용할 수 있게 고안되었다.\n",
    "\n",
    "앞서 사용했던 **RDD가 schema를 정하지 않는** 것과 달리, **DataFrame은 모델 schema**를 설정해서 사용을 한다. '열'에 대해 명칭 및 **데이터 타잎**을 가지고 있고, 이를 지켜서 데이터를 저장하게 된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Schema\n",
    "\n",
    "**Row**는 DataFrame의 행으로, 데이터 요소항목을 묶어서 구성한다. **Python list나 dict**를 사용하여 Row를 구성할 수 있다.\n",
    "**Column**은 DataFrame의 **열**이고, 다음과 같은 **데이터 타잎**을 가진다.\n",
    "\n",
    "```python\n",
    "- NullType\n",
    "- StringType\n",
    "- BinaryType\n",
    "- BooleanType\n",
    "- DateType\n",
    "- TimestampType\n",
    "- DoubleType\n",
    "- DecimalType\n",
    "- ShortType\n",
    "- ArrayType\n",
    "- MapType\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### DataFrame의 API\n",
    "\n",
    "RDD와 마찬가지로 DataFrame을 구성하여 **머신러닝**의 입력데이터로 사용할 수 있다.\n",
    "현재 버전 2.0부터 RDD에 대한 지원은 줄여나가고, **버전 3.0 이후에는 DataFrame API를 공식적으로 지원**한다고 발표한 바 있다. RDD보다 우선적으로 사용하는 것이 좋겠다. 아래는 DataFrame에서 제공하는 일부 API이다.\n",
    "\n",
    "기능 | 설명 | 예제\n",
    "-------|-------|-------\n",
    "```json``` | json 파일에서 읽기 | ```spark.read.json(\"employee.json\")```\n",
    "```show``` | DataFrame에 로딩된 데이터 읽기 | ```df.show()```\n",
    "```schema``` | 데이터 schema 보기 | ```df.printSchema()```\n",
    "```select``` | 열을 선택 | ```df.select(\"name\")```\n",
    "```filter``` | 조건으로 선택하여 읽기 | ```df.filter(dfs(\"age\") > 23).show()```\n",
    "```groupBy``` | 그룹으로 나누기 | ```df.groupBy(\"age\").count().show()```\n",
    "```dropna``` | na를 삭제 | ```df.dropna()``` ```df.na.drop()```\n",
    "```fillna``` | na를 값으로 채우기 | ```fillna()```\n",
    "```count``` | 행 세기 | ```df.count()```\n",
    "```drop``` | 삭제 | ```df.drop(\"name\")```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.4 DataFrame 생성\n",
    "\n",
    "DataFrame은 관계형데이터베이스 RDB 테이블과 같이, schema를 정해서 생성한다.\n",
    "**schema를 정해주지 않으면, Spark가 자동으로 유추**하게 된다.\n",
    "Python 리스트, RDD, Pandas DataFrame, Hive, csv, JSON, RDB, XML, Parquet, Apache Cassandra 등 다양한 채널에서 읽어서 DataFrame을 만들 수 있다.\n",
    "\n",
    "* **```spark.createDataFrame()```** 함수는 Python List, RDD, Pandas DataFrame 등에서 읽어서 DataFrame을 만들 수 있다.\n",
    "* 또는 **```spark.read.text, spark.read.json, spark.read.parquet, spark.read.load```** 함수로 옵션을 설정해서 외부 파일 등에서 읽을 수 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### S.4.1 schema 생성하기\n",
    "\n",
    "DataFrame은 데이터 모델 schema를 정의하고, 각 컬럼의 명칭과 데이터타입을 정해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 자동으로 인식하는 schema\n",
    "\n",
    "우선 단순하게 Python 자료구조를 사용해서 생성해 본다.\n",
    "아래 열이 3개인 데이터를 **```createDataFrame()```** 함수를 사용하여 넣어 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "myList=[('1','kim, js', 170),\n",
    "        ('1','lee, sm', 175),\n",
    "        ('2','lim, yg',180),\n",
    "        ('2','lee', 170)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "myDf=spark.createDataFrame(myList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "이 경우 Spark가 **자동으로 schema를 설정**한다.\n",
    "```printSchema()``` 함수로 schema를 출력해 보기로 하자.\n",
    "* **컬럼**명은 **일련번호**를 가지고 생성된다. schema를 정하지 않았으므로, 열은 '_1', '_2'와 같이 명명된다.\n",
    "* **데이터 타잎**도 유추해서 생성한다. 올바르게 되지 않을 경우도 있다는 점에 유의한다. 아래에서 보듯이 컬럼 1 학년은 ```String```, 컬럼 2 이름은 ```String```, 컬럼 3 키는 ```long```으로 인식하고 있다.\n",
    "* nullable은 결측값이 허용되는지를 말하는 것이다. true이면 허용된다는 의미이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_1', '_2', '_3']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한 줄을 출력해 보자. 출력은 Row() 타입으로 앞서 정의한 schema와 일치하는 값을 포함하여 출력한다.\n",
    "즉 첫째, 둘째는 문자열로 세째는 long 타입이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(_1='1', _2='kim, js', _3=170)]\n"
     ]
    }
   ],
   "source": [
    "print (myDf.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 컬럼명 설정\n",
    "\n",
    "앞서 컬럼 Column을 정의하지 않고 DataFrame을 생성하였는데, 이번에는 **컬럼을 정해서** 생성하자.\n",
    "**```createDataFrame()```** 함수에 **인자로 컬럼명을 리스트 ```['year','name','height']```로** 정해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['year','name','height']\n",
    "_myDf = spark.createDataFrame(myList, cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['year', 'name', 'height']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_myDf.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그리고 한 줄 출력해 보면, 컬럼명이 변경되어 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(year='1', name='kim, js', height=170)]\n"
     ]
    }
   ],
   "source": [
    "print (_myDf.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 100개를 생성해 보자.\n",
    "우선 데이터의 원천이 되는 names, items를 정의하자.\n",
    "여기서 하나씩 선택하여 데이터를 생성하게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"kim\",\"lee\",\"lee\",\"lim\"]\n",
    "items = [\"espresso\",\"latte\",\"americano\",\"affocato\",\"long black\",\"macciato\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "이 names, items 배열에서 **modulus**를 활용하여 하나씩 선택하여 데이터를 생성한다.\n",
    "**names**는 4개이므로 4로 나눈 나머지와 **items**는 6개이므로 6으로 나눈 나머지를 하나씩 선택하고 있다.\n",
    "그리고 컬럼명을 ```[\"name\",\"coffee\"]```으로 정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "coffeeDf = spark.createDataFrame([(names[i%4], items[i%6]) for i in range(100)],\\\n",
    "                           [\"name\",\"coffee\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자동으로 생성된 schema가 데이터타입을 잘 집어내고 있다.\n",
    "name, coffee 모두 string이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- coffee: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "coffeeDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+\n",
      "|name|    coffee|\n",
      "+----+----------+\n",
      "| kim|  espresso|\n",
      "| lee|     latte|\n",
      "| lee| americano|\n",
      "| lim|  affocato|\n",
      "| kim|long black|\n",
      "| lee|  macciato|\n",
      "| lee|  espresso|\n",
      "| lim|     latte|\n",
      "| kim| americano|\n",
      "| lee|  affocato|\n",
      "+----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "coffeeDf.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Row 객체를 사용해서 생성\n",
    "\n",
    "* Row 생성\n",
    "**Row**를 사용해 보자.\n",
    "Row는 **이름(Column)이 붙여진 행**으로 **관계형데이터베이스 레코드 Record**와 같다. \n",
    "속성 명은 'year', 'name', 'height'로 명명한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "Person = Row('year','name', 'height')\n",
    "row1=Person('1','kim, js',170)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Row 속성 읽기\n",
    "\n",
    "속성명을 읽을 때에는 **```row.key```** 또는 Python dict형식으로 **```row[key]```**와 같이 속성을 읽을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row1:  1 kim, js 170\n"
     ]
    }
   ],
   "source": [
    "print (\"row1: \", row1.year, row1.name, row1.height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Row를 Dictionary로 저장\n",
    "\n",
    "Row는 속성명과 값을 가지고 있기 때문에 Dictionary로 쉽게 변환할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'year': '1', 'name': 'kim, js', 'height': 170}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row1.asDict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionary에서 키 또는 값을 읽을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['year', 'name', 'height'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row1.asDict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values(['1', 'kim, js', 170])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row1.asDict().values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Row에서 DataFrame 생성\n",
    "\n",
    "위에서 설정한 Row를 사용하여 DataFrame을 만들어 보자.\n",
    "**Python list에 Row를 넣어** 구성한다.\n",
    "첫번째는 앞서 만든 row1 객체를 넣을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRows = [row1,\n",
    "          Person('1','lee, sm', 175),\n",
    "          Person('2','lim, yg',180),\n",
    "          Person('2','lee',170)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "myDf=spark.createDataFrame(myRows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "printSchema()를 해보면, **데이터 타잎**은 string, long으로 Spark에서 **자동** 인식되었다는 것을 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: long (nullable = true)\n",
      "\n",
      "None\n",
      "+----+-------+------+\n",
      "|year|   name|height|\n",
      "+----+-------+------+\n",
      "|   1|kim, js|   170|\n",
      "|   1|lee, sm|   175|\n",
      "|   2|lim, yg|   180|\n",
      "|   2|    lee|   170|\n",
      "+----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (myDf.printSchema())\n",
    "myDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### schema를 정의하고 생성\n",
    "\n",
    "모델 schema를 정하고, **데이터 타잎**을 정의해 DataFrame을 생성해 본다.\n",
    "**```StructType```**으로 구조체를 선언하고, 컬럼에 대해 **```StructField```**를 설정한다.\n",
    "* **컬럼**의 명칭\n",
    "* 앞서 소개했던 **데이터 타잎**\n",
    "* 마지막은 **NULL**이 허용되는지 여부\n",
    "\n",
    "```python\n",
    "StructType([\n",
    "    StructField(컬럼명, StringType(), True),\n",
    "    ...\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "mySchema=StructType([\n",
    "    StructField(\"year\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"height\", IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "앞서 **myRows**를 데이터로, **mySchema**에서는 컬럼 명과 데이터타잎을 정의하여 ```createDataFrame()```함수의 인자로 넘겨주고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDf=spark.createDataFrame(myRows, mySchema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 설정한 데이터타입으로 설정되어있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(year='1', name='kim, js', height=170)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDf.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.4.2 RDD에서 생성하기\n",
    "\n",
    "RDD는 schema가 정해지지 않은 비구조적 데이터이다.\n",
    "이와 같이 **schema를 정의하지 않으면, Spark는 schema를 유추**하게 된다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### schema 자동 인식\n",
    "\n",
    "RDD로부터 DataFrame을 생성할 수 있다. 이 경우 schema를 설정하지 않으면 자동으로 인식된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "myList=[('1','kim, js',170), ('1','lee, sm', 175), f('2','lim, yg',180), ('2','lee',170)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "myRdd = spark.sparkContext.parallelize(myList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```toDF()```로 변환하거나 직접 ```createDataFrame()``` 함수를 사용하여 DataFrame을 생성할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "rddDf=myRdd.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rddDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "또는 RDD에서 DataFrame을 생성할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "rddDf=spark.createDataFrame(myRdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rddDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Row를 사용\n",
    "\n",
    "학년year는 앞에서는 **string**으로 인식되었다. 이번 예제에서는 **형변환**을 해 본다.\n",
    "RDD의 ```map()``` 함수를 사용하여 각 속성을 읽고 ```int()``` 함수로 형변환을 한다.\n",
    "각 속성에 명칭, year, name, height를 설정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "_myRdd=myRdd.map(lambda x:Row(year=int(x[0]), name=x[1], height=int(x[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "_myDf=spark.createDataFrame(_myRdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_myDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(year=1, name='kim, js', height=170)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_myDf.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```Row()```를 사용하여 RDD를 생성할 수도 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "r1=Row(name=\"js1\", age=10)\n",
    "r2=Row(name=\"js2\", age=20)\n",
    "_myRdd=spark.sparkContext.parallelize([r1,r2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='js1', age=10), Row(name='js2', age=20)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_myRdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "####  schema를 정의하고 생성\n",
    "\n",
    "앞서 보았듯이, schema를 정의하고 RDD에서 DataFrame을 생성할 수 있다.\n",
    "**```StructType```**을 선언하고,\n",
    "컬럼에 대해 **```StructField```**를 **컬럼명**, **데이터 타잎**, **NULL**이 허용되는지 여부를 설정한다.\n",
    "과거 버전에서는 컬럼명이 정렬되면서 age, name 순서가 변경되었다.\n",
    "현재는 **컬럼명을 정렬하지 않으므로, 순서대로** 아래와 같이 생성하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema=StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    #StructField(\"created\", TimestampType(), True)\n",
    "])\n",
    "_myDf=spark.createDataFrame(_myRdd, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_myDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|name|age|\n",
      "+----+---+\n",
      "| js1| 10|\n",
      "| js2| 20|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_myDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "schema를 정해서 RDD로부터 DataFrame을 생성할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "myRdd=spark.sparkContext.parallelize([(1, 'kim', 50.0), (2, 'lee', 60.0), (3, 'park', 70.0)])\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"height\", DoubleType(), True)\n",
    "])\n",
    "_myDf = spark.createDataFrame(myRdd, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_myDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+\n",
      "| id|name|height|\n",
      "+---+----+------+\n",
      "|  1| kim|  50.0|\n",
      "|  2| lee|  60.0|\n",
      "|  3|park|  70.0|\n",
      "+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_myDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.4.3 Pandas\n",
    "\n",
    "Spark Dataframe은 다른 프로그래밍 언어에서도 분석도구로 많이 사용되는 형식이다.\n",
    "엑셀 스프레드쉬트와 비슷하다. 또한 최근 많이 사용되는 **R**의 Dataframe이나 **Python Pandas**를 예로 들 수 있다.\n",
    "Spark와 Pandas의 Dataframe을 비교하면,\n",
    "**Pandas**는 데이터 양이 적은 경우, Spark는 분산처리할 수 있으므로 빅데이터에 보다 적합하다.\n",
    "API를 사용하게 되면 Spark Dataframe과 Pandas 간에는 차이가 있다.\n",
    "\n",
    "구분 | DataFrame | Pandas\n",
    "-------|-------|-------\n",
    "csv 파일 읽기 | read.json() | read_csv()\n",
    "데이터타입 | inferschema=True 설정하면 추정 | 모두 strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Dataframe을 Pandas로 변환\n",
    "\n",
    "Spark Dataframe을 ```toPandas()``` 함수를 사용하여 **Pandas로 변환**할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>name</th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>kim, js</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>lee, sm</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>lim, yg</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>lee</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  year     name  height\n",
       "0    1  kim, js     170\n",
       "1    1  lee, sm     175\n",
       "2    2  lim, yg     180\n",
       "3    2      lee     170"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDf.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Pandas에서 csv 쓰기\n",
    "\n",
    "Dataframe을 **csv**파일로 내보내고 Pandas로 읽어보자.\n",
    "\n",
    "Dataframe을 csv로 쓰려면 라이브러리 ```com.databricks.spark.csv```를 사용해야 한다. **파일이 아니라 디렉토리가 생성**되고 그 안에 파일로 쓰여지게 된다. Pandas를 사용하면 우리가 보통 사용하는 하나의 파일로 쓰여진다.\n",
    "\n",
    "한 번 생성이 되면 덮어쓰기를 하지 않기 때문에, 다시 write() 할 경우에는 이전 디렉토리를 삭제하도록 하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "path file:/C:/Users/zaqxs/Code/201910787/s-master/data/_myDf.csv already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-c7a6cb19a019>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmyDf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'com.databricks.spark.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'_myDf.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[0;32m   1107\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1108\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1109\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1111\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1305\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1307\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    115\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: path file:/C:/Users/zaqxs/Code/201910787/s-master/data/_myDf.csv already exists."
     ]
    }
   ],
   "source": [
    "myDf.write.format('com.databricks.spark.csv').save(os.path.join('data','_myDf.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dir _myDf.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Pandas를 이용하여 Dataframe을 csv파일로 내보낼 수 있다.\n",
    "\n",
    "```python\n",
    ",year,name,height\n",
    "0,1,\"kim, js\",170\n",
    "1,1,\"lee, sm\",175\n",
    "2,2,\"lim, yg\",180\n",
    "3,2,lee,170\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDf.toPandas().to_csv(os.path.join('data','myDf.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas에서 컬럼을 생성,삭제 해보자.\n",
    "recode - 현재 변수 값을 다시 줄 경우\n",
    "\n",
    "나라변 국제전화코드는 Japan: 81, South Korea: 82, Hong Kong: 852, Australia: 61을 사용한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "icc = pd.DataFrame( { 'country': ['South Korea','Japan','Hong Kong'],'codes': [81, 82, 852] })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>codes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>South Korea</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Japan</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hong Kong</td>\n",
       "      <td>852</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       country  codes\n",
       "0  South Korea     81\n",
       "1        Japan     82\n",
       "2    Hong Kong    852"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "icc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전화코드가 81인 경우 출력해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>codes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>South Korea</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       country  codes\n",
       "0  South Korea     81"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "icc[icc['codes']==81]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.4.4 csv 파일에서 생성\n",
    "\n",
    "#### RDD에서 DataFrame\n",
    "\n",
    "앞서 RDD에서 읽었던 csv파일을 다시 읽어보자.\n",
    "\n",
    "```sparkContext.textFile()``` 함수로 읽은 파일은 RDD이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "cfile= os.path.join(\"data\", \"ds_spark_2cols.csv\")\n",
    "lines = spark.sparkContext.textFile(cfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "RDD에서 ```Row()```를 사용하여 Dataframe으로 변환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "_col12 = lines.map(lambda l: l.split(\",\"))\n",
    "col12 = _col12.map(lambda p: Row(col1=int(p[0].strip()), col2=int(p[1].strip())))\n",
    "\n",
    "_myDf = spark.createDataFrame(col12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- col1: long (nullable = true)\n",
      " |-- col2: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(col1=35, col2=2),\n",
       " Row(col1=40, col2=27),\n",
       " Row(col1=12, col2=38),\n",
       " Row(col1=15, col2=31),\n",
       " Row(col1=21, col2=1),\n",
       " Row(col1=14, col2=19),\n",
       " Row(col1=46, col2=1),\n",
       " Row(col1=10, col2=34),\n",
       " Row(col1=28, col2=3),\n",
       " Row(col1=48, col2=1),\n",
       " Row(col1=16, col2=2),\n",
       " Row(col1=30, col2=3),\n",
       " Row(col1=32, col2=2),\n",
       " Row(col1=48, col2=1),\n",
       " Row(col1=31, col2=2),\n",
       " Row(col1=22, col2=1),\n",
       " Row(col1=12, col2=3),\n",
       " Row(col1=39, col2=29),\n",
       " Row(col1=19, col2=37),\n",
       " Row(col1=25, col2=2)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_myDf.printSchema()\n",
    "_myDf.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### DataFrame으로 직접 읽기\n",
    "\n",
    "format().load() 또는 csv() 함수로 csv 파일을 읽어서 DataFrame을 만들 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data/ds_spark.csv\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/ds_spark.csv\n",
    "1,2,3,4\n",
    "11,22,33,44\n",
    "111,222,333,444"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### format load\n",
    "\n",
    "csv 패키지를 사용해서 읽어 본다. 우선 [Spark의 csv 패키지를 추가한다.](https://spark-packages.org/package/databricks/spark-csv) 패키지는 설정파일 ```spark-defaults.conf```에 추가할 수 있다.\n",
    "\n",
    "```python\n",
    "$ vim conf/spark-defaults.conf\n",
    "spark.jars.packages=com.databricks:spark-csv_2.11:1.5.0\n",
    "```\n",
    "\n",
    "```format(\"csv\").load(\"path\")```이라고 하고, options() 설정을 미리 넣을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df = spark\\\n",
    "        .read\\\n",
    "        .format('com.databricks.spark.csv')\\\n",
    "        .options(header='true', inferschema='true', delimiter=',')\\\n",
    "        .load(os.path.join('data','ds_spark.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+\n",
      "|  1|  2|  3|  4|\n",
      "+---+---+---+---+\n",
      "| 11| 22| 33| 44|\n",
      "|111|222|333|444|\n",
      "+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- 1: integer (nullable = true)\n",
      " |-- 2: integer (nullable = true)\n",
      " |-- 3: integer (nullable = true)\n",
      " |-- 4: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "inferschema를 제외하면, string으로 자동인식한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark\\\n",
    "        .read\\\n",
    "        .format('com.databricks.spark.csv')\\\n",
    "        .options(header='true', delimiter=',')\\\n",
    "        .load(os.path.join('data','ds_spark.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- 1: string (nullable = true)\n",
      " |-- 2: string (nullable = true)\n",
      " |-- 3: string (nullable = true)\n",
      " |-- 4: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### csv\n",
    "\n",
    "또는 \n",
    "```csv(\"path\")```로 직접 DataFrame으로 읽을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+\n",
      "|  1|  2|  3|  4|\n",
      "+---+---+---+---+\n",
      "| 11| 22| 33| 44|\n",
      "|111|222|333|444|\n",
      "+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark\\\n",
    "        .read\\\n",
    "        .options(header='true', inferschema='true', delimiter=',')\\\n",
    "        .csv(os.path.join('data', 'ds_spark.csv'))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.4.5 tsv 파일 읽기\n",
    "\n",
    "tsv (tab-separated values)는 **Tab으로 분리된 파일**을 말한다.\n",
    "'\\t'이 포함되어 있는 경우, 혹시 string으로 데이터타잎을 설정하기도 한다 (과거 Spark 버전에서)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "TAB은 whitespace이므로 그냥 split()을 해도 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.658985, 4.285136])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array([float(x) for x in '1.658985\t4.285136'.split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "URL로 가서 http://wiki.stat.ucla.edu/socr/index.php/SOCR_Data_Dinov_020108_HeightsWeights\n",
    "마우스로 긁어서 50행만 복사해 보자.\n",
    "18세 1993년 18세 이하 225,000 건의 키 (in), 몸무게 (lbs) 데이터이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data/ds_spark_heightweight.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/ds_spark_heightweight.txt\n",
    "1\t65.78\t112.99\n",
    "2\t71.52\t136.49\n",
    "3\t69.40\t153.03\n",
    "4\t68.22\t142.34\n",
    "5\t67.79\t144.30\n",
    "6\t68.70\t123.30\n",
    "7\t69.80\t141.49\n",
    "8\t70.01\t136.46\n",
    "9\t67.90\t112.37\n",
    "10\t66.78\t120.67\n",
    "11\t66.49\t127.45\n",
    "12\t67.62\t114.14\n",
    "13\t68.30\t125.61\n",
    "14\t67.12\t122.46\n",
    "15\t68.28\t116.09\n",
    "16\t71.09\t140.00\n",
    "17\t66.46\t129.50\n",
    "18\t68.65\t142.97\n",
    "19\t71.23\t137.90\n",
    "20\t67.13\t124.04\n",
    "21\t67.83\t141.28\n",
    "22\t68.88\t143.54\n",
    "23\t63.48\t97.90\n",
    "24\t68.42\t129.50\n",
    "25\t67.63\t141.85\n",
    "26\t67.21\t129.72\n",
    "27\t70.84\t142.42\n",
    "28\t67.49\t131.55\n",
    "29\t66.53\t108.33\n",
    "30\t65.44\t113.89\n",
    "31\t69.52\t103.30\n",
    "32\t65.81\t120.75\n",
    "33\t67.82\t125.79\n",
    "34\t70.60\t136.22\n",
    "35\t71.80\t140.10\n",
    "36\t69.21\t128.75\n",
    "37\t66.80\t141.80\n",
    "38\t67.66\t121.23\n",
    "39\t67.81\t131.35\n",
    "40\t64.05\t106.71\n",
    "41\t68.57\t124.36\n",
    "42\t65.18\t124.86\n",
    "43\t69.66\t139.67\n",
    "44\t67.97\t137.37\n",
    "45\t65.98\t106.45\n",
    "46\t68.67\t128.76\n",
    "47\t66.88\t145.68\n",
    "48\t67.70\t116.82\n",
    "49\t69.82\t143.62\n",
    "50\t69.09\t134.93"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### RDD\n",
    "\n",
    "우선 RDD로 읽어보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "_tRdd=spark.sparkContext\\\n",
    "    .textFile(os.path.join('data','ds_spark_heightweight.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "tsv는 '\\t'로 분리해도 되고, split() 함수는 <TAB>을 포함한 whitespace로 분할하게 되므로 그냥 두어도 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tRdd=rdd.map(lambda x:x.split('\\t'))\n",
    "_tRddSplitted = _tRdd.map(lambda x:x.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 형변환\n",
    "\n",
    "위 tsv 파일에서 생성한 RDD를 탭으로 분리하면서, ```float()```로 형변환을 해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.0, 65.78, 112.99]]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import numpy as np\n",
    "#myRdd=rdd.map(lambda line:np.array([float(x) for x in line.split('\\t')]))\n",
    "tRdd=_tRdd.map(lambda line:[float(x) for x in line.split('\\t')])\n",
    "tRdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### schema 설정\n",
    "\n",
    "schema를 **자동으로 설정하면, string**으로 읽혀진다.\n",
    "schema를 설정한다고 해도, string -> integer, double로 형변환은 이루어지지 않는다.\n",
    "string의 **형변환을 명시적**으로 해주어야 한다.\n",
    "\n",
    "```python\n",
    "mySchema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"weight\", DoubleType(), True),\n",
    "    StructField(\"height\", DoubleType(), True)\n",
    "])\n",
    "myDf=spark.createDataFrame(myRdd, mySchema)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### DataFrame 생성\n",
    "\n",
    "위 tRdd로부터 컬럼명을 주어 DataFrame을 생성해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "tDfNamed = spark.createDataFrame(tRdd, [\"id\",\"weight\",\"height\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: double (nullable = true)\n",
      " |-- weight: double (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tDfNamed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1.0, weight=65.78, height=112.99)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tDfNamed.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 컬럼을 split으로 분할\n",
    "\n",
    "text() 함수를 이용해서 파일을 읽을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "tDftxt = spark.read.text(os.path.join('data','ds_spark_heightweight.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<TAB>으로 분리되지 못해 전체를 변수명 value, 타입은 string으로 읽고 있어, 이를 분리해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tDftxt.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "pyspark.sql.functions은 함수이므로, import할 경우\n",
    "\n",
    "* ```import pyspark.sql.functions.split``` 이렇게 하지 않고, \n",
    "* ```from pyspark.sql.functions import split``` 이렇게 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split\n",
    "\n",
    "split_col = split(tDftxt['value'], '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "분리된 컬럼은 getItem() 함수로 가져와서 각 각 weight, height 컬럼이 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'split(value, \t, -1)[1]'>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_col.getItem(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "tDftxt = tDftxt.withColumn('weight', split_col.getItem(1))\n",
    "tDftxt = tDftxt.withColumn('height', split_col.getItem(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------+------+\n",
      "|          value|weight|height|\n",
      "+---------------+------+------+\n",
      "| 1\t65.78\t112.99| 65.78|112.99|\n",
      "| 2\t71.52\t136.49| 71.52|136.49|\n",
      "| 3\t69.40\t153.03| 69.40|153.03|\n",
      "| 4\t68.22\t142.34| 68.22|142.34|\n",
      "| 5\t67.79\t144.30| 67.79|144.30|\n",
      "| 6\t68.70\t123.30| 68.70|123.30|\n",
      "| 7\t69.80\t141.49| 69.80|141.49|\n",
      "| 8\t70.01\t136.46| 70.01|136.46|\n",
      "| 9\t67.90\t112.37| 67.90|112.37|\n",
      "|10\t66.78\t120.67| 66.78|120.67|\n",
      "|11\t66.49\t127.45| 66.49|127.45|\n",
      "|12\t67.62\t114.14| 67.62|114.14|\n",
      "|13\t68.30\t125.61| 68.30|125.61|\n",
      "|14\t67.12\t122.46| 67.12|122.46|\n",
      "|15\t68.28\t116.09| 68.28|116.09|\n",
      "|16\t71.09\t140.00| 71.09|140.00|\n",
      "|17\t66.46\t129.50| 66.46|129.50|\n",
      "|18\t68.65\t142.97| 68.65|142.97|\n",
      "|19\t71.23\t137.90| 71.23|137.90|\n",
      "|20\t67.13\t124.04| 67.13|124.04|\n",
      "+---------------+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tDftxt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### csv 함수로 tsv 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n",
      "|_c0|  _c1|   _c2|\n",
      "+---+-----+------+\n",
      "|  1|65.78|112.99|\n",
      "|  2|71.52|136.49|\n",
      "|  3| 69.4|153.03|\n",
      "|  4|68.22|142.34|\n",
      "|  5|67.79| 144.3|\n",
      "|  6| 68.7| 123.3|\n",
      "|  7| 69.8|141.49|\n",
      "|  8|70.01|136.46|\n",
      "|  9| 67.9|112.37|\n",
      "| 10|66.78|120.67|\n",
      "| 11|66.49|127.45|\n",
      "| 12|67.62|114.14|\n",
      "| 13| 68.3|125.61|\n",
      "| 14|67.12|122.46|\n",
      "| 15|68.28|116.09|\n",
      "| 16|71.09| 140.0|\n",
      "| 17|66.46| 129.5|\n",
      "| 18|68.65|142.97|\n",
      "| 19|71.23| 137.9|\n",
      "| 20|67.13|124.04|\n",
      "+---+-----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tDf = spark\\\n",
    "    .read\\\n",
    "    .options(header='false', inferschema='true', delimiter='\\t')\\\n",
    "    .csv(os.path.join('data', 'ds_spark_heightweight.txt'))\n",
    "tDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.4.6 JSON 파일에서 생성\n",
    "\n",
    "#### JSON\n",
    "JSON은 JavaScript Object Notation, 즉 자바스크립트에서 사용되는 표기법. 사람이 읽을 수 있는 텍스트로 표기하며, key-value 쌍으로 되어 있다.\n",
    "현재 널리 쓰이고 있어 XML 대용으로 널리 쓰이고 있다.\n",
    "Spark example 폴더에 있는 ```people.json``` JSON 파일이다.\n",
    "\n",
    "```python\n",
    "{\"name\":\"Michael\"}\n",
    "{\"name\":\"Andy\", \"age\":30}\n",
    "{\"name\":\"Justin\", \"age\":19}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 트위터 데이터\n",
    "\n",
    "트위터 데이터는 JSON 형식을 가지고 있다.\n",
    "아래는 트윗 1개의 샘플이다.\n",
    "실제 트윗 데이터를 구할 수 없다면 아래 샘플을 파일로 저장한 후 사용하면 된다.\n",
    "\n",
    "```python\n",
    "{\"contributors\": null, \"truncated\": false, \"text\": \"RT @soompi: #SEVENTEEN’s Mingyu, Jin Se Yeon, And Leeteuk To MC For 2016 Super Seoul Dream Concert \\nhttps://t.co/1XRSaRBbE0 https://t.co/fi…\", \"is_quote_status\": false, \"in_reply_to_status_id\": null, \"id\": 801657325836763136, \"favorite_count\": 0, \"entities\": {\"symbols\": [], \"user_mentions\": [{\"id\": 17659206, \"indices\": [3, 10], \"id_str\": \"17659206\", \"screen_name\": \"soompi\", \"name\": \"Soompi\"}], \"hashtags\": [{\"indices\": [12, 22], \"text\": \"SEVENTEEN\"}], \"urls\": [{\"url\": \"https://t.co/1XRSaRBbE0\", \"indices\": [100, 123], \"expanded_url\": \"http://www.soompi.com/2016/11/20/seventeens-mingyu-jin-se-yeon-leeteuk-mc-dream-concert/\", \"display_url\": \"soompi.com/2016/11/20/sev…\"}]}, \"retweeted\": false, \"coordinates\": null, \"source\": \"<a href=\\\"http://twitter.com/download/android\\\" rel=\\\"nofollow\\\">Twitter for Android</a>\", \"in_reply_to_screen_name\": null, \"in_reply_to_user_id\": null, \"retweet_count\": 1487, \"id_str\": \"801657325836763136\", \"favorited\": false, \"retweeted_status\": {\"contributors\": null, \"truncated\": false, \"text\": \"#SEVENTEEN’s Mingyu, Jin Se Yeon, And Leeteuk To MC For 2016 Super Seoul Dream Concert \\nhttps://t.co/1XRSaRBbE0 https://t.co/fifXHpF8or\", \"is_quote_status\": false, \"in_reply_to_status_id\": null, \"id\": 800593781586132993, \"favorite_count\": 1649, \"entities\": {\"symbols\": [], \"user_mentions\": [], \"hashtags\": [{\"indices\": [0, 10], \"text\": \"SEVENTEEN\"}], \"urls\": [{\"url\": \"https://t.co/1XRSaRBbE0\", \"indices\": [88, 111], \"expanded_url\": \"http://www.soompi.com/2016/11/20/seventeens-mingyu-jin-se-yeon-leeteuk-mc-dream-concert/\", \"display_url\": \"soompi.com/2016/11/20/sev…\"}], \"media\": [{\"expanded_url\": \"https://twitter.com/soompi/status/800593781586132993/photo/1\", \"display_url\": \"pic.twitter.com/fifXHpF8or\", \"url\": \"https://t.co/fifXHpF8or\", \"media_url_https\": \"https://pbs.twimg.com/media/CxxHMk8UsAA4cUT.jpg\", \"id_str\": \"800593115165798400\", \"sizes\": {\"small\": {\"h\": 382, \"resize\": \"fit\", \"w\": 680}, \"large\": {\"h\": 449, \"resize\": \"fit\", \"w\": 800}, \"medium\": {\"h\": 449, \"resize\": \"fit\", \"w\": 800}, \"thumb\": {\"h\": 150, \"resize\": \"crop\", \"w\": 150}}, \"indices\": [112, 135], \"type\": \"photo\", \"id\": 800593115165798400, \"media_url\": \"http://pbs.twimg.com/media/CxxHMk8UsAA4cUT.jpg\"}]}, \"retweeted\": false, \"coordinates\": null, \"source\": \"<a href=\\\"https://about.twitter.com/products/tweetdeck\\\" rel=\\\"nofollow\\\">TweetDeck</a>\", \"in_reply_to_screen_name\": null, \"in_reply_to_user_id\": null, \"retweet_count\": 1487, \"id_str\": \"800593781586132993\", \"favorited\": false, \"user\": {\"follow_request_sent\": false, \"has_extended_profile\": true, \"profile_use_background_image\": true, \"default_profile_image\": false, \"id\": 17659206, \"profile_background_image_url_https\": \"https://pbs.twimg.com/profile_background_images/699864769/1cdde0a85f5c0a994ae1fb06d545a5ec.png\", \"verified\": true, \"translator_type\": \"none\", \"profile_text_color\": \"999999\", \"profile_image_url_https\": \"https://pbs.twimg.com/profile_images/792117259489583104/4khJk3zz_normal.jpg\", \"profile_sidebar_fill_color\": \"000000\", \"entities\": {\"url\": {\"urls\": [{\"url\": \"http://t.co/3evT80UlR9\", \"indices\": [0, 22], \"expanded_url\": \"http://www.soompi.com\", \"display_url\": \"soompi.com\"}]}, \"description\": {\"urls\": []}}, \"followers_count\": 987867, \"profile_sidebar_border_color\": \"000000\", \"id_str\": \"17659206\", \"profile_background_color\": \"1E1E1E\", \"listed_count\": 3982, \"is_translation_enabled\": true, \"utc_offset\": -28800, \"statuses_count\": 80038, \"description\": \"The original K-pop community. We take gifs, OTPs, and reporting on your bias' fashion choices seriously. But not rumors. Ain't nobody got time for that.\", \"friends_count\": 3532, \"location\": \"Worldwide\", \"profile_link_color\": \"31B6F4\", \"profile_image_url\": \"http://pbs.twimg.com/profile_images/792117259489583104/4khJk3zz_normal.jpg\", \"following\": false, \"geo_enabled\": false, \"profile_banner_url\": \"https://pbs.twimg.com/profile_banners/17659206/1478803767\", \"profile_background_image_url\": \"http://pbs.twimg.com/profile_background_images/699864769/1cdde0a85f5c0a994ae1fb06d545a5ec.png\", \"screen_name\": \"soompi\", \"lang\": \"en\", \"profile_background_tile\": true, \"favourites_count\": 1493, \"name\": \"Soompi\", \"notifications\": false, \"url\": \"http://t.co/3evT80UlR9\", \"created_at\": \"Wed Nov 26 20:48:27 +0000 2008\", \"contributors_enabled\": false, \"time_zone\": \"Pacific Time (US & Canada)\", \"protected\": false, \"default_profile\": false, \"is_translator\": false}, \"geo\": null, \"in_reply_to_user_id_str\": null, \"possibly_sensitive\": false, \"lang\": \"en\", \"created_at\": \"Mon Nov 21 06:56:46 +0000 2016\", \"in_reply_to_status_id_str\": null, \"place\": null, \"extended_entities\": {\"media\": [{\"expanded_url\": \"https://twitter.com/soompi/status/800593781586132993/photo/1\", \"display_url\": \"pic.twitter.com/fifXHpF8or\", \"url\": \"https://t.co/fifXHpF8or\", \"media_url_https\": \"https://pbs.twimg.com/media/CxxHMk8UsAA4cUT.jpg\", \"id_str\": \"800593115165798400\", \"sizes\": {\"small\": {\"h\": 382, \"resize\": \"fit\", \"w\": 680}, \"large\": {\"h\": 449, \"resize\": \"fit\", \"w\": 800}, \"medium\": {\"h\": 449, \"resize\": \"fit\", \"w\": 800}, \"thumb\": {\"h\": 150, \"resize\": \"crop\", \"w\": 150}}, \"indices\": [112, 135], \"type\": \"photo\", \"id\": 800593115165798400, \"media_url\": \"http://pbs.twimg.com/media/CxxHMk8UsAA4cUT.jpg\"}]}, \"metadata\": {\"iso_language_code\": \"en\", \"result_type\": \"recent\"}}, \"user\": {\"follow_request_sent\": false, \"has_extended_profile\": false, \"profile_use_background_image\": true, \"default_profile_image\": true, \"id\": 791090169818521600, \"profile_background_image_url_https\": null, \"verified\": false, \"translator_type\": \"none\", \"profile_text_color\": \"333333\", \"profile_image_url_https\": \"https://abs.twimg.com/sticky/default_profile_images/default_profile_6_normal.png\", \"profile_sidebar_fill_color\": \"DDEEF6\", \"entities\": {\"description\": {\"urls\": []}}, \"followers_count\": 0, \"profile_sidebar_border_color\": \"C0DEED\", \"id_str\": \"791090169818521600\", \"profile_background_color\": \"F5F8FA\", \"listed_count\": 0, \"is_translation_enabled\": false, \"utc_offset\": null, \"statuses_count\": 96, \"description\": \"\", \"friends_count\": 7, \"location\": \"\", \"profile_link_color\": \"1DA1F2\", \"profile_image_url\": \"http://abs.twimg.com/sticky/default_profile_images/default_profile_6_normal.png\", \"following\": false, \"geo_enabled\": false, \"profile_background_image_url\": null, \"screen_name\": \"enriquesanq\", \"lang\": \"es\", \"profile_background_tile\": false, \"favourites_count\": 161, \"name\": \"Enrique santos\", \"notifications\": false, \"url\": null, \"created_at\": \"Wed Oct 26 01:32:49 +0000 2016\", \"contributors_enabled\": false, \"time_zone\": null, \"protected\": false, \"default_profile\": true, \"is_translator\": false}, \"geo\": null, \"in_reply_to_user_id_str\": null, \"possibly_sensitive\": false, \"lang\": \"en\", \"created_at\": \"Thu Nov 24 05:22:55 +0000 2016\", \"in_reply_to_status_id_str\": null, \"place\": null, \"metadata\": {\"iso_language_code\": \"en\", \"result_type\": \"recent\"}}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/ds_twitter_seoul_3.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/ds_twitter_seoul_3.json\n",
    "{\"contributors\": null, \"truncated\": false, \"text\": \"RT @soompi: #SEVENTEEN’s Mingyu, Jin Se Yeon, And Leeteuk To MC For 2016 Super Seoul Dream Concert \\nhttps://t.co/1XRSaRBbE0 https://t.co/fi…\", \"is_quote_status\": false, \"in_reply_to_status_id\": null, \"id\": 801657325836763136, \"favorite_count\": 0, \"entities\": {\"symbols\": [], \"user_mentions\": [{\"id\": 17659206, \"indices\": [3, 10], \"id_str\": \"17659206\", \"screen_name\": \"soompi\", \"name\": \"Soompi\"}], \"hashtags\": [{\"indices\": [12, 22], \"text\": \"SEVENTEEN\"}], \"urls\": [{\"url\": \"https://t.co/1XRSaRBbE0\", \"indices\": [100, 123], \"expanded_url\": \"http://www.soompi.com/2016/11/20/seventeens-mingyu-jin-se-yeon-leeteuk-mc-dream-concert/\", \"display_url\": \"soompi.com/2016/11/20/sev…\"}]}, \"retweeted\": false, \"coordinates\": null, \"source\": \"<a href=\\\"http://twitter.com/download/android\\\" rel=\\\"nofollow\\\">Twitter for Android</a>\", \"in_reply_to_screen_name\": null, \"in_reply_to_user_id\": null, \"retweet_count\": 1487, \"id_str\": \"801657325836763136\", \"favorited\": false, \"retweeted_status\": {\"contributors\": null, \"truncated\": false, \"text\": \"#SEVENTEEN’s Mingyu, Jin Se Yeon, And Leeteuk To MC For 2016 Super Seoul Dream Concert \\nhttps://t.co/1XRSaRBbE0 https://t.co/fifXHpF8or\", \"is_quote_status\": false, \"in_reply_to_status_id\": null, \"id\": 800593781586132993, \"favorite_count\": 1649, \"entities\": {\"symbols\": [], \"user_mentions\": [], \"hashtags\": [{\"indices\": [0, 10], \"text\": \"SEVENTEEN\"}], \"urls\": [{\"url\": \"https://t.co/1XRSaRBbE0\", \"indices\": [88, 111], \"expanded_url\": \"http://www.soompi.com/2016/11/20/seventeens-mingyu-jin-se-yeon-leeteuk-mc-dream-concert/\", \"display_url\": \"soompi.com/2016/11/20/sev…\"}], \"media\": [{\"expanded_url\": \"https://twitter.com/soompi/status/800593781586132993/photo/1\", \"display_url\": \"pic.twitter.com/fifXHpF8or\", \"url\": \"https://t.co/fifXHpF8or\", \"media_url_https\": \"https://pbs.twimg.com/media/CxxHMk8UsAA4cUT.jpg\", \"id_str\": \"800593115165798400\", \"sizes\": {\"small\": {\"h\": 382, \"resize\": \"fit\", \"w\": 680}, \"large\": {\"h\": 449, \"resize\": \"fit\", \"w\": 800}, \"medium\": {\"h\": 449, \"resize\": \"fit\", \"w\": 800}, \"thumb\": {\"h\": 150, \"resize\": \"crop\", \"w\": 150}}, \"indices\": [112, 135], \"type\": \"photo\", \"id\": 800593115165798400, \"media_url\": \"http://pbs.twimg.com/media/CxxHMk8UsAA4cUT.jpg\"}]}, \"retweeted\": false, \"coordinates\": null, \"source\": \"<a href=\\\"https://about.twitter.com/products/tweetdeck\\\" rel=\\\"nofollow\\\">TweetDeck</a>\", \"in_reply_to_screen_name\": null, \"in_reply_to_user_id\": null, \"retweet_count\": 1487, \"id_str\": \"800593781586132993\", \"favorited\": false, \"user\": {\"follow_request_sent\": false, \"has_extended_profile\": true, \"profile_use_background_image\": true, \"default_profile_image\": false, \"id\": 17659206, \"profile_background_image_url_https\": \"https://pbs.twimg.com/profile_background_images/699864769/1cdde0a85f5c0a994ae1fb06d545a5ec.png\", \"verified\": true, \"translator_type\": \"none\", \"profile_text_color\": \"999999\", \"profile_image_url_https\": \"https://pbs.twimg.com/profile_images/792117259489583104/4khJk3zz_normal.jpg\", \"profile_sidebar_fill_color\": \"000000\", \"entities\": {\"url\": {\"urls\": [{\"url\": \"http://t.co/3evT80UlR9\", \"indices\": [0, 22], \"expanded_url\": \"http://www.soompi.com\", \"display_url\": \"soompi.com\"}]}, \"description\": {\"urls\": []}}, \"followers_count\": 987867, \"profile_sidebar_border_color\": \"000000\", \"id_str\": \"17659206\", \"profile_background_color\": \"1E1E1E\", \"listed_count\": 3982, \"is_translation_enabled\": true, \"utc_offset\": -28800, \"statuses_count\": 80038, \"description\": \"The original K-pop community. We take gifs, OTPs, and reporting on your bias' fashion choices seriously. But not rumors. Ain't nobody got time for that.\", \"friends_count\": 3532, \"location\": \"Worldwide\", \"profile_link_color\": \"31B6F4\", \"profile_image_url\": \"http://pbs.twimg.com/profile_images/792117259489583104/4khJk3zz_normal.jpg\", \"following\": false, \"geo_enabled\": false, \"profile_banner_url\": \"https://pbs.twimg.com/profile_banners/17659206/1478803767\", \"profile_background_image_url\": \"http://pbs.twimg.com/profile_background_images/699864769/1cdde0a85f5c0a994ae1fb06d545a5ec.png\", \"screen_name\": \"soompi\", \"lang\": \"en\", \"profile_background_tile\": true, \"favourites_count\": 1493, \"name\": \"Soompi\", \"notifications\": false, \"url\": \"http://t.co/3evT80UlR9\", \"created_at\": \"Wed Nov 26 20:48:27 +0000 2008\", \"contributors_enabled\": false, \"time_zone\": \"Pacific Time (US & Canada)\", \"protected\": false, \"default_profile\": false, \"is_translator\": false}, \"geo\": null, \"in_reply_to_user_id_str\": null, \"possibly_sensitive\": false, \"lang\": \"en\", \"created_at\": \"Mon Nov 21 06:56:46 +0000 2016\", \"in_reply_to_status_id_str\": null, \"place\": null, \"extended_entities\": {\"media\": [{\"expanded_url\": \"https://twitter.com/soompi/status/800593781586132993/photo/1\", \"display_url\": \"pic.twitter.com/fifXHpF8or\", \"url\": \"https://t.co/fifXHpF8or\", \"media_url_https\": \"https://pbs.twimg.com/media/CxxHMk8UsAA4cUT.jpg\", \"id_str\": \"800593115165798400\", \"sizes\": {\"small\": {\"h\": 382, \"resize\": \"fit\", \"w\": 680}, \"large\": {\"h\": 449, \"resize\": \"fit\", \"w\": 800}, \"medium\": {\"h\": 449, \"resize\": \"fit\", \"w\": 800}, \"thumb\": {\"h\": 150, \"resize\": \"crop\", \"w\": 150}}, \"indices\": [112, 135], \"type\": \"photo\", \"id\": 800593115165798400, \"media_url\": \"http://pbs.twimg.com/media/CxxHMk8UsAA4cUT.jpg\"}]}, \"metadata\": {\"iso_language_code\": \"en\", \"result_type\": \"recent\"}}, \"user\": {\"follow_request_sent\": false, \"has_extended_profile\": false, \"profile_use_background_image\": true, \"default_profile_image\": true, \"id\": 791090169818521600, \"profile_background_image_url_https\": null, \"verified\": false, \"translator_type\": \"none\", \"profile_text_color\": \"333333\", \"profile_image_url_https\": \"https://abs.twimg.com/sticky/default_profile_images/default_profile_6_normal.png\", \"profile_sidebar_fill_color\": \"DDEEF6\", \"entities\": {\"description\": {\"urls\": []}}, \"followers_count\": 0, \"profile_sidebar_border_color\": \"C0DEED\", \"id_str\": \"791090169818521600\", \"profile_background_color\": \"F5F8FA\", \"listed_count\": 0, \"is_translation_enabled\": false, \"utc_offset\": null, \"statuses_count\": 96, \"description\": \"\", \"friends_count\": 7, \"location\": \"\", \"profile_link_color\": \"1DA1F2\", \"profile_image_url\": \"http://abs.twimg.com/sticky/default_profile_images/default_profile_6_normal.png\", \"following\": false, \"geo_enabled\": false, \"profile_background_image_url\": null, \"screen_name\": \"enriquesanq\", \"lang\": \"es\", \"profile_background_tile\": false, \"favourites_count\": 161, \"name\": \"Enrique santos\", \"notifications\": false, \"url\": null, \"created_at\": \"Wed Oct 26 01:32:49 +0000 2016\", \"contributors_enabled\": false, \"time_zone\": null, \"protected\": false, \"default_profile\": true, \"is_translator\": false}, \"geo\": null, \"in_reply_to_user_id_str\": null, \"possibly_sensitive\": false, \"lang\": \"en\", \"created_at\": \"Thu Nov 24 05:22:55 +0000 2016\", \"in_reply_to_status_id_str\": null, \"place\": null, \"metadata\": {\"iso_language_code\": \"en\", \"result_type\": \"recent\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 파일에서 트윗 읽기\n",
    "\n",
    "그러나 JSON파일은 **JSON이 아니라 문자열**이다. 파일에서 읽은 후 **JSON으로 변환**을 해야 한다.\n",
    "트윗은 '\\n'으로 하나씩 구분되어 있고 ```readlines()``` 함수로 전체를 읽을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "_jfname=os.path.join('src','ds_twitter_seoul_3.json')\n",
    "with open(_jfname, 'rb') as f:\n",
    "    data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data_json_str = json.loads(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data_json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_json_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pandas에서  트윗 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bytes"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "위 Tweet데이터는 json형식이다. 따라서 list 구조로 만들어 주려면, 앞 뒤로 대괄호 ```[ ]```를 넣고, 각 tweet은 컴마로 연결한다.\n",
    "```join()``` 함수는 인자를 구분자 \",\"로 병합한다.\n",
    "\n",
    "```python\n",
    "\",\".join([\"A\", \"B\", \"C\"]) # A,B,C\n",
    "```\n",
    "\n",
    "data는 bytes이고, 이를 join()하는 함수는 str을 대상으로 하기 때문에, 문자열에 ```b```를 붙여 bytes로 변환한 후 연결한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_json_str = b\"[\" + b','.join(data) + b\"]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아직 JSON으로 변환이 되지 않았고, 문자열 전체 길이를 알아 보자. 파일의 크기를 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6910"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_json_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 Pandas로 읽어 보자. Pandas 라이브러리에서 제공하는 read_json()함수를 사용한다.\n",
    "\n",
    "read_json()에 경로를 넘겨주어도 되지 않나??????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tweetPd = pd.read_json(data_json_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shape()은 행과 열의 갯수를 알려준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 26)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetPd.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "count() 함수는 행의 갯수를 나타내는데, 숫자가 서로 다른 것은 비워있는 경우가 서로 다르기 때문이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contributors                 0\n",
      "coordinates                  0\n",
      "created_at                   1\n",
      "entities                     1\n",
      "favorite_count               1\n",
      "favorited                    1\n",
      "geo                          0\n",
      "id                           1\n",
      "id_str                       1\n",
      "in_reply_to_screen_name      0\n",
      "in_reply_to_status_id        0\n",
      "in_reply_to_status_id_str    0\n",
      "in_reply_to_user_id          0\n",
      "in_reply_to_user_id_str      0\n",
      "is_quote_status              1\n",
      "lang                         1\n",
      "metadata                     1\n",
      "place                        0\n",
      "possibly_sensitive           1\n",
      "retweet_count                1\n",
      "retweeted                    1\n",
      "retweeted_status             1\n",
      "source                       1\n",
      "text                         1\n",
      "truncated                    1\n",
      "user                         1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print (tweetPd.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "컬럼 'id'를 10개만 읽어 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    801657325836763136\n",
       "Name: id, dtype: int64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetPd['id'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataFrame에서 트윗 읽기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "jfile= os.path.join('src','ds_twitter_seoul_3.json')\n",
    "\n",
    "tweetDf= spark.read.json(jfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- contributors: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- entities: struct (nullable = true)\n",
      " |    |-- hashtags: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- indices: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |-- text: string (nullable = true)\n",
      " |    |-- symbols: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |    |-- urls: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- display_url: string (nullable = true)\n",
      " |    |    |    |-- expanded_url: string (nullable = true)\n",
      " |    |    |    |-- indices: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |-- url: string (nullable = true)\n",
      " |    |-- user_mentions: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- id: long (nullable = true)\n",
      " |    |    |    |-- id_str: string (nullable = true)\n",
      " |    |    |    |-- indices: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |-- screen_name: string (nullable = true)\n",
      " |-- favorite_count: long (nullable = true)\n",
      " |-- favorited: boolean (nullable = true)\n",
      " |-- geo: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- id_str: string (nullable = true)\n",
      " |-- in_reply_to_screen_name: string (nullable = true)\n",
      " |-- in_reply_to_status_id: string (nullable = true)\n",
      " |-- in_reply_to_status_id_str: string (nullable = true)\n",
      " |-- in_reply_to_user_id: string (nullable = true)\n",
      " |-- in_reply_to_user_id_str: string (nullable = true)\n",
      " |-- is_quote_status: boolean (nullable = true)\n",
      " |-- lang: string (nullable = true)\n",
      " |-- metadata: struct (nullable = true)\n",
      " |    |-- iso_language_code: string (nullable = true)\n",
      " |    |-- result_type: string (nullable = true)\n",
      " |-- place: string (nullable = true)\n",
      " |-- possibly_sensitive: boolean (nullable = true)\n",
      " |-- retweet_count: long (nullable = true)\n",
      " |-- retweeted: boolean (nullable = true)\n",
      " |-- retweeted_status: struct (nullable = true)\n",
      " |    |-- contributors: string (nullable = true)\n",
      " |    |-- coordinates: string (nullable = true)\n",
      " |    |-- created_at: string (nullable = true)\n",
      " |    |-- entities: struct (nullable = true)\n",
      " |    |    |-- hashtags: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- indices: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |    |-- text: string (nullable = true)\n",
      " |    |    |-- media: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- display_url: string (nullable = true)\n",
      " |    |    |    |    |-- expanded_url: string (nullable = true)\n",
      " |    |    |    |    |-- id: long (nullable = true)\n",
      " |    |    |    |    |-- id_str: string (nullable = true)\n",
      " |    |    |    |    |-- indices: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |    |-- media_url: string (nullable = true)\n",
      " |    |    |    |    |-- media_url_https: string (nullable = true)\n",
      " |    |    |    |    |-- sizes: struct (nullable = true)\n",
      " |    |    |    |    |    |-- large: struct (nullable = true)\n",
      " |    |    |    |    |    |    |-- h: long (nullable = true)\n",
      " |    |    |    |    |    |    |-- resize: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- w: long (nullable = true)\n",
      " |    |    |    |    |    |-- medium: struct (nullable = true)\n",
      " |    |    |    |    |    |    |-- h: long (nullable = true)\n",
      " |    |    |    |    |    |    |-- resize: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- w: long (nullable = true)\n",
      " |    |    |    |    |    |-- small: struct (nullable = true)\n",
      " |    |    |    |    |    |    |-- h: long (nullable = true)\n",
      " |    |    |    |    |    |    |-- resize: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- w: long (nullable = true)\n",
      " |    |    |    |    |    |-- thumb: struct (nullable = true)\n",
      " |    |    |    |    |    |    |-- h: long (nullable = true)\n",
      " |    |    |    |    |    |    |-- resize: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- w: long (nullable = true)\n",
      " |    |    |    |    |-- type: string (nullable = true)\n",
      " |    |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |-- symbols: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |-- urls: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- display_url: string (nullable = true)\n",
      " |    |    |    |    |-- expanded_url: string (nullable = true)\n",
      " |    |    |    |    |-- indices: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |-- user_mentions: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      " |    |-- extended_entities: struct (nullable = true)\n",
      " |    |    |-- media: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- display_url: string (nullable = true)\n",
      " |    |    |    |    |-- expanded_url: string (nullable = true)\n",
      " |    |    |    |    |-- id: long (nullable = true)\n",
      " |    |    |    |    |-- id_str: string (nullable = true)\n",
      " |    |    |    |    |-- indices: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |    |-- media_url: string (nullable = true)\n",
      " |    |    |    |    |-- media_url_https: string (nullable = true)\n",
      " |    |    |    |    |-- sizes: struct (nullable = true)\n",
      " |    |    |    |    |    |-- large: struct (nullable = true)\n",
      " |    |    |    |    |    |    |-- h: long (nullable = true)\n",
      " |    |    |    |    |    |    |-- resize: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- w: long (nullable = true)\n",
      " |    |    |    |    |    |-- medium: struct (nullable = true)\n",
      " |    |    |    |    |    |    |-- h: long (nullable = true)\n",
      " |    |    |    |    |    |    |-- resize: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- w: long (nullable = true)\n",
      " |    |    |    |    |    |-- small: struct (nullable = true)\n",
      " |    |    |    |    |    |    |-- h: long (nullable = true)\n",
      " |    |    |    |    |    |    |-- resize: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- w: long (nullable = true)\n",
      " |    |    |    |    |    |-- thumb: struct (nullable = true)\n",
      " |    |    |    |    |    |    |-- h: long (nullable = true)\n",
      " |    |    |    |    |    |    |-- resize: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- w: long (nullable = true)\n",
      " |    |    |    |    |-- type: string (nullable = true)\n",
      " |    |    |    |    |-- url: string (nullable = true)\n",
      " |    |-- favorite_count: long (nullable = true)\n",
      " |    |-- favorited: boolean (nullable = true)\n",
      " |    |-- geo: string (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- id_str: string (nullable = true)\n",
      " |    |-- in_reply_to_screen_name: string (nullable = true)\n",
      " |    |-- in_reply_to_status_id: string (nullable = true)\n",
      " |    |-- in_reply_to_status_id_str: string (nullable = true)\n",
      " |    |-- in_reply_to_user_id: string (nullable = true)\n",
      " |    |-- in_reply_to_user_id_str: string (nullable = true)\n",
      " |    |-- is_quote_status: boolean (nullable = true)\n",
      " |    |-- lang: string (nullable = true)\n",
      " |    |-- metadata: struct (nullable = true)\n",
      " |    |    |-- iso_language_code: string (nullable = true)\n",
      " |    |    |-- result_type: string (nullable = true)\n",
      " |    |-- place: string (nullable = true)\n",
      " |    |-- possibly_sensitive: boolean (nullable = true)\n",
      " |    |-- retweet_count: long (nullable = true)\n",
      " |    |-- retweeted: boolean (nullable = true)\n",
      " |    |-- source: string (nullable = true)\n",
      " |    |-- text: string (nullable = true)\n",
      " |    |-- truncated: boolean (nullable = true)\n",
      " |    |-- user: struct (nullable = true)\n",
      " |    |    |-- contributors_enabled: boolean (nullable = true)\n",
      " |    |    |-- created_at: string (nullable = true)\n",
      " |    |    |-- default_profile: boolean (nullable = true)\n",
      " |    |    |-- default_profile_image: boolean (nullable = true)\n",
      " |    |    |-- description: string (nullable = true)\n",
      " |    |    |-- entities: struct (nullable = true)\n",
      " |    |    |    |-- description: struct (nullable = true)\n",
      " |    |    |    |    |-- urls: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |-- url: struct (nullable = true)\n",
      " |    |    |    |    |-- urls: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |-- display_url: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- expanded_url: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- indices: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |-- favourites_count: long (nullable = true)\n",
      " |    |    |-- follow_request_sent: boolean (nullable = true)\n",
      " |    |    |-- followers_count: long (nullable = true)\n",
      " |    |    |-- following: boolean (nullable = true)\n",
      " |    |    |-- friends_count: long (nullable = true)\n",
      " |    |    |-- geo_enabled: boolean (nullable = true)\n",
      " |    |    |-- has_extended_profile: boolean (nullable = true)\n",
      " |    |    |-- id: long (nullable = true)\n",
      " |    |    |-- id_str: string (nullable = true)\n",
      " |    |    |-- is_translation_enabled: boolean (nullable = true)\n",
      " |    |    |-- is_translator: boolean (nullable = true)\n",
      " |    |    |-- lang: string (nullable = true)\n",
      " |    |    |-- listed_count: long (nullable = true)\n",
      " |    |    |-- location: string (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- notifications: boolean (nullable = true)\n",
      " |    |    |-- profile_background_color: string (nullable = true)\n",
      " |    |    |-- profile_background_image_url: string (nullable = true)\n",
      " |    |    |-- profile_background_image_url_https: string (nullable = true)\n",
      " |    |    |-- profile_background_tile: boolean (nullable = true)\n",
      " |    |    |-- profile_banner_url: string (nullable = true)\n",
      " |    |    |-- profile_image_url: string (nullable = true)\n",
      " |    |    |-- profile_image_url_https: string (nullable = true)\n",
      " |    |    |-- profile_link_color: string (nullable = true)\n",
      " |    |    |-- profile_sidebar_border_color: string (nullable = true)\n",
      " |    |    |-- profile_sidebar_fill_color: string (nullable = true)\n",
      " |    |    |-- profile_text_color: string (nullable = true)\n",
      " |    |    |-- profile_use_background_image: boolean (nullable = true)\n",
      " |    |    |-- protected: boolean (nullable = true)\n",
      " |    |    |-- screen_name: string (nullable = true)\n",
      " |    |    |-- statuses_count: long (nullable = true)\n",
      " |    |    |-- time_zone: string (nullable = true)\n",
      " |    |    |-- translator_type: string (nullable = true)\n",
      " |    |    |-- url: string (nullable = true)\n",
      " |    |    |-- utc_offset: long (nullable = true)\n",
      " |    |    |-- verified: boolean (nullable = true)\n",
      " |-- source: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- truncated: boolean (nullable = true)\n",
      " |-- user: struct (nullable = true)\n",
      " |    |-- contributors_enabled: boolean (nullable = true)\n",
      " |    |-- created_at: string (nullable = true)\n",
      " |    |-- default_profile: boolean (nullable = true)\n",
      " |    |-- default_profile_image: boolean (nullable = true)\n",
      " |    |-- description: string (nullable = true)\n",
      " |    |-- entities: struct (nullable = true)\n",
      " |    |    |-- description: struct (nullable = true)\n",
      " |    |    |    |-- urls: array (nullable = true)\n",
      " |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |-- favourites_count: long (nullable = true)\n",
      " |    |-- follow_request_sent: boolean (nullable = true)\n",
      " |    |-- followers_count: long (nullable = true)\n",
      " |    |-- following: boolean (nullable = true)\n",
      " |    |-- friends_count: long (nullable = true)\n",
      " |    |-- geo_enabled: boolean (nullable = true)\n",
      " |    |-- has_extended_profile: boolean (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- id_str: string (nullable = true)\n",
      " |    |-- is_translation_enabled: boolean (nullable = true)\n",
      " |    |-- is_translator: boolean (nullable = true)\n",
      " |    |-- lang: string (nullable = true)\n",
      " |    |-- listed_count: long (nullable = true)\n",
      " |    |-- location: string (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- notifications: boolean (nullable = true)\n",
      " |    |-- profile_background_color: string (nullable = true)\n",
      " |    |-- profile_background_image_url: string (nullable = true)\n",
      " |    |-- profile_background_image_url_https: string (nullable = true)\n",
      " |    |-- profile_background_tile: boolean (nullable = true)\n",
      " |    |-- profile_image_url: string (nullable = true)\n",
      " |    |-- profile_image_url_https: string (nullable = true)\n",
      " |    |-- profile_link_color: string (nullable = true)\n",
      " |    |-- profile_sidebar_border_color: string (nullable = true)\n",
      " |    |-- profile_sidebar_fill_color: string (nullable = true)\n",
      " |    |-- profile_text_color: string (nullable = true)\n",
      " |    |-- profile_use_background_image: boolean (nullable = true)\n",
      " |    |-- protected: boolean (nullable = true)\n",
      " |    |-- screen_name: string (nullable = true)\n",
      " |    |-- statuses_count: long (nullable = true)\n",
      " |    |-- time_zone: string (nullable = true)\n",
      " |    |-- translator_type: string (nullable = true)\n",
      " |    |-- url: string (nullable = true)\n",
      " |    |-- utc_offset: string (nullable = true)\n",
      " |    |-- verified: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweetDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetDf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark DataFrame에서도 위 Pandas와 컬럼 'id'를 10개 출력할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+\n",
      "|                id|                text|\n",
      "+------------------+--------------------+\n",
      "|801657325836763136|RT @soompi: #SEVE...|\n",
      "+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweetDf.select('id', 'text').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제: 월드컵 데이터 JSON의 URL에서 DataFrame 생성\n",
    "\n",
    "url에서 데이터를 직접 읽어 DataFrame을 생성하는 방법은 지원되지 않고 있다.\n",
    "requests 라이브러리를 사용하여, url에서 데이터를 가져오고, DataFrame을 생성하여 보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### URL에서 JSON 읽기\n",
    "\n",
    "웹에서 읽는 JSON은 문자열이라는 점에 유의한다. 따라서 json()함수로 변환하는 것이 필요하다.\n",
    "```python\n",
    "[\n",
    "  {\n",
    "    \"Competition\": \"World Cup\",\n",
    "    \"Year\": 1930,\n",
    "    \"Team\": \"Argentina\",\n",
    "    \"Number\": \"\",\n",
    "    \"Position\": \"GK\",\n",
    "    \"FullName\": \"Ãngel Bossio\",\n",
    "    \"Club\": \"Club AtlÃ©tico Talleres de Remedios de Escalada\",\n",
    "    \"ClubCountry\": \"Argentina\",\n",
    "    \"DateOfBirth\": \"1905-5-5\",\n",
    "    \"IsCaptain\": false\n",
    "  },\n",
    "  {\n",
    "    \"Competition\": \"World Cup\",\n",
    "    ...\n",
    "    \"IsCaptain\": false\n",
    "  },\n",
    "  ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "r=requests.get(\"https://raw.githubusercontent.com/jokecamp/FootballData/master/World%20Cups/all-world-cup-players.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "requuest.get() 함수는 Response를 반환한다.\n",
    "이러한 Reponse에 포함된 텍스트를 사용하게 된다.\n",
    "웹에서 읽은 텍스트는 어떤 자료구조를 가졌다고 하더라도 단순 문자열이다. 텍스트를 살펴보고, 구조가 있다면 알맞게 변환해주어야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of Response:  <class 'requests.models.Response'>\n"
     ]
    }
   ],
   "source": [
    "print(\"Type of Response: \", type(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 읽은 텍스트를 json으로 읽는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc=r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> <class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print (type(wc), type(wc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wc는 아래와 같이 ```[ {...}, {...}, ...]```, 즉 배열내에 dictionary가 컴마로 연결되어 있다.\n",
    "이와 같이 파일에서 읽을 경우, 그 구조를 정확하게 이해하는 것이 중요하다.\n",
    "\n",
    "```python\n",
    "[{'Competition': 'World Cup',\n",
    "  'Year': 1930,\n",
    "  'Team': 'Argentina',\n",
    "  'Number': '',\n",
    "  'Position': 'GK',\n",
    "  'FullName': 'Ãngel Bossio',\n",
    "  'Club': 'Club AtlÃ©tico Talleres de Remedios de Escalada',\n",
    "  'ClubCountry': 'Argentina',\n",
    "  'DateOfBirth': '1905-5-5',\n",
    "  'IsCaptain': False},\n",
    " {'Competition': 'World Cup',\n",
    "  'Year': 1930,\n",
    "  'Team': 'Argentina',\n",
    "  'Number': '',\n",
    "  'Position': 'GK',\n",
    "  'FullName': 'Juan Botasso',\n",
    "  'Club': 'Quilmes AtlÃ©tico Club',\n",
    "  'ClubCountry': 'Argentina',\n",
    "  'DateOfBirth': '1908-10-23',\n",
    "  'IsCaptain': False},\n",
    " ... ]\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Competition': 'World Cup',\n",
       " 'Year': 1930,\n",
       " 'Team': 'Argentina',\n",
       " 'Number': '',\n",
       " 'Position': 'GK',\n",
       " 'FullName': 'Ãngel Bossio',\n",
       " 'Club': 'Club AtlÃ©tico Talleres de Remedios de Escalada',\n",
       " 'ClubCountry': 'Argentina',\n",
       " 'DateOfBirth': '1905-5-5',\n",
       " 'IsCaptain': False}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### DataFrame 생성\n",
    "\n",
    "위 ```wc```는 JSON을 포함하는 리스트이다.\n",
    "JSON은 key, value로 구성되어, 컬럼명을 key에서 가져올 수 있다.\n",
    "이전 버전에서는 가능했으나, 현재는 Python dict로부터 DataFrame을 생성하려면 아래와 같이 pyspark.sql.Row를 활용해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "_wcDf=spark.createDataFrame(wc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Row를 이용해서 Dictionary에서 DataFrame 생성하기\n",
    "\n",
    "앞서 살펴보았듯이, wc는 리스트이고 그 요소는 dictionary이다.\n",
    "dictionary 구조를 풀어서 Row에 넘겨주어야 한다.\n",
    "우선 Python에서 어떻게 인자를 풀어서 넘겨주는지 배워보자.\n",
    "\n",
    "인자가 복수일 경우 **리스트, dictionary와 같은 구조**로 묶게 되는데, **함수 호출하면서 풀어야할 필요가 있을 경우에 별표 연산자** ```*args```, ```**args```를 사용한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 별표 1개 ```*```: **리스트**에서 인자 풀어내기\n",
    "\n",
    "예를 들어, range() 함수는 시작과 끝을 인자로 가지는 함수이다.\n",
    "시작과 끝이라는 2개의 인자를 함수에 넘겨주어 출력해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "myList = [1, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(1,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음과 같이 정수인자 자리에 리스트를 넘겨주면 타입오류가 발생한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-106-7028d29cee9e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmyList\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# TypeError: 'list' object cannot be interpreted as an integer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'list' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "list(range(myList))  # TypeError: 'list' object cannot be interpreted as an integer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "시작, 끝 변수가 따로 없을 경우, **리스트를 넘겨주고 여기서 풀어서 하나씩 인자를 사용할 경우** ```*```를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(*myList))  # unpack args and get 1 for the start and 6 for the end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 예를 보자. f()는 복수의 인자를 받아서 하나씩 출력하는 함수이다.\n",
    "f()를 호출할 때 여러 개를 넘겨주면, 1개만 받으므로 타입오류가 발생한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "f() takes 1 positional argument but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-108-4972646694d6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"~\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: f() takes 1 positional argument but 4 were given"
     ]
    }
   ],
   "source": [
    "def f(args):\n",
    "    for i in args:\n",
    "        print(i, end=\"~\")\n",
    "\n",
    "f(0, 1, 2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "복수의 인자를 받게 될 것이라면 * 연산자를 사용한다. 그러면 복수의 인자를 풀어서 사용하게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0~1~2~3~"
     ]
    }
   ],
   "source": [
    "def f(*args):\n",
    "    for i in args:\n",
    "        print(i, end=\"~\")\n",
    "\n",
    "f(0, 1, 2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 별표 2 개 ```**```: **dictionary**에서 인자 풀어내기\n",
    "\n",
    "dictionary를 넘겨주고 여기서 key, value를 하나씩 사용할 경우 ```**```를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jsl in 2020\n"
     ]
    }
   ],
   "source": [
    "def printCapital(name, year):\n",
    "    print(f\"{name} in {year}\")\n",
    "\n",
    "myDict = {\"name\": \"jsl\", \"year\": 2020}\n",
    "printCapital(**myDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dictionary인자를 풀어서 Row에 넘겨주기\n",
    "\n",
    "반복문으로 하나의 dictionary를 가져와 ```Row()```를 생성하고 있다.\n",
    "별표 2개를 겹쳐 쓴 ```**```는 dictionary를 풀어서 Row 생성자에 넘겨주고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "wcDf = spark.createDataFrame(Row(**x) for x in wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Competition: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      " |-- Team: string (nullable = true)\n",
      " |-- Number: string (nullable = true)\n",
      " |-- Position: string (nullable = true)\n",
      " |-- FullName: string (nullable = true)\n",
      " |-- Club: string (nullable = true)\n",
      " |-- ClubCountry: string (nullable = true)\n",
      " |-- DateOfBirth: string (nullable = true)\n",
      " |-- IsCaptain: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wcDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터가 잘 읽혔는지, 일부를 출력해보자. 아래에서 보듯이, 아르헨티나 문자가 유니코드로 잘 출력이 되고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Competition='World Cup', Year=1930, Team='Argentina', Number='', Position='GK', FullName='Ãngel Bossio', Club='Club AtlÃ©tico Talleres de Remedios de Escalada', ClubCountry='Argentina', DateOfBirth='1905-5-5', IsCaptain=False)]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wcDf.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###  DataFrame의 shema 설정\n",
    "\n",
    "wcRdd에서 DataFrame을 생성하려고 하면 아래와 같이 schema를 설정할 수 있다.\n",
    "그러나 영어가 아닌 다른 나라의 유니코드 문자 또는 생년월일의 결측값 등으로 오류가 발생한다.\n",
    "자동으로 인식한 Schema가 더 낫게 인식하고 있다.\n",
    "\n",
    "```python\n",
    "from pyspark.sql.types import *\n",
    "wcSchema=StructType([\n",
    "    StructField(\"Club\", StringType(), True),\n",
    "    StructField(\"ClubCountry\", StringType(), True),\n",
    "    StructField(\"Competition\", StringType(), True),\n",
    "    StructField(\"DateOfBirth\", DateType(), True),\n",
    "    StructField(\"FullName\", StringType(), True),\n",
    "    StructField(\"IsCaptain\", BooleanType(), True),\n",
    "    StructField(\"Number\", IntegerType(), True),\n",
    "    StructField(\"Position\", StringType(), True),\n",
    "    StructField(\"Team\", StringType(), True),\n",
    "    StructField(\"Year\", IntegerType(), True)\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### RDD 생성\n",
    "\n",
    "RDD를 통해서 DataFrame 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcRdd=spark.sparkContext.parallelize(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Competition': 'World Cup',\n",
       "  'Year': 1930,\n",
       "  'Team': 'Argentina',\n",
       "  'Number': '',\n",
       "  'Position': 'GK',\n",
       "  'FullName': 'Ãngel Bossio',\n",
       "  'Club': 'Club AtlÃ©tico Talleres de Remedios de Escalada',\n",
       "  'ClubCountry': 'Argentina',\n",
       "  'DateOfBirth': '1905-5-5',\n",
       "  'IsCaptain': False}]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wcRdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Club: string (nullable = true)\n",
      " |-- ClubCountry: string (nullable = true)\n",
      " |-- Competition: string (nullable = true)\n",
      " |-- DateOfBirth: string (nullable = true)\n",
      " |-- FullName: string (nullable = true)\n",
      " |-- IsCaptain: boolean (nullable = true)\n",
      " |-- Number: string (nullable = true)\n",
      " |-- Position: string (nullable = true)\n",
      " |-- Team: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wcDfFromRdd = spark.createDataFrame(wcRdd)\n",
    "wcDfFromRdd.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Club='Club AtlÃ©tico Talleres de Remedios de Escalada', ClubCountry='Argentina', Competition='World Cup', DateOfBirth='1905-5-5', FullName='Ãngel Bossio', IsCaptain=False, Number='', Position='GK', Team='Argentina', Year=1930)]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wcDfFromRdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결측 값\n",
    "\n",
    "\n",
    "* ```null```은 결측, 즉 \"no value\" 또는 \"nothing\"을 말한다.\n",
    "* ```NaN```은 \"Not a Number\", 즉 수학에서 0.0/0.0과 같이 의미가 없는 연산의 결과를 말한다.\n",
    "\n",
    "```IsCaptain``` 항목은 boolean 타입이라 isnan() 함수가 오류를 발생한다. ```'isnan(`IsCaptain`)' due to data type mismatch: argument 1 requires (double or float) type, however, '`IsCaptain`' is of boolean type.```\n",
    "그래서 항목에서 제거하고 확인하기로 하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = wcDf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols.remove('IsCaptain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+----+------+--------+--------+----+-----------+-----------+\n",
      "|Competition|Year|Team|Number|Position|FullName|Club|ClubCountry|DateOfBirth|\n",
      "+-----------+----+----+------+--------+--------+----+-----------+-----------+\n",
      "|          0|   0|   0|     0|       0|       0|   0|          0|          0|\n",
      "+-----------+----+----+------+--------+--------+----+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "wcDf.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in cols]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 형변환\n",
    "\n",
    "스키마를 자동으로 유추한 형식을 보면,\n",
    "DateOfBirth, Number 컬럼이 문자열로 인식되어 있어 만족스럽지 못하다.\n",
    "컬럼 'DateOfBirth'는 'DoB'로 ```DateType()```, 컬럼 'Number'는 'NumberInt'로 \"Integer\" 형으로 설정해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### ```DateType``` 형변환\n",
    "\n",
    "* Python ```datetime```을 사용한 DateType 컬럼 생성\n",
    "\n",
    "Python ```datetime```은 날짜 문자열을 다음과 같은 년, 월, 일 형식으로 변환해준다.\n",
    "\n",
    "표시 | 설명               |예\n",
    "---|--------------------|--------------------\n",
    "%d | 숫자로 표현한 2자리 일자 | 01, 02, ..., 31\n",
    "%m | 숫자로 표현한 2자리 월  | 01, 02, ..., 12\n",
    "%y | 숫자로 표현한 2자리 년수 (세기 불포함) | 00, 01, ..., 99\n",
    "%Y | 숫자로 표현한 4자리 년수 (세기 포함)  | 0001, 0001, ..., 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1991-11-25 00:00:00\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "print (datetime.strptime(\"11/25/1991\", '%m/%d/%Y'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "datetime을 사용해서 일자를 DateType()으로 변환해보자.\n",
    "이 때 사용자정의함수 udf()를 만들어 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DateType\n",
    "toDate = udf(lambda x: datetime.strptime(x, '%m/%d/%Y'), DateType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "withColumn()은 컬럼을 추가하는 명령어이다. 방금 만든 udf 함수를 호출해서 형변환을 하고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcDf = wcDf.withColumn('date1', toDate(wcDf['DateOfBirth']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그 결과를 출력하면 아래와 같이 ```ValueError```가 출력된다.\n",
    "\n",
    "```python\n",
    "wcDf.take(1)\n",
    "\n",
    "ValueError: time data '1905-5-5' does not match format '%m/%d/%Y'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* pyspark ```to_date()``` 함수\n",
    "\n",
    "이번에는 pyspark에서 제공하는 to_date() 함수를 사용하자.\n",
    "방금 오류가 발생한 컬럼은 제거하고 해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcDf = wcDf.drop('date1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to_date() 함수는 string (StringType)을 date (DateType) 으로 형변환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "_wcDfCasted=wcDf.withColumn('date2', to_date(wcDf['DateOfBirth'], 'yyyy-MM-dd'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* pyspark ```cast()``` 함수\n",
    "\n",
    "pyspark의 cast() 함수로 형변환을 해보자.\n",
    "DateOfBirth와 더불어 Number 컬럼도 같이 Integer로 형변환을 해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DateType\n",
    "\n",
    "wcDfCasted = _wcDfCasted.withColumn('date3', _wcDfCasted['DateOfBirth'].cast(DateType()))\n",
    "wcDfCasted = wcDfCasted.withColumn('NumberInt', wcDfCasted['Number'].cast(\"integer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Competition: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      " |-- Team: string (nullable = true)\n",
      " |-- Number: string (nullable = true)\n",
      " |-- Position: string (nullable = true)\n",
      " |-- FullName: string (nullable = true)\n",
      " |-- Club: string (nullable = true)\n",
      " |-- ClubCountry: string (nullable = true)\n",
      " |-- DateOfBirth: string (nullable = true)\n",
      " |-- IsCaptain: boolean (nullable = true)\n",
      " |-- date2: date (nullable = true)\n",
      " |-- date3: date (nullable = true)\n",
      " |-- NumberInt: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wcDfCasted.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그대로 출력하면 오류가 발생할 수 있다.\n",
    "```SparkUpgradeException: You may get a different result due to the upgrading of Spark 3.0```라는 오류가 발생하면\n",
    "**```spark.sql.legacy.timeParserPolicy=LEGACY```**라고 설정을 변경해주어야 한다.\n",
    "\n",
    "아래와 같이 to_data() 또는 cast() 함수를 사용하여 일자 형변환 결과를 올바르게 출력하고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Competition='World Cup', Year=1930, Team='Argentina', Number='', Position='GK', FullName='Ãngel Bossio', Club='Club AtlÃ©tico Talleres de Remedios de Escalada', ClubCountry='Argentina', DateOfBirth='1905-5-5', IsCaptain=False, date2=datetime.date(1905, 5, 5), date3=datetime.date(1905, 5, 5), NumberInt=None)]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "wcDfCasted.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 코드 정리\n",
    "\n",
    "지금까지 작성한 코드는 **상호대화형**으로 작성되었다.\n",
    "배우거나 연습하기 위해 작성된 코드가 포함되었기 때문에 기능이 중복된 코드가 있을 수 있다.\n",
    "\n",
    "프로그램을 하는 **목적**이 무엇인지를 생각하고, **필요한 코드**만 남겨두자.\n",
    "URL에서 JSON 데이터를 읽어 DataFrame을 생성하고, 한 줄 출력하는 코드만 필요하다.\n",
    "일괄실행하기 위해서는 spark를 생성하는 코드가 있어야 한다.\n",
    "정리하면서 라이브러리는 앞 부분으로 위치시킨다.\n",
    "주석도 코드를 이해하는데 도움이 되므로 넣어준다.\n",
    "프로그램 절차에 맞추어 수정이 필요한 부분도 있다. 예를 들어 wcDfCasted는 wcDf로 변경하였다.\n",
    "\n",
    "아래 프로그램에 main() 함수를 넣어서 정리하면 더욱 좋겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Competition='World Cup', Year=1930, Team='Argentina', Number='', Position='GK', FullName='Ãngel Bossio', Club='Club AtlÃ©tico Talleres de Remedios de Escalada', ClubCountry='Argentina', DateOfBirth='1905-5-5', IsCaptain=False, date3=datetime.date(1905, 5, 5), NumberInt=None)]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import DateType\n",
    "\n",
    "import pyspark\n",
    "os.environ[\"PYSPARK_PYTHON\"]=\"/usr/bin/python3\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"]=\"/usr/bin/python3\"\n",
    "\n",
    "myConf=pyspark.SparkConf()\n",
    "spark = pyspark.sql.SparkSession\\\n",
    "    .builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"myApp\")\\\n",
    "    .config(conf=myConf)\\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "\n",
    "# read url json\n",
    "r=requests.get(\"https://raw.githubusercontent.com/jokecamp/FootballData/master/World%20Cups/all-world-cup-players.json\")\n",
    "wc=r.json()\n",
    "\n",
    "# read dictionary into Row\n",
    "wcDf = spark.createDataFrame(Row(**x) for x in wc)\n",
    "\n",
    "# cast DoB string into date, Number string into integer\n",
    "wcDfCasted = wcDf.withColumn('date3', wcDf['DateOfBirth'].cast(DateType()))\n",
    "wcDfCasted = wcDfCasted.withColumn('NumberInt', wcDfCasted['Number'].cast(\"integer\"))\n",
    "\n",
    "wcDfCasted.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### S.4.7 Parquet 파일 읽기, 쓰기\n",
    "\n",
    "Parquet(파케이)는 나무조각을 붙여넣은 마룻바닥이라는 뜻으로, Apache Hadoop에서는 컬럼별로 저장하는 데이터 압축형식으로, DataFrame에서 이 파일을 쓰거나, 읽을 수 있다. \n",
    "\n",
    "```python\n",
    "-rw-r--r-- 1 jsl jsl 522 10월  7 15:27 part-r-00000-0318688b-018f-4e55-858b-b4b78ac56532.snappy.parquet\n",
    "-rw-r--r-- 1 jsl jsl   0 10월  7 15:27 _SUCCESS\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 사용했던 ```_myDf```를 parquet형식으로 저장해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zaqxs\\Code\\201910787\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "_myDf.write.parquet(os.path.join(\"data\",\"people.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "앞서 쓰여진 parquet으로부터 읽어서 출력할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "_pDf=spark.read.parquet(os.path.join(\"data\",\"people.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|col1|col2|\n",
      "+----+----+\n",
      "|  35|   2|\n",
      "|  40|  27|\n",
      "|  12|  38|\n",
      "|  15|  31|\n",
      "|  21|   1|\n",
      "|  14|  19|\n",
      "|  46|   1|\n",
      "|  10|  34|\n",
      "|  28|   3|\n",
      "|  48|   1|\n",
      "|  16|   2|\n",
      "|  30|   3|\n",
      "|  32|   2|\n",
      "|  48|   1|\n",
      "|  31|   2|\n",
      "|  22|   1|\n",
      "|  12|   3|\n",
      "|  39|  29|\n",
      "|  19|  37|\n",
      "|  25|   2|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_pDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.5 DataFrame API 사용해 보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 빈 DataFrame 생성\n",
    "\n",
    "비어있는 DataFrame을 생성하려면, 빈 schema를 설정하고 emptyRdd()를 사용해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType\n",
    "\n",
    "schema = StructType([])\n",
    "emptyDf = spark.createDataFrame(spark.sparkContext.emptyRDD(), schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emptyDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### range\n",
    "\n",
    "비어 있는 DataFrame보다는 일련의 수를 가진 DataFrame을 만들어 보자.\n",
    "```range(start, end=None, setp=1, numPartitions=None)``` 함수는 Python range() 함수와 같이 정수를 생성하고, 컬럼명 id를 설정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  2|\n",
      "|  4|\n",
      "|  6|\n",
      "|  8|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(0, 10, 2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 컬럼 추가 withColumn, 컬럼 삭제 Drop\n",
    "\n",
    "```withColumn()```은 열을 추가한다.\n",
    "앞서 사용하였던 키, 몸무게 파일에서 DataFrame을 생성하고 컬럼을 생성해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "tDf = spark\\\n",
    "    .read\\\n",
    "    .options(header='false', inferschema='true', delimiter='\\t')\\\n",
    "    .csv(os.path.join('data', 'ds_spark_heightweight.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "컬럼은 자동 생성이 되었으므로, ```'_c0', '_c1', '_c2'```이고 사용하기 불편하므로 의미있는 컬럼으로 변경하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_c0', '_c1', '_c2']"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tDf.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기존에 있는 ```_1```행을 ```integer```로 형변환해서 ```id```행을 만들고, 기존의 ```_1```행을 삭제해보자.\n",
    "```drop()```은 열을 삭제할 때 사용한다. 같은 방식으로 나머지도 변경해보자.\n",
    "컬럼은 점연산자 tDf._c0 또는 인덱스 tDf['_c1'] 어느 방식이나 모두 가능하다.\n",
    "\n",
    "cast() 함수에는 \"integer\" 또는 Spark의 데이터타입 IntegerType()이라고 적어도 된다.\n",
    "또는 statement로 적어도 된다.\n",
    "```python\n",
    "from pyspark.sql import functions as F\n",
    "tDf.withColumn(\"id\", F.expr(\"CAST(id AS INTEGER)\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "tDf = tDf.withColumn(\"id\", tDf._c0.cast(\"integer\"))\n",
    "tDf = tDf.withColumn(\"height\", tDf['_c1'].cast(\"double\"))\n",
    "tDf = tDf.withColumn(\"weight\", tDf['_c2'].cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- _c1: double (nullable = true)\n",
      " |-- _c2: double (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      " |-- weight: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "컬럼이 중복되었으므로, 삭제하자. 하나씩 또는 다음과 같이 한꺼번에 삭제할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "tDf = tDf.drop('_c0').drop('_c1').drop('_c2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      " |-- weight: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "형변환이 잘 되었고 컬럼명이 변경된 결과를 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, height=65.78, weight=112.99)]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tDf.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사용자정의 함수 udf\n",
    "\n",
    "UDF User Defined Functions는 사용자 정의함수로서 DataFrame의 withColumn() 함수와 같이 사용되어 새로운 컬럼을만드는 경우 유용하다.\n",
    "보통 함수와 같이 함수명과 반환 값을 미리 정의해서 lambda 함수 또는 다른 함수를 사용할 수 있다.\n",
    "**다른 함수를 직접 사용할 수 없고, udf()를 통해서 호출**해야 한다.\n",
    "udf()는 **코드가 복잡한 경우에 함수를 분리해서 처리**하면 유용하겠다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 Pandas로 저장한 csv파일을 읽어서 DataFrame을 만들어 보자.\n",
    "header를 있는 그대로 가져오고,\n",
    "schema도 현재 데이터에서 추정하도록 설정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDf = spark\\\n",
    "        .read.format('com.databricks.spark.csv')\\\n",
    "        .options(header='true', inferschema='true')\\\n",
    "        .load(os.path.join('data','myDf.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```header='true'```라고 설정을 해서, 컬럼명은 있는 그대로 year, name, height로 읽는다.\n",
    "printSchema()를 출력해보면, year는 정수, name은 문자열, height는 정수로 정의되어 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------+------+\n",
      "|_c0|year|   name|height|\n",
      "+---+----+-------+------+\n",
      "|  0|   1|kim, js|   170|\n",
      "|  1|   1|lee, sm|   175|\n",
      "|  2|   2|lim, yg|   180|\n",
      "|  3|   2|    lee|   170|\n",
      "+---+----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### udf함수로 대문자 변환 withColumn\n",
    "\n",
    "대문자로 변환하고 반환하는 함수를 만들어보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uppercase(s):\n",
    "    return s.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A'"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uppercase(\"a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 정의된 함수를 바로 호출하면 오류가 발생한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Column' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-154-85e35a83822c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmyDf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmyDf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nameUpper\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muppercase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmyDf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-152-cc3ea1335364>\u001b[0m in \u001b[0;36muppercase\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0muppercase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'Column' object is not callable"
     ]
    }
   ],
   "source": [
    "myDf = myDf.withColumn(\"nameUpper\", uppercase(myDf.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "udf()를 통해 함수를 호출해야 한다.\n",
    "반환값의 타잎을 ```StringType()```으로 정의하고 있다는 점에 유의하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "upperUdf = udf(uppercase, StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 정의한 udf() 함수를 withColumn()과 같이 사용하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDf = myDf.withColumn(\"nameUpper\", upperUdf(myDf['name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------+------+---------+\n",
      "|_c0|year|   name|height|nameUpper|\n",
      "+---+----+-------+------+---------+\n",
      "|  0|   1|kim, js|   170|  KIM, JS|\n",
      "|  1|   1|lee, sm|   175|  LEE, SM|\n",
      "|  2|   2|lim, yg|   180|  LIM, YG|\n",
      "|  3|   2|    lee|   170|      LEE|\n",
      "+---+----+-------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### udf함수로 Double변환 withColumn\n",
    "\n",
    "udf() 함수에 lambda를 사용해도 된다.\n",
    "**withColumn()에서 다른 함수를 호출하지 못하고, udf() 함수를 통해** 호출한다.\n",
    "\n",
    "앞서 배운 ```withColumn()```에서 udf를 호출해서 ```height``` 컬럼을 ```long```에서 ```DoubleType()```으로 만들어 보자.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "toDoublefunc = udf(lambda x: float(x), DoubleType())\n",
    "myDf = myDf.withColumn(\"heightD\", toDoublefunc(myDf.height))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```dtypes```로 컬럼명과 데이터타입을 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_c0', 'int'),\n",
       " ('year', 'int'),\n",
       " ('name', 'string'),\n",
       " ('height', 'int'),\n",
       " ('nameUpper', 'string'),\n",
       " ('heightD', 'double')]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDf.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### udf함수로 정수변환 withColumn\n",
    "\n",
    "year의 문자열을 정수로 형변환 해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.sql.functions import udf, struct\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "toint=udf(lambda x:int(x), IntegerType())\n",
    "myDf=myDf.withColumn(\"yearI\", toint(myDf['year']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: integer (nullable = true)\n",
      " |-- nameUpper: string (nullable = true)\n",
      " |-- heightD: double (nullable = true)\n",
      " |-- yearI: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------+------+---------+-------+-----+\n",
      "|_c0|year|   name|height|nameUpper|heightD|yearI|\n",
      "+---+----+-------+------+---------+-------+-----+\n",
      "|  0|   1|kim, js|   170|  KIM, JS|  170.0|    1|\n",
      "|  1|   1|lee, sm|   175|  LEE, SM|  175.0|    1|\n",
      "|  2|   2|lim, yg|   180|  LIM, YG|  180.0|    2|\n",
      "|  3|   2|    lee|   170|      LEE|  170.0|    2|\n",
      "+---+----+-------+------+---------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "yearI 컬럼명만 선택하여 조회해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|yearI|\n",
      "+-----+\n",
      "|    1|\n",
      "|    1|\n",
      "|    2|\n",
      "|    2|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.select('yearI').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### udf함수로 조건에 따른 withColumn\n",
    "\n",
    "udf()함수를 사용하여, 조건에 따라 컬럼을 생성해보자.\n",
    "키를 175를 기준으로 이분화하고 반환값을 ```StringType()```으로 정의해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "height_udf = udf(lambda height: \"taller\" if height >=175 else \"shorter\", StringType())\n",
    "heightDf=myDf.withColumn(\"height>175\", height_udf(myDf.heightD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------+------+---------+-------+-----+----------+\n",
      "|_c0|year|   name|height|nameUpper|heightD|yearI|height>175|\n",
      "+---+----+-------+------+---------+-------+-----+----------+\n",
      "|  0|   1|kim, js|   170|  KIM, JS|  170.0|    1|   shorter|\n",
      "|  1|   1|lee, sm|   175|  LEE, SM|  175.0|    1|    taller|\n",
      "|  2|   2|lim, yg|   180|  LIM, YG|  180.0|    2|    taller|\n",
      "|  3|   2|    lee|   170|      LEE|  170.0|    2|   shorter|\n",
      "+---+----+-------+------+---------+-------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "heightDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 컬럼명 변경 withColumnRenamed\n",
    "\n",
    "앞서 ```withCoumun()``` 명령어로 열을 추가해 보았다. ```withColumnRenamed()``` 함수는 컬럼명을 변경한다. 컬럼명을 ```name```에서 ```full name```으로 변경해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "tDf=tDf.withColumnRenamed('id','ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n",
      "| ID|height|weight|\n",
      "+---+------+------+\n",
      "|  1| 65.78|112.99|\n",
      "|  2| 71.52|136.49|\n",
      "|  3|  69.4|153.03|\n",
      "+---+------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tDf.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 그래프\n",
    "\n",
    "#### plot\n",
    "\n",
    "Spark에는 그래프를 그리는 기능이 없다.\n",
    "Python matplotlib을 이용해서 그래프를 표현한다.\n",
    "2차원 ```plot()```을 하기 위해서는, x와 y축 값이 필요하다.\n",
    "DataFrame -> RDD로 변환하고, map() 함수를 사용하여 배열로부터 weight, height를 분리해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "_weightRdd=tDf.rdd.map(lambda fields:fields[1]).collect()\n",
    "_heightRdd=tDf.rdd.map(lambda fields:fields[2]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x, y 값이 잘 추출되었는지 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[65.78 71.52 69.4  68.22 67.79]\n",
      "[112.99 136.49 153.03 142.34 144.3 ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print (np.array(_weightRdd)[:5])\n",
    "print (np.array(_heightRdd)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 추출한 height, weight RDD를 numpy array를 사용해서 2차원 x, y로 변환해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(np.array(_weightRdd), np.array(_heightRdd),'x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### boxplot, violinplot\n",
    "\n",
    "하나의 컬럼만 선택해서 그래프를 그려보자.\n",
    "* boxplot의 가운데 박스 IQR (Interquartile Range)은 4분위 값의 2, 3번째와 50% 값, 즉 25%~75%의 구간을 의미한다.\n",
    "그리고 위 아래 가로선은 IQR 값의 1.5배되는 구역을 나타낸다. 값의 분포와 outlier를 확인하기 편리하다.\n",
    "* violin plot 역시 boxplot과 유사한 기능이지만, 밀도를 보여주고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = tDf.select(\"height\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>68.05240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.82398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>63.48000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>66.94000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>67.86500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>69.18000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>71.80000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         height\n",
       "count  50.00000\n",
       "mean   68.05240\n",
       "std     1.82398\n",
       "min    63.48000\n",
       "25%    66.94000\n",
       "50%    67.86500\n",
       "75%    69.18000\n",
       "max    71.80000"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "height.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax1 = fig.add_subplot(1, 2, 1)  # subplot location\n",
    "ax1 = plt.boxplot(height)\n",
    "\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "ax2 = sns.violinplot(data=height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### aggregate functions\n",
    "\n",
    "avg, collect_list, countDistinct, count, kurtosis, max, min, mean, skewness, stddev, sum, variance 등의 함수가 지원된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dictionary 형식\n",
    "\n",
    "agg() 함수에 dictionary 형식으로 ```컬렴명: aggregate functions```으로 적어준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|count(height)|\n",
      "+-------------+\n",
      "|           50|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tDf.agg({\"height\":\"count\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|    kurtosis(height)|\n",
      "+--------------------+\n",
      "|-0.00944222604387468|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tDf.agg({\"height\":\"kurtosis\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|      avg(height)|\n",
      "+-----------------+\n",
      "|68.05240000000002|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tDf.agg({\"height\":\"avg\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|min(height)|\n",
      "+-----------+\n",
      "|      63.48|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "tDf.agg(F.min(\"height\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 컬럼 조회 select\n",
    "\n",
    "컬럼은 아래와 같이 ```select()```로 선택할 수 있다.\n",
    "컬럼은 그 명칭을 사용하거나 ```myDf.name```, 인덱스 ```myDf['name']```로 선택할 수 있다.\n",
    "\n",
    "컬럼 선택 | 예제 | 권고\n",
    "-----|-----|-----\n",
    "점 연산자로 컬럼을 선택 | myDf.name | N\n",
    "인덱스로 컬럼을 선택 | myDf['name'] | Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 컬럼명으로 직접 조회 못해\n",
    "\n",
    "컬럼명만 아래와 같이 적어주어도 조회할 수 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'name'>"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDf['name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또는 컬럼 데이터를 조회하기 위해 컬럼에 show(), collect() 함수를 사용해서는 안된다. ```select()```를 사용해서 컬럼을 읽을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Column' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-179-2fdc8d3e858e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmyDf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'Column' object is not callable"
     ]
    }
   ],
   "source": [
    "myDf['name'].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 컬럼 조회"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|kim, js|\n",
      "|lee, sm|\n",
      "|lim, yg|\n",
      "|    lee|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_name=myDf.select('name')\n",
    "_name.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여러 컬럼을 조회하기 위해서는 해당 컬럼을 넣어주면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|   name|height|\n",
      "+-------+------+\n",
      "|kim, js|   170|\n",
      "|lee, sm|   175|\n",
      "|lim, yg|   180|\n",
      "|    lee|   170|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_name=myDf.select('name', 'height').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또는 리스트를 넣어주어 여러 컬럼을 선택할 수도 있다.\n",
    "**리스트를 풀어야 하므로, 앞서 배웠던 ```*``` 연산자**를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|   name|height|\n",
      "+-------+------+\n",
      "|kim, js|   170|\n",
      "|lee, sm|   175|\n",
      "|lim, yg|   180|\n",
      "|    lee|   170|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols = ['name', 'height']\n",
    "myDf.select(*cols).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 컬럼을 List로 변환\n",
    "\n",
    "select() 함수는 Row()로 구성된 컬럼을 선택하게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='kim, js'),\n",
       " Row(name='lee, sm'),\n",
       " Row(name='lim, yg'),\n",
       " Row(name='lee')]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDf.select('name').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List로 변환하려면, map() 함수로는 가능하지 않다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='kim, js'),\n",
       " Row(name='lee, sm'),\n",
       " Row(name='lim, yg'),\n",
       " Row(name='lee')]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDf.select('name').rdd.map(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2차원 Row List를 제거하기 위해서 인덱스를 사용하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kim, js', 'lee, sm', 'lim, yg', 'lee']"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDf.select('name').rdd.map(lambda x: x[0]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또는  flatMap()으로 2차원 구조 Row List를 1차원 List로 변환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kim, js', 'lee, sm', 'lim, yg', 'lee']"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDf.select('name').rdd.flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### select like\n",
    "\n",
    "```%``` 연산자는 0 또는 그 이상의 문자를 의미한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+---------------+\n",
      "|   name|height|name LIKE %lee%|\n",
      "+-------+------+---------------+\n",
      "|kim, js|   170|          false|\n",
      "|lee, sm|   175|           true|\n",
      "|lim, yg|   180|          false|\n",
      "|    lee|   170|           true|\n",
      "+-------+------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.select(\"name\", \"height\", myDf.name.like(\"%lee%\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### select startswith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+---------------------+\n",
      "|   name|height|startswith(name, kim)|\n",
      "+-------+------+---------------------+\n",
      "|kim, js|   170|                 true|\n",
      "|lee, sm|   175|                false|\n",
      "|lim, yg|   180|                false|\n",
      "|    lee|   170|                false|\n",
      "+-------+------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.select(\"name\", \"height\", myDf.name.startswith(\"kim\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### select endswith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------------------+\n",
      "|   name|height|endswith(name, lee)|\n",
      "+-------+------+-------------------+\n",
      "|kim, js|   170|              false|\n",
      "|lee, sm|   175|              false|\n",
      "|lim, yg|   180|              false|\n",
      "|    lee|   170|               true|\n",
      "+-------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.select(\"name\", \"height\", myDf.name.endswith(\"lee\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### alias\n",
    "\n",
    "이름을 변경할 경우 alias() 함수를 사용할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame의 명칭을 변경해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDf1 = myDf.alias(\"myDf1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "컬럼의 명칭을 변경해보자.\n",
    "그러기 위해서는 우선 컬럼을 **```select()```** 함수로 골라낸 후, **```alias```**로 **컬럼명**을 정할 수 있다.\n",
    "name 컬럼을 **```substr```**으로 1,3문자를 선택한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|short name|\n",
      "+----------+\n",
      "|       kim|\n",
      "|       lee|\n",
      "|       lim|\n",
      "+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf1.select(myDf1.name.substr(1,3).alias(\"short name\")).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 행과 열을 선택 select, when, otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------------------------------+\n",
      "|height|CASE WHEN (height < 175) THEN 1 ELSE 0 END|\n",
      "+------+------------------------------------------+\n",
      "|   170|                                         1|\n",
      "|   175|                                         0|\n",
      "|   180|                                         0|\n",
      "|   170|                                         1|\n",
      "+------+------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "myDf.select(\"height\", when(myDf.height < 175, 1).otherwise(0)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alias 명령어로 컬럼을 변경해 줄 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n",
      "|height|<175|\n",
      "+------+----+\n",
      "|   170|   1|\n",
      "|   175|   0|\n",
      "|   180|   0|\n",
      "|   170|   1|\n",
      "+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "myDf.select(\"height\", (when(myDf.height < 175, 1).otherwise(0)).alias('<175')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또는 0, 1이 아닌 문자열로 생성할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "_myDf=myDf.select(when(myDf['heightD'] >175.0, \">175\").otherwise(\"<175\").alias(\"how tall\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|how tall|\n",
      "+--------+\n",
      "|    <175|\n",
      "|    <175|\n",
      "|    >175|\n",
      "|    <175|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_myDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "withColumn() 함수를 사용하면, DataFrame에 컬럼을 추가하게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------+------+---------+-------+-----+--------+\n",
      "|_c0|year|   name|height|nameUpper|heightD|yearI|how tall|\n",
      "+---+----+-------+------+---------+-------+-----+--------+\n",
      "|  0|   1|kim, js|   170|  KIM, JS|  170.0|    1|    <175|\n",
      "|  1|   1|lee, sm|   175|  LEE, SM|  175.0|    1|    <175|\n",
      "|  2|   2|lim, yg|   180|  LIM, YG|  180.0|    2|    >175|\n",
      "|  3|   2|    lee|   170|      LEE|  170.0|    2|    <175|\n",
      "+---+----+-------+------+---------+-------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_myDf = myDf.withColumn('how tall', when(myDf['heightD'] >175.0, \">175\").otherwise(\"<175\"))\n",
    "_myDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 행과 열을 선택  where, select\n",
    "\n",
    "\n",
    "DataFrame은 관계형데이터베이스의 테이블과 매우 유사하다. **SQL 명령**을 사용하듯이 ```where()```, ```select()```, ```groupby()``` 함수를 사용할 수 있다.\n",
    "\n",
    "* 행: ```where()```에 따라 컬럼의 **조건에 맞는 행을 선택**하고 (filter),\n",
    "* 열: 앞서 배운 ```select()```로 열을 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------+------+---------+-------+-----+\n",
      "|_c0|year|   name|height|nameUpper|heightD|yearI|\n",
      "+---+----+-------+------+---------+-------+-----+\n",
      "|  0|   1|kim, js|   170|  KIM, JS|  170.0|    1|\n",
      "|  3|   2|    lee|   170|      LEE|  170.0|    2|\n",
      "+---+----+-------+------+---------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.where(myDf['height'] < 175).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|   name|height|\n",
      "+-------+------+\n",
      "|kim, js|   170|\n",
      "|    lee|   170|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.where(myDf['height'] < 175)\\\n",
    "    .select(myDf['name'], myDf['height']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter\n",
    "\n",
    "```filter()```는 조건에 따라 데이터를 걸러낸다.\n",
    "앞서 배웠던 ```where()```와 유사한 기능을 수행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------+------+---------+-------+-----+\n",
      "|_c0|year|   name|height|nameUpper|heightD|yearI|\n",
      "+---+----+-------+------+---------+-------+-----+\n",
      "|  2|   2|lim, yg|   180|  LIM, YG|  180.0|    2|\n",
      "+---+----+-------+------+---------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.filter(myDf['height'] > 175).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### regexp_replace 컬럼의 내용 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------+------+---------+-------+-----+-------+\n",
      "|_c0|year|   name|height|nameUpper|heightD|yearI|nameNew|\n",
      "+---+----+-------+------+---------+-------+-----+-------+\n",
      "|  0|   1|kim, js|   170|  KIM, JS|  170.0|    1|kim, js|\n",
      "|  1|   1|lee, sm|   175|  LEE, SM|  175.0|    1|lim, sm|\n",
      "|  2|   2|lim, yg|   180|  LIM, YG|  180.0|    2|lim, yg|\n",
      "|  3|   2|    lee|   170|      LEE|  170.0|    2|    lim|\n",
      "+---+----+-------+------+---------+-------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "_heightDf = myDf.withColumn('nameNew', regexp_replace('name', 'lee', 'lim'))\n",
    "_heightDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### groupBy\n",
    "\n",
    "컬럼을 학년에 따라,\n",
    "```groupBy()```하면 아래와 같이 데이터만 집단화하게 된다.\n",
    "집단화하면 개수를 세거나, 합계를 내거나 어떤 통계량을 계산이 필요하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.group.GroupedData at 0x1d93cc84278>"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDf.groupby(myDf['year'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### groupBy하고 max\n",
    "\n",
    "컬럼을 기준으로 구분지어서 평균, 합계, 갯수, 최대, 최소 등을 구할 수 있다.\n",
    "첫 컬럼 학년을 ```groupby()```해서 최대값 ```max()```를 구해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+---------+-----------+------------+----------+\n",
      "|year|max(_c0)|max(year)|max(height)|max(heightD)|max(yearI)|\n",
      "+----+--------+---------+-----------+------------+----------+\n",
      "|   1|       1|        1|        175|       175.0|         1|\n",
      "|   2|       3|        2|        180|       180.0|         2|\n",
      "+----+--------+---------+-----------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.groupby(myDf['year']).max().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### groupBy, agg\n",
    "\n",
    "```agg()```는 합계 함수를 계산할 수 있으며, 지원하는 함수는 ```avg, max, min, sum, count```이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```year``` 컬럼에 대해 ```agg()``` 함수로 계산할 수 있다.\n",
    "\n",
    "dictionary 형식으로 key는 컬럼명, value는 합계 함수를 적어준다.\n",
    "예를 들어 {\"heightD\":\"avg\"}에서 \"heightD\"는 컬럼명, \"avg\"는 합계함수이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+\n",
      "|year|avg(heightD)|\n",
      "+----+------------+\n",
      "|   1|       172.5|\n",
      "|   2|       175.0|\n",
      "+----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.groupBy('year').agg({\"heightD\":\"avg\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### groupBy 국가별 인원수\n",
    "\n",
    "월드컵 데이터를 groupBy 해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|ClubCountry|count|\n",
      "+-----------+-----+\n",
      "|   England |    4|\n",
      "|   Paraguay|   93|\n",
      "|     Russia|   51|\n",
      "|        POL|   11|\n",
      "|        BRA|   27|\n",
      "|    Senegal|    1|\n",
      "|     Sweden|  154|\n",
      "|   Colombia|    1|\n",
      "|        FRA|  155|\n",
      "|        ALG|    8|\n",
      "|   England |    1|\n",
      "|       RUS |    1|\n",
      "|     Turkey|   65|\n",
      "|      Zaire|   22|\n",
      "|       Iraq|   22|\n",
      "|    Germany|  206|\n",
      "|        RSA|   16|\n",
      "|        ITA|  224|\n",
      "|        UKR|   38|\n",
      "|        GHA|    8|\n",
      "+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wcDf.groupBy(wcDf.ClubCountry).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### groupBy 국가별 포지션별 인원수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+----+----+----+----+\n",
      "|ClubCountry|    |  DF|  FW|  GK|  MF|\n",
      "+-----------+----+----+----+----+----+\n",
      "|   England |null|null|   2|null|   2|\n",
      "|   Paraguay|null|  26|  37|  10|  20|\n",
      "|     Russia|null|  20|  11|   4|  16|\n",
      "|        POL|null|   2|   2|   3|   4|\n",
      "|        BRA|null|   7|   5|   4|  11|\n",
      "|    Senegal|null|null|null|   1|null|\n",
      "|     Sweden|null|  40|  47|  25|  42|\n",
      "|   Colombia|null|null|   1|null|null|\n",
      "|        ALG|null|   2|null|   6|null|\n",
      "|        FRA|null|  46|  41|  18|  50|\n",
      "|   England |null|null|null|null|   1|\n",
      "|       RUS |null|null|null|   1|null|\n",
      "|     Turkey|null|  20|  13|  12|  20|\n",
      "|      Zaire|null|   6|   5|   3|   8|\n",
      "|       Iraq|null|   6|   4|   3|   9|\n",
      "|    Germany|null|  64|  51|  16|  75|\n",
      "|        RSA|null|   5|   2|   3|   6|\n",
      "|        UKR|null|  13|   7|   4|  14|\n",
      "|        ITA|null|  74|  42|  19|  89|\n",
      "|        CMR|null|   1|   1|   1|null|\n",
      "+-----------+----+----+----+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wcDf.groupBy('ClubCountry').pivot('Position').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F 함수\n",
    "\n",
    "또는 아래와 같이 별도 ```pyspark.sql.functions```을 사용할 수 있다.\n",
    "앞서 pyspark.sql.functions은 함수이므로, ```from pyspark.sql.functions import split``` 이렇게 한다.\n",
    "또는 ```from pyspark.sql import functions as F```라고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+------------+------------+\n",
      "|min(heightD)|max(heightD)|avg(heightD)|sum(heightD)|\n",
      "+------------+------------+------------+------------+\n",
      "|       170.0|       180.0|      173.75|       695.0|\n",
      "+------------+------------+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "myDf.agg(F.min(myDf.heightD),F.max(myDf.heightD),F.avg(myDf.heightD),F.sum(myDf.heightD)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 행 추가\n",
    "\n",
    "행을 추가하려면, DataFrame을 서로 합치는 방법으로 가능하다.\n",
    "추가할 행으로 DataFrame을 만들고, union() 함수로 합쳐야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "createFrame() 함수에는 리스트를 넣어주어야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "toAppendDf = spark.createDataFrame([Row(4, 1, \"choi, js\", 177)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Union can only be performed on tables with the same number of columns, but the first table has 7 columns and the second table has 4 columns;\n'Union false, false\n:- Project [_c0#773, year#774, name#775, height#776, nameUpper#803, heightD#836, <lambda>(year#774) AS yearI#844]\n:  +- Project [_c0#773, year#774, name#775, height#776, nameUpper#803, <lambda>(height#776) AS heightD#836]\n:     +- Project [_c0#773, year#774, name#775, height#776, uppercase(name#775) AS nameUpper#803]\n:        +- Relation[_c0#773,year#774,name#775,height#776] csv\n+- LogicalRDD [_1#5688L, _2#5689L, _3#5690, _4#5691L], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-208-dce48b789b9a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0m_myDf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmyDf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoAppendDf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36munion\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m   1826\u001b[0m         \u001b[0mAlso\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstandard\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mSQL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mresolves\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0mby\u001b[0m \u001b[0mposition\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0mby\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1827\u001b[0m         \"\"\"\n\u001b[1;32m-> 1828\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1829\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1830\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1305\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1307\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    115\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Union can only be performed on tables with the same number of columns, but the first table has 7 columns and the second table has 4 columns;\n'Union false, false\n:- Project [_c0#773, year#774, name#775, height#776, nameUpper#803, heightD#836, <lambda>(year#774) AS yearI#844]\n:  +- Project [_c0#773, year#774, name#775, height#776, nameUpper#803, <lambda>(height#776) AS heightD#836]\n:     +- Project [_c0#773, year#774, name#775, height#776, uppercase(name#775) AS nameUpper#803]\n:        +- Relation[_c0#773,year#774,name#775,height#776] csv\n+- LogicalRDD [_1#5688L, _2#5689L, _3#5690, _4#5691L], false\n"
     ]
    }
   ],
   "source": [
    "_myDf = myDf.union(toAppendDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------+------+---------+-------+-----+--------+\n",
      "|_c0|year|   name|height|nameUpper|heightD|yearI|how tall|\n",
      "+---+----+-------+------+---------+-------+-----+--------+\n",
      "|  0|   1|kim, js|   170|  KIM, JS|  170.0|    1|    <175|\n",
      "|  1|   1|lee, sm|   175|  LEE, SM|  175.0|    1|    <175|\n",
      "|  2|   2|lim, yg|   180|  LIM, YG|  180.0|    2|    >175|\n",
      "|  3|   2|    lee|   170|      LEE|  170.0|    2|    <175|\n",
      "+---+----+-------+------+---------+-------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_myDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### partition 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDf.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### repartition\n",
    "\n",
    "repartition()은 partition의 개수를 늘리거나 줄이거나 재설정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "_myDf = myDf.repartition(4)\n",
    "print(_myDf.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### coalesce\n",
    "\n",
    "coalesce()는 partition을 **줄일 때** 사용한다.\n",
    "앞서 4개의 partition을 가진 ```_myDf```를 2로 줄여보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "_myDf2 = _myDf.coalesce(2)\n",
    "print(_myDf2.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 통계 요약 describe\n",
    "\n",
    "column이 연산가능한 데이터타잎인 경우, 요약 값을 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+-------+------------------+---------+------------------+------------------+\n",
      "|summary|               _c0|              year|   name|            height|nameUpper|           heightD|             yearI|\n",
      "+-------+------------------+------------------+-------+------------------+---------+------------------+------------------+\n",
      "|  count|                 4|                 4|      4|                 4|        4|                 4|                 4|\n",
      "|   mean|               1.5|               1.5|   null|            173.75|     null|            173.75|               1.5|\n",
      "| stddev|1.2909944487358056|0.5773502691896257|   null|4.7871355387816905|     null|4.7871355387816905|0.5773502691896257|\n",
      "|    min|                 0|                 1|kim, js|               170|  KIM, JS|             170.0|                 1|\n",
      "|    max|                 3|                 2|lim, yg|               180|  LIM, YG|             180.0|                 2|\n",
      "+-------+------------------+------------------+-------+------------------+---------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결측값\n",
    "\n",
    "결측값을 채우는 함수이다.\n",
    "* df.na.fill(0) 모든 컬럼의 na를 0으로 교체\n",
    "* df.fillna( { 'c0':0, 'c1':0 } ) 컬럼 c0, c1의 na를 0으로 교체\n",
    "\n",
    "결측값을 삭제할 수도 있다.\n",
    "* df.na.drop(subset=[\"c0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: int, year: int, name: string, height: int, nameUpper: string, heightD: double, yearI: int]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "myDf.where(F.col(\"height\").isNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+------+---------+-------+-----+\n",
      "|_c0|year|name|height|nameUpper|heightD|yearI|\n",
      "+---+----+----+------+---------+-------+-----+\n",
      "|  0|   0|   0|     0|        0|      0|    0|\n",
      "+---+----+----+------+---------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "myDf.select([count(when(isnan(c), c)).alias(c) for c in myDf.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+------+---------+-------+-----+\n",
      "|_c0|year|name|height|nameUpper|heightD|yearI|\n",
      "+---+----+----+------+---------+-------+-----+\n",
      "|  0|   0|   0|     0|        0|      0|    0|\n",
      "+---+----+----+------+---------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "myDf.select([count(when(col(c).isNull(), c)).alias(c) for c in myDf.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제: 년별 분기별 대여건수\n",
    "\n",
    "서울시 열린데이터 https://data.seoul.go.kr/ 에서 제공하는 ```서울특별시_공공자전거 일별 대여건수_(2018~2019.03).csv```를 분석해보자.\n",
    "파일은 웹 검색을 해서 다운로드해서 사용하면 된다.\n",
    "데이터는 일자별로, 대여건수이이고, 몇 줄만 출력해보면 다음과 같다.\n",
    "\n",
    "|      date| count|\n",
    "|----------|------|\n",
    "|2018-01-01|  4950|\n",
    "|2018-01-02|  7136|\n",
    "|2018-01-03|  7156|\n",
    "|2018-01-04|  7102|\n",
    "|2018-01-05|  7705|\n",
    "\n",
    "### 문제 1-1: 년도별 대여건수 합계\n",
    "데이터는 2018, 2019년 15개월 간의 대여건수이다. 년도별로 대여건수의 합계를 계산해서 출력하자.\n",
    "\n",
    "|year|sum(count)|\n",
    "|----|----------|\n",
    "|2018|  10124874|\n",
    "|2019|   1871935|\n",
    "\n",
    "\n",
    "### 문제 1-2: 년도별, 월별 대여건수 합계\n",
    "년별, 월별로 대여건수를 계산하여 합계를 계산하여 출력한다.\n",
    "\n",
    "### 문제 1-3: 년도별, 월별 대여건수 그래프\n",
    "문제 1-2의 출력을 선 그래프로 그려보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 읽기\n",
    "\n",
    "서울시 열린데이터에서 데이터 ```서울특별시_공공자전거 일별 대여건수_(2018~2019.03).csv``` 를 다운로드 받아서 저장한다.\n",
    "csv 형식으로 schema는 자동 인식하도록 읽는다.\n",
    "일자는 ```timestamp```로 건수는 ```integer```로 인식되었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "_bicycle = spark.read.format('com.databricks.spark.csv')\\\n",
    "    .options(header='true', inferschema='true').load('data/서울특별시_공공자전거 일별 대여건수_(2018_2019.03).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- 대여일자: string (nullable = true)\n",
      " |-- 대여건수: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_bicycle.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전체 건수는 455건, 5건의 데이터만 읽어보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "455"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_bicycle.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|  대여일자|대여건수|\n",
      "+----------+--------+\n",
      "|2018-01-01|    4950|\n",
      "|2018-01-02|    7136|\n",
      "|2018-01-03|    7156|\n",
      "|2018-01-04|    7102|\n",
      "|2018-01-05|    7705|\n",
      "+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_bicycle.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 컬럼명 변경\n",
    "\n",
    "앞서 보듯이 파일을 읽으면서 컬럼명이 인식되었는데 \" count\"가 맨 앞에 공백이 하나 있게 되어 변경해보자.\n",
    "일단 붙여진 컬럼의 명칭을 변경하려면 ```withColumnRenamed()```를 연결하여 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "bicycle=_bicycle\\\n",
    "    .withColumnRenamed(\"date\", \"Date\")\\\n",
    "    .withColumnRenamed(\" count\", \"Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 컬럼 만들기: substr\n",
    "\n",
    "```substr()``` 함수는 인자가 2개로서, 앞글자 '1'은 시작 '4'는 4글자를 의미한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'Date'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-243-55c2da141dbc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbicycle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbicycle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"year\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbicycle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1642\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1643\u001b[0m             raise AttributeError(\n\u001b[1;32m-> 1644\u001b[1;33m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0m\u001b[0;32m   1645\u001b[0m         \u001b[0mjc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1646\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'Date'"
     ]
    }
   ],
   "source": [
    "bicycle=bicycle.withColumn(\"year\",bicycle.Date.substr(1, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- 대여일자: string (nullable = true)\n",
      " |-- 대여건수: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bicycle.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'Date'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-245-07e5c9a7479c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbicycle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbicycle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"month\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbicycle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1642\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1643\u001b[0m             raise AttributeError(\n\u001b[1;32m-> 1644\u001b[1;33m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0m\u001b[0;32m   1645\u001b[0m         \u001b[0mjc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1646\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'Date'"
     ]
    }
   ],
   "source": [
    "bicycle=bicycle.withColumn(\"month\",bicycle.Date.substr(6, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|  대여일자|대여건수|\n",
      "+----------+--------+\n",
      "|2018-01-01|    4950|\n",
      "|2018-01-02|    7136|\n",
      "|2018-01-03|    7156|\n",
      "|2018-01-04|    7102|\n",
      "|2018-01-05|    7705|\n",
      "+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bicycle.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 컬럼 만들기: F 함수\n",
    "\n",
    "함수를 이용해 년, 월, 일 등을 추출할 수 있다.\n",
    "먼저 앞서 생성된 column을 삭제하고 나서 해보자.\n",
    "여러 컬럼을 삭제하기 위해서는 ```*```를 앞에 붙여 준다.\n",
    "물론 하나씩 삭제할 수도 있고, 그러면 별표는 불필요하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['year','month']\n",
    "df = bicycle.drop(*columns_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "year, month 컬럼이 삭제되고 Date, Count 컬럼만 남겨졌다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- 대여일자: string (nullable = true)\n",
      " |-- 대여건수: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pyspark.sql.functions의 year(), month() 함수를 사용하여 년, 월을 추출하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve '`date`' given input columns: [대여건수, 대여일자];\n'Project [대여일자#6381, 대여건수#6382, year('date) AS year#6420]\n+- Relation[대여일자#6381,대여건수#6382] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-249-1e9318244696>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mbicycle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbicycle\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;33m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'year'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'date'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;33m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'month'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmonth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'date'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[1;34m(self, colName, col)\u001b[0m\n\u001b[0;32m   2453\u001b[0m         \"\"\"\n\u001b[0;32m   2454\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"col should be Column\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2455\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2456\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2457\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwithColumnRenamed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexisting\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1305\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1307\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    115\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: cannot resolve '`date`' given input columns: [대여건수, 대여일자];\n'Project [대여일자#6381, 대여건수#6382, year('date) AS year#6420]\n+- Relation[대여일자#6381,대여건수#6382] csv\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "bicycle = bicycle\\\n",
    "    .withColumn('year', F.year('date'))\\\n",
    "    .withColumn('month', F.month('date'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- 대여일자: string (nullable = true)\n",
      " |-- 대여건수: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bicycle.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 substr() 결과와 비교해보자. year, month가 올바르게 추출되었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|  대여일자|대여건수|\n",
      "+----------+--------+\n",
      "|2018-01-01|    4950|\n",
      "|2018-01-02|    7136|\n",
      "|2018-01-03|    7156|\n",
      "|2018-01-04|    7102|\n",
      "|2018-01-05|    7705|\n",
      "+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bicycle.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "show()는 앞 부분 보여주고 있다. 월이 1만 보여서, 다른 월의 결과를 보고 싶다면 filter()해주면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'month'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-252-34642b6094f9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbicycle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbicycle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmonth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1642\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1643\u001b[0m             raise AttributeError(\n\u001b[1;32m-> 1644\u001b[1;33m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0m\u001b[0;32m   1645\u001b[0m         \u001b[0mjc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1646\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'month'"
     ]
    }
   ],
   "source": [
    "bicycle.filter(bicycle.month == 2).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 분기\n",
    "\n",
    "1월 2월 3월은 1분기, 4~6은 2분기, 7~9는 3분기, 10~12는 4분기로 구분한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "def classifyQuarter(s):\n",
    "    q=\"\"\n",
    "    if 1<=s and s< 4:\n",
    "        q=\"Q1\"\n",
    "    elif 4<=s and s<7:\n",
    "        q=\"Q2\"\n",
    "    elif 7<=s and s<10:\n",
    "        q=\"Q3\"\n",
    "    elif 10<=s and s<=12:\n",
    "        q=\"Q4\"\n",
    "    else:\n",
    "        q=\"no\"\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "quarter_udf = udf(classifyQuarter, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'month'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-255-8c93ea28a077>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbicycle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbicycle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"quarter\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquarter_udf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbicycle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmonth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1642\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1643\u001b[0m             raise AttributeError(\n\u001b[1;32m-> 1644\u001b[1;33m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0m\u001b[0;32m   1645\u001b[0m         \u001b[0mjc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1646\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'month'"
     ]
    }
   ],
   "source": [
    "bicycle=bicycle.withColumn(\"quarter\", quarter_udf(bicycle.month))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "잘 분류되었는지 건수를 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve '`quarter`' given input columns: [대여건수, 대여일자];\n'Aggregate ['quarter], ['quarter, count(1) AS count#6435L]\n+- Relation[대여일자#6381,대여건수#6382] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-256-3165e422a010>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbicycle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'quarter'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\sql\\group.py\u001b[0m in \u001b[0;36m_api\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_api\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mjdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jgd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1305\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1307\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    115\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: cannot resolve '`quarter`' given input columns: [대여건수, 대여일자];\n'Aggregate ['quarter], ['quarter, count(1) AS count#6435L]\n+- Relation[대여일자#6381,대여건수#6382] csv\n"
     ]
    }
   ],
   "source": [
    "bicycle.groupBy('quarter').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|  대여일자|대여건수|\n",
      "+----------+--------+\n",
      "|2018-01-01|    4950|\n",
      "|2018-01-02|    7136|\n",
      "|2018-01-03|    7156|\n",
      "|2018-01-04|    7102|\n",
      "|2018-01-05|    7705|\n",
      "+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bicycle.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 년도별 대여건수 합계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Cannot resolve column name \"count\" among (대여일자, 대여건수)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-258-4f316c8ed34b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbicycle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'year'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"count\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m\"sum\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\sql\\group.py\u001b[0m in \u001b[0;36magg\u001b[1;34m(self, *exprs)\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mexprs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"exprs should not be empty\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexprs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexprs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m             \u001b[0mjdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jgd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexprs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m             \u001b[1;31m# Columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1305\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1307\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    115\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Cannot resolve column name \"count\" among (대여일자, 대여건수)"
     ]
    }
   ],
   "source": [
    "bicycle.groupBy('year').agg({\"count\":\"sum\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 분기별 대여건수 합계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Cannot resolve column name \"count\" among (대여일자, 대여건수)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-259-74d0988a83dc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbicycle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'quarter'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"count\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m\"sum\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\sql\\group.py\u001b[0m in \u001b[0;36magg\u001b[1;34m(self, *exprs)\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mexprs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"exprs should not be empty\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexprs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexprs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m             \u001b[0mjdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jgd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexprs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m             \u001b[1;31m# Columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1305\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1307\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    115\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Cannot resolve column name \"count\" among (대여일자, 대여건수)"
     ]
    }
   ],
   "source": [
    "bicycle.groupBy('quarter').agg({\"count\":\"sum\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Cannot resolve column name \"count\" among (대여일자, 대여건수)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-260-ec9778353088>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbicycle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'quarter'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"count\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m\"avg\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\sql\\group.py\u001b[0m in \u001b[0;36magg\u001b[1;34m(self, *exprs)\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mexprs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"exprs should not be empty\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexprs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexprs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m             \u001b[0mjdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jgd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexprs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m             \u001b[1;31m# Columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1305\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1307\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    115\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Cannot resolve column name \"count\" among (대여일자, 대여건수)"
     ]
    }
   ],
   "source": [
    "bicycle.groupBy('quarter').agg({\"count\":\"avg\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 년도별, 월별 (분기별) 대여건수 합계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve '`month`' given input columns: [대여건수, 대여일자];\n'Project ['month]\n+- Relation[대여일자#6381,대여건수#6382] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-261-9a936f34ae24>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbicycle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'year'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpivot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'month'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"count\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m\"sum\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\sql\\group.py\u001b[0m in \u001b[0;36mpivot\u001b[1;34m(self, pivot_col, values)\u001b[0m\n\u001b[0;32m    257\u001b[0m         \"\"\"\n\u001b[0;32m    258\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalues\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 259\u001b[1;33m             \u001b[0mjgd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jgd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpivot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpivot_col\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    260\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m             \u001b[0mjgd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jgd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpivot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpivot_col\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1305\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1307\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    115\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: cannot resolve '`month`' given input columns: [대여건수, 대여일자];\n'Project ['month]\n+- Relation[대여일자#6381,대여건수#6382] csv\n"
     ]
    }
   ],
   "source": [
    "bicycle.groupBy('year').pivot('month').agg({\"count\":\"sum\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve '`quarter`' given input columns: [대여건수, 대여일자];\n'Project ['quarter]\n+- Relation[대여일자#6381,대여건수#6382] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-262-e45e0681ed23>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbicycle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'year'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpivot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'quarter'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"count\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m\"sum\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\sql\\group.py\u001b[0m in \u001b[0;36mpivot\u001b[1;34m(self, pivot_col, values)\u001b[0m\n\u001b[0;32m    257\u001b[0m         \"\"\"\n\u001b[0;32m    258\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalues\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 259\u001b[1;33m             \u001b[0mjgd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jgd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpivot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpivot_col\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    260\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m             \u001b[0mjgd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jgd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpivot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpivot_col\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1305\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1307\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    115\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: cannot resolve '`quarter`' given input columns: [대여건수, 대여일자];\n'Project ['quarter]\n+- Relation[대여일자#6381,대여건수#6382] csv\n"
     ]
    }
   ],
   "source": [
    "bicycle.groupBy('year').pivot('quarter').agg({\"count\":\"sum\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "bicycleP = bicycle.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas의 info() 함수는 DataFrame의 컬럼, 데이터타입 dtypes를 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 455 entries, 0 to 454\n",
      "Data columns (total 2 columns):\n",
      "대여일자    455 non-null object\n",
      "대여건수    455 non-null int32\n",
      "dtypes: int32(1), object(1)\n",
      "memory usage: 5.4+ KB\n"
     ]
    }
   ],
   "source": [
    "bicycleP.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 년도별 대여건수 합계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'year'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-265-83901dad68c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#bicycleP.groupby('year').aggregate({'Count':np.sum})\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mbicycleP\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'year'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maggregate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'Count'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'sum'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda2\\envs\\py35\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mgroupby\u001b[1;34m(self, by, axis, level, as_index, sort, group_keys, squeeze, observed, **kwargs)\u001b[0m\n\u001b[0;32m   7630\u001b[0m         return groupby(self, by=by, axis=axis, level=level, as_index=as_index,\n\u001b[0;32m   7631\u001b[0m                        \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup_keys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgroup_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msqueeze\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 7632\u001b[1;33m                        observed=observed, **kwargs)\n\u001b[0m\u001b[0;32m   7633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7634\u001b[0m     def asfreq(self, freq, method=None, how=None, normalize=False,\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py35\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py\u001b[0m in \u001b[0;36mgroupby\u001b[1;34m(obj, by, **kwds)\u001b[0m\n\u001b[0;32m   2108\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'invalid type: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2110\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mklass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mby\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda2\\envs\\py35\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, squeeze, observed, **kwargs)\u001b[0m\n\u001b[0;32m    358\u001b[0m                                                     \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m                                                     \u001b[0mobserved\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobserved\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 360\u001b[1;33m                                                     mutated=self.mutated)\n\u001b[0m\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py35\\lib\\site-packages\\pandas\\core\\groupby\\grouper.py\u001b[0m in \u001b[0;36m_get_grouper\u001b[1;34m(obj, key, axis, level, sort, observed, mutated, validate)\u001b[0m\n\u001b[0;32m    576\u001b[0m                 \u001b[0min_axis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgpr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgpr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 578\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    579\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGrouper\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mgpr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkey\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m             \u001b[1;31m# Add key to exclusions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'year'"
     ]
    }
   ],
   "source": [
    "#bicycleP.groupby('year').aggregate({'Count':np.sum})\n",
    "bicycleP.groupby('year').aggregate({'Count':'sum'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 년도별, 월별 대여건수 합계\n",
    "\n",
    "index는 행, columns는 열 데이터를 정의한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Count'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-266-822b81d179b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpivot_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbicycleP\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Count'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'year'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'month'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maggfunc\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;34m'sum'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda2\\envs\\py35\\lib\\site-packages\\pandas\\core\\reshape\\pivot.py\u001b[0m in \u001b[0;36mpivot_table\u001b[1;34m(data, values, index, columns, aggfunc, fill_value, margins, dropna, margins_name)\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[0mto_filter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Count'"
     ]
    }
   ],
   "source": [
    "pd.pivot_table(bicycleP, values = 'Count', index = ['year'], columns = ['month'], aggfunc= 'sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2018년만 선택해서 년도별 x 분기별 대여건수를 출력해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'year'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda2\\envs\\py35\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2656\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2657\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2658\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'year'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-267-ea18de6f18c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbicycleP2018\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbicycleP\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbicycleP\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'year'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m2018\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda2\\envs\\py35\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2925\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2926\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2927\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2928\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2929\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py35\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2657\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2658\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2659\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2660\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2661\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'year'"
     ]
    }
   ],
   "source": [
    "bicycleP2018=bicycleP[bicycleP['year']==2018]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bicycleP2018' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-268-e69fd8dac9e7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbicycleP2018byQ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpivot_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbicycleP2018\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Count'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'year'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'quarter'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maggfunc\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;34m'sum'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'bicycleP2018' is not defined"
     ]
    }
   ],
   "source": [
    "bicycleP2018byQ = pd.pivot_table(bicycleP2018, values = 'Count', index = ['year'], columns = ['quarter'], aggfunc= 'sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bicycleP2018byQ' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-269-525d0df7016a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbicycleP2018byQ\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'bicycleP2018byQ' is not defined"
     ]
    }
   ],
   "source": [
    "bicycleP2018byQ.iloc[:,0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 년별 월별 대여건수 그래프\n",
    "\n",
    "앞장 RDD에서 만들어진 단어빈도는 리스트에 저장되었다. 따라서 리스트에서 데이터를 추출하여 그래프를 그렸다.\n",
    "groupBy에서 생성된 월별 대여건수는 pandas로 변환하여 그려보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 년별 월별 대여건수 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve '`month`' given input columns: [대여건수, 대여일자];\n'Project ['month]\n+- Relation[대여일자#6381,대여건수#6382] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-270-53d170a3d85f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msumMonthly\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbicycle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'year'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpivot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'month'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"count\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m\"sum\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\sql\\group.py\u001b[0m in \u001b[0;36mpivot\u001b[1;34m(self, pivot_col, values)\u001b[0m\n\u001b[0;32m    257\u001b[0m         \"\"\"\n\u001b[0;32m    258\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalues\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 259\u001b[1;33m             \u001b[0mjgd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jgd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpivot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpivot_col\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    260\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m             \u001b[0mjgd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jgd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpivot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpivot_col\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1305\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1307\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    115\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: cannot resolve '`month`' given input columns: [대여건수, 대여일자];\n'Project ['month]\n+- Relation[대여일자#6381,대여건수#6382] csv\n"
     ]
    }
   ],
   "source": [
    "sumMonthly=bicycle.groupBy('year').pivot('month').agg({\"count\":\"sum\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sumMonthly' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-271-565e43930b9e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msumMonthly\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'sumMonthly' is not defined"
     ]
    }
   ],
   "source": [
    "pdf=sumMonthly.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pdf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-272-9750bac3a30b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'pdf' is not defined"
     ]
    }
   ],
   "source": [
    "pdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \n",
    "\n",
    "위 데이터에서 'year' 컬럼이 없어야 그래프를 그릴 수 있다.\n",
    "* drop() 명령어에 삭제할 컬럼명 'year'와 1을 적어준다.\n",
    "0은 행 (index), 1은 컬럼을 삭제한다는 의미이다.\n",
    "* transpose() 함수를 통해 열로 변환하여 (행 데이터는 plot을 할 수 없다), 그래프를 그린다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pdf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-273-4ee16b105462>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'year'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'pdf' is not defined"
     ]
    }
   ],
   "source": [
    "my=pdf.drop('year', 1).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 pdf를 변환한 my를 출력하면, 명령어가 적용되어 year 컬럼이 삭제되고, transpose되어 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'my' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-274-8c42a99f2faf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'my' is not defined"
     ]
    }
   ],
   "source": [
    "my"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```plot()``` 함수는 컬럼을 적지 않으면 모든 컬럼에 대해서 plot한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'my' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-275-b566faf0dcc7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2018\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2019\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'my' is not defined"
     ]
    }
   ],
   "source": [
    "my.columns=[2018, 2019]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'my' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-276-74fd2a4a79d5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'line'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'my' is not defined"
     ]
    }
   ],
   "source": [
    "my.plot(kind='line')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.6 Spark SQL\n",
    "\n",
    "관계형 데이터베이스 RDB에서 사용하는 Sql을 사용하여 DataFrame으로부터 데이터를 조회할 수 있다. DataFrame과 달리, RDD는 비구조적인 경우에 사용하므로 테이블로 변환한 후 Sql을 사용하게 된다.\n",
    "\n",
    "* Spark SQL 구성\n",
    "\n",
    "구분 | 설명\n",
    "-----|-----\n",
    "Language API | Python, Java, Scala, Hive QL API를 제공\n",
    "Schema RDD | RDD에 Schema를 적용해 임시 테이블로 변환한다.<br>createOrReplaceTempView<br>createGlobalTempView\n",
    "Data Sources | 다양한 형식 지원 - HDFS, Cassandra, HBase, RDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "앞서 만들어 놓은 World Cup 데이터를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Competition: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      " |-- Team: string (nullable = true)\n",
      " |-- Number: string (nullable = true)\n",
      " |-- Position: string (nullable = true)\n",
      " |-- FullName: string (nullable = true)\n",
      " |-- Club: string (nullable = true)\n",
      " |-- ClubCountry: string (nullable = true)\n",
      " |-- DateOfBirth: string (nullable = true)\n",
      " |-- IsCaptain: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wcDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "이제 임시 테이블 ```wc```를 만들고, Sql문으로 데이터를 조회해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+----+\n",
      "|                Club|     Team|Year|\n",
      "+--------------------+---------+----+\n",
      "|Club AtlÃ©tico Ta...|Argentina|1930|\n",
      "+--------------------+---------+----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wcDf.createOrReplaceTempView(\"wc\")\n",
    "spark.sql(\"select Club,Team,Year from wc\").show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+---------+----+\n",
      "|    FullName|                Club|     Team|Year|\n",
      "+------------+--------------------+---------+----+\n",
      "|Ãngel Bossio|Club AtlÃ©tico Ta...|Argentina|1930|\n",
      "+------------+--------------------+---------+----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wcPlayers=spark.sql(\"select FullName,Club,Team,Year from wc\")\n",
    "wcPlayers.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='wc', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```wcPlayers```를 RDD로 변환해서 이름만 출력해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full name: Ãngel Bossio\n",
      "Full name: Juan Botasso\n",
      "Full name: Roberto Cherro\n",
      "Full name: Alberto Chividini\n",
      "Full name: \n"
     ]
    }
   ],
   "source": [
    "namesRdd=wcPlayers.rdd.map(lambda x: \"Full name: \"+x[0])\n",
    "for e in namesRdd.take(5):\n",
    "    print (e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### sql.functions and join\n",
    "\n",
    "리스트에 포함되어 있는 과일에 고유번호를 할당해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucketDf=spark.createDataFrame([[1,[\"orange\", \"apple\", \"pineapple\"]],\n",
    "                                [2,[\"watermelon\",\"apple\",\"bananas\"]]],\n",
    "                               [\"bucketId\",\"items\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```truncate```는 행의 값을 잘라내지 않고 출력한다.\n",
    "```show(bucketDf.count(), truncate=False)```는 모든 행을 완전하게 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------------------+\n",
      "|bucketId|items                       |\n",
      "+--------+----------------------------+\n",
      "|1       |[orange, apple, pineapple]  |\n",
      "|2       |[watermelon, apple, bananas]|\n",
      "+--------+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bucketDf.show(bucketDf.count(), truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* explode\n",
    "\n",
    "컬럼에 List 또는 배열이 포함된 경우 ```explode()``` 함수는 이를 flat해서 새로운 컬럼을 생성하게 된다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "bDf=bucketDf.select(bucketDf.bucketId, explode(bucketDf.items).alias('item'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "|bucketId|      item|\n",
      "+--------+----------+\n",
      "|       1|    orange|\n",
      "|       1|     apple|\n",
      "|       1| pineapple|\n",
      "|       2|watermelon|\n",
      "|       2|     apple|\n",
      "|       2|   bananas|\n",
      "+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "또 다른 DataFrame을 생성해보자. 나중에 앞의 DataFrame과 join하게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "fDf=spark.createDataFrame([[\"orange\", \"F1\"],\n",
    "                            [\"\", \"F2\"],\n",
    "                            [\"pineapple\",\"F3\"],\n",
    "                            [\"watermelon\",\"F4\"],\n",
    "                            [\"bananas\",\"F5\"]],\n",
    "                            [\"item\",\"itemId\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|      item|itemId|\n",
      "+----------+------+\n",
      "|    orange|    F1|\n",
      "|          |    F2|\n",
      "| pineapple|    F3|\n",
      "|watermelon|    F4|\n",
      "|   bananas|    F5|\n",
      "+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* join\n",
    "\n",
    "join은 ```inner, cross, outer, full, full_outer, left, left_outer, right, right_outer, left_semi, left_anti``` 여러 종류가 있다. ```inner```기준으로 item이 일치하지 않는 것은 제외하게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "joinDf=fDf.join(bDf, fDf.item==bDf.item, \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------+\n",
      "|itemId|      item|bucketId|\n",
      "+------+----------+--------+\n",
      "|    F5|   bananas|       2|\n",
      "|    F1|    orange|       1|\n",
      "|    F3| pineapple|       1|\n",
      "|    F4|watermelon|       2|\n",
      "+------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinDf.select(fDf.itemId,fDf.item,bDf.bucketId).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 문제 S-1: 네트워크에 불법적으로 침입하는 사용자의 분석\n",
    "\n",
    "### 문제\n",
    "\n",
    "네트워크에 불법적으로 침입하는 시도는 허용되어서는 안된다.\n",
    "1998년 MIT Lincoln Labs에서 DARPA Intrusion Detection Evaluation Program을 연구하였다.\n",
    "이 데이터의 일부가 1999년 KDD로 만들어져 배포되고 있다.\n",
    "https://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html\n",
    "\n",
    "### 해결\n",
    "\n",
    "마지막 행에 attack의 유형이 구분되어 있다. 네트워크 침입 유형의 특징을 분석해 보자.\n",
    "탐지예방 모델을 구축할 수 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "KDD데이터는 41 항목으로 구성되어 있다.\n",
    "\n",
    "```python\n",
    "연결(초) | duration: continuous.\n",
    "프로토콜 (tcp,udp,etc) | protocol_type: symbolic.\n",
    "서비스 (http,telnet, etc) | service: symbolic.\n",
    "flag: symbolic.\n",
    "src_bytes: continuous.\n",
    "dst_bytes: continuous.\n",
    "land: symbolic.\n",
    "wrong_fragment: continuous.\n",
    "urgent: continuous.\n",
    "hot: continuous.\n",
    "num_failed_logins: continuous.\n",
    "logged_in: symbolic.\n",
    "num_compromised: continuous.\n",
    "root_shell: continuous.\n",
    "su_attempted: continuous.\n",
    "num_root: continuous.\n",
    "num_file_creations: continuous.\n",
    "num_shells: continuous.\n",
    "num_access_files: continuous.\n",
    "num_outbound_cmds: continuous.\n",
    "is_host_login: symbolic.\n",
    "is_guest_login: symbolic.\n",
    "count: continuous.\n",
    "srv_count: continuous.\n",
    "serror_rate: continuous.\n",
    "srv_serror_rate: continuous.\n",
    "rerror_rate: continuous.\n",
    "srv_rerror_rate: continuous.\n",
    "same_srv_rate: continuous.\n",
    "diff_srv_rate: continuous.\n",
    "srv_diff_host_rate: continuous.\n",
    "dst_host_count: continuous.\n",
    "dst_host_srv_count: continuous.\n",
    "dst_host_same_srv_rate: continuous.\n",
    "dst_host_diff_srv_rate: continuous|.\n",
    "dst_host_same_src_port_rate: continuous.\n",
    "dst_host_srv_diff_host_rate: continuous.\n",
    "dst_host_serror_rate: continuous.\n",
    "dst_host_srv_serror_rate: continuous.\n",
    "dst_host_rerror_rate: continuous.\n",
    "dst_host_srv_rerror_rate: continuous.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 파일 내려받기\n",
    "\n",
    "KDD 파일은 **gz** 압축되어 있다. 파일 확장자 'gz'은 'gzip'이라는 압축 도구에서 생성된 파일이다. 지금은 WinZip에서 읽을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "_url = 'http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz'\n",
    "_fname = os.path.join(os.getcwd(),'data','kddcup.data_10_percent.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파일이 로컬 디렉토리 ```data```에 존재하면, 즉 이미 내려받았으므로 또 내려받지 않는다. 그렇지 않을 경우에만 ```urlretrieve()``` 함수로 내려받는다. 오류가 발생하면, (1) 파일이 없거나, (2) 파일을 모두 내려 받지 않았거나, (3) 파일이 깨져있을 수 있다. 내려받은 디렉토리로 가서 그 파일이 존재하는지, winzip같은 유틸리티로 해당 gz을 풀어보고 확인하든지, 적당한 에디터로 해당 파일에 내용이 있는지 확인해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zaqxs\\Code\\201910787\\s-master\\data\\kddcup.data_10_percent.gz data does not exist! retrieving..\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlretrieve\n",
    "\n",
    "if(not os.path.exists(_fname)):\n",
    "    print (\"{} data does not exist! retrieving..\".format(_fname))\n",
    "    _f=urlretrieve(_url,_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zaqxs\\Code\\201910787\\s-master\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD 생성\n",
    "\n",
    "**RDD**는 gz와 같은 **압축파일에서 데이터를 읽어서** 생성할 수 있다.\n",
    "\n",
    "반면, DataFrame은 구조schema를 정의해야 하기 때문에 쉽지 않다. 여기서는 **오류**가 발생한다.\n",
    "따라서 RDD를 생성하고 난 후, 그로부터 DataFrame을 생성하고, Sql을 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```textFile()``` 함수로 RDD를 생성한다. ```count()```는 행의 수를 돌려주는 action 함수이다. action 함수는 바로 실행되므로 시간이 좀 걸린다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "_rdd = spark.sparkContext.textFile(_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "494021"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0,tcp,http,SF,181,5450,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,9,9,1.00,0.00,0.11,0.00,0.00,0.00,0.00,0.00,normal.']"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_rdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```map()``` 함수를 사용하여 csv 형식으로 구성된 파일을 컴마(,)로 분리한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "_allRdd=_rdd.map(lambda x: x.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['0',\n",
       "  'tcp',\n",
       "  'http',\n",
       "  'SF',\n",
       "  '181',\n",
       "  '5450',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '1',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '8',\n",
       "  '8',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '1.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '9',\n",
       "  '9',\n",
       "  '1.00',\n",
       "  '0.00',\n",
       "  '0.11',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  'normal.']]"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_allRdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정상, 공격 건수\n",
    "\n",
    "데이터가 ```normal```인 경우와 아닌 경우로 구분하자.\n",
    "```filter()```는 41번째 행을 조건에 따라 데이터를 구분한다.\n",
    "```count()``` 함수로 건수를 계산하면 'normal' 97,278, 'attack'은 396,743 건이다.\n",
    "\n",
    "침입구분 | 건수\n",
    "-------|-------\n",
    "normal | 97278\n",
    "attack | 396743\n",
    "전체 | 494021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "_normalRdd=_allRdd.filter(lambda x: x[41]==\"normal.\")\n",
    "_attackRdd=_allRdd.filter(lambda x: x[41]!=\"normal.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97278"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_normalRdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "396743"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_attackRdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### attack별 건수\n",
    "\n",
    "**attack 종류**는 41번째 열에 구분되어 있다. 총 494,021건을 정상 'noraml'과 나머지는 'attack'으로 구분한다.\n",
    "'attack'은 크게 4종류로 나눈다. DOS는 서비스 거부, R2L 원격침입, U2R은 루트권한침입, probing은 탐지이다.\n",
    "\n",
    "attack 4종류 | 설명 | 41번째 열\n",
    "-----|-----|-----\n",
    "DOS | denial-of-service, e.g. syn flood | back, land, neptune, pod, smurf, teardrop\n",
    "R2L | unauthorized access from a remote machine | ftp_write, guess_passwd, imap, multihop, phf, spy, warezclient, warezmaster\n",
    "U2R | unauthorized access to local superuser (root) privileges | buffer_overflow, loadmodule, perl, rootkit\n",
    "probing | surveillance and other probing | ipsweep, nmap, portsweep, satan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "열41에 대해 건수를 세어보자.\n",
    "```reduceByKey()```는 인자로 '함수'가 필요. 키별로 '함수를 사용해서' 계산한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('normal.', 97278),\n",
       " ('buffer_overflow.', 30),\n",
       " ('loadmodule.', 9),\n",
       " ('perl.', 3),\n",
       " ('neptune.', 107201),\n",
       " ('smurf.', 280790),\n",
       " ('guess_passwd.', 53),\n",
       " ('pod.', 264),\n",
       " ('teardrop.', 979),\n",
       " ('portsweep.', 1040),\n",
       " ('ipsweep.', 1247),\n",
       " ('land.', 21),\n",
       " ('ftp_write.', 8),\n",
       " ('back.', 2203),\n",
       " ('imap.', 12),\n",
       " ('satan.', 1589),\n",
       " ('phf.', 4),\n",
       " ('nmap.', 231),\n",
       " ('multihop.', 7),\n",
       " ('warezmaster.', 20),\n",
       " ('warezclient.', 1020),\n",
       " ('spy.', 2),\n",
       " ('rootkit.', 10)]"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_41 = _allRdd.map(lambda x: (x[41], 1))\n",
    "_41.reduceByKey(lambda x,y: x+y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```groupByKey()```는 키별로 group한다. 위 ```reduceByKey()```와 달리 ```mapValues()```를 사용해 값을 별도로 계산한다는 점에 유의하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('normal.', 97278),\n",
       " ('buffer_overflow.', 30),\n",
       " ('loadmodule.', 9),\n",
       " ('perl.', 3),\n",
       " ('neptune.', 107201),\n",
       " ('smurf.', 280790),\n",
       " ('guess_passwd.', 53),\n",
       " ('pod.', 264),\n",
       " ('teardrop.', 979),\n",
       " ('portsweep.', 1040),\n",
       " ('ipsweep.', 1247),\n",
       " ('land.', 21),\n",
       " ('ftp_write.', 8),\n",
       " ('back.', 2203),\n",
       " ('imap.', 12),\n",
       " ('satan.', 1589),\n",
       " ('phf.', 4),\n",
       " ('nmap.', 231),\n",
       " ('multihop.', 7),\n",
       " ('warezmaster.', 20),\n",
       " ('warezclient.', 1020),\n",
       " ('spy.', 2),\n",
       " ('rootkit.', 10)]"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_41 = _allRdd.map(lambda x: (x[41], 1))\n",
    "def f(x): return len(x)\n",
    "_41.groupByKey().mapValues(f).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Dataframe 생성\n",
    "\n",
    "열 0, 1, 2, 3, 4, 5, 41을 선별하여 스키마를 정해서 RDD를 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "_csv = _rdd.map(lambda l: l.split(\",\"))\n",
    "_csvRdd = _csv.map(lambda p: \n",
    "    Row(\n",
    "        duration=int(p[0]), \n",
    "        protocol=p[1],\n",
    "        service=p[2],\n",
    "        flag=p[3],\n",
    "        src_bytes=int(p[4]),\n",
    "        dst_bytes=int(p[5]),\n",
    "        attack=p[41]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "RDD를 Dataframe으로 변환한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "_df=spark.createDataFrame(_csvRdd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- duration: long (nullable = true)\n",
      " |-- protocol: string (nullable = true)\n",
      " |-- service: string (nullable = true)\n",
      " |-- flag: string (nullable = true)\n",
      " |-- src_bytes: long (nullable = true)\n",
      " |-- dst_bytes: long (nullable = true)\n",
      " |-- attack: string (nullable = true)\n",
      "\n",
      "+--------+--------+-------+----+---------+---------+-------+\n",
      "|duration|protocol|service|flag|src_bytes|dst_bytes| attack|\n",
      "+--------+--------+-------+----+---------+---------+-------+\n",
      "|       0|     tcp|   http|  SF|      181|     5450|normal.|\n",
      "|       0|     tcp|   http|  SF|      239|      486|normal.|\n",
      "|       0|     tcp|   http|  SF|      235|     1337|normal.|\n",
      "|       0|     tcp|   http|  SF|      219|     1337|normal.|\n",
      "|       0|     tcp|   http|  SF|      217|     2032|normal.|\n",
      "+--------+--------+-------+----+---------+---------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_df.printSchema()\n",
    "_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### attack 분류\n",
    "\n",
    "네트워크 침입이 'attack' 또는 'normal'에 따라 구분해서 ```attackB``` 컬럼을 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "attack_udf = udf(lambda x: \"normal\" if x ==\"normal.\" else \"attack\", StringType())\n",
    "myDf=_df.withColumn(\"attackB\", attack_udf(_df.attack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- duration: long (nullable = true)\n",
      " |-- protocol: string (nullable = true)\n",
      " |-- service: string (nullable = true)\n",
      " |-- flag: string (nullable = true)\n",
      " |-- src_bytes: long (nullable = true)\n",
      " |-- dst_bytes: long (nullable = true)\n",
      " |-- attack: string (nullable = true)\n",
      " |-- attackB: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "네트워크 침입 attack을 세분화하여 normal, dos, r2l, u2r, probling으로 **5종류**로 구분한다.\n",
    "구분 문자열이 **점('.')**으로 끝난다는 점에 주의하다.\n",
    "\n",
    "attack 4종류 | 설명 | 41번째 열\n",
    "-----|-----|-----\n",
    "DOS | denial-of-service, e.g. syn flood | back, land, neptune, pod, smurf, teardrop\n",
    "R2L | unauthorized access from a remote machine | ftp_write, guess_passwd, imap, multihop, phf, spy, warezclient, warezmaster\n",
    "U2R | unauthorized access to local superuser (root) privileges | buffer_overflow, loadmodule, perl, rootkit\n",
    "probing | surveillance and other probing | ipsweep, nmap, portsweep, satan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "위 표에 따라 ```udf()``` 함수를 사용해서 if문으로 'noraml' 및 'attack'을 총 5가지 종류로 구분한다.\n",
    "반환 값은 ```StringType()```이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "def classify41(s):\n",
    "    _5=\"\"\n",
    "    if s==\"normal.\":\n",
    "        _5=\"normal\"\n",
    "    elif s==\"back.\" or s==\"land.\" or s==\"neptune.\" or s==\"pod.\" or s==\"smurf.\" or s==\"teardrop.\":\n",
    "        _5=\"dos\"\n",
    "    elif s==\"ftp_write.\" or s==\"guess_passwd.\" or s==\"imap.\" or s==\"multihop.\" or s==\"phf.\" or\\\n",
    "        s==\"spy.\" or s==\"warezclient.\" or s==\"warezmaster.\":\n",
    "        _5=\"r2l\"\n",
    "    elif s==\"buffer_overflow.\" or s==\"loadmodule.\" or s==\"perl.\" or s==\"rootkit.\":\n",
    "        _5=\"u2r\"\n",
    "    elif s==\"ipsweep.\" or s==\"nmap.\" or s==\"portsweep.\" or s==\"satan.\":\n",
    "        _5=\"probing\"\n",
    "    return _5\n",
    "\n",
    "attack5_udf = udf(classify41, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "myDf=myDf.withColumn(\"attack5\", attack5_udf(_df.attack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- duration: long (nullable = true)\n",
      " |-- protocol: string (nullable = true)\n",
      " |-- service: string (nullable = true)\n",
      " |-- flag: string (nullable = true)\n",
      " |-- src_bytes: long (nullable = true)\n",
      " |-- dst_bytes: long (nullable = true)\n",
      " |-- attack: string (nullable = true)\n",
      " |-- attackB: string (nullable = true)\n",
      " |-- attack5: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "잘 분류되었는지 일부 데이터를 살펴보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-------+----+---------+---------+-------+-------+-------+\n",
      "|duration|protocol|service|flag|src_bytes|dst_bytes| attack|attackB|attack5|\n",
      "+--------+--------+-------+----+---------+---------+-------+-------+-------+\n",
      "|       0|     tcp|   http|  SF|      181|     5450|normal.| normal| normal|\n",
      "|       0|     tcp|   http|  SF|      239|      486|normal.| normal| normal|\n",
      "|       0|     tcp|   http|  SF|      235|     1337|normal.| normal| normal|\n",
      "|       0|     tcp|   http|  SF|      219|     1337|normal.| normal| normal|\n",
      "|       0|     tcp|   http|  SF|      217|     2032|normal.| normal| normal|\n",
      "+--------+--------+-------+----+---------+---------+-------+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### attack, normal 특징 분석\n",
    "\n",
    "```attack5``` 별로 건수를 세어보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|attack5| count|\n",
      "+-------+------+\n",
      "|probing|  4107|\n",
      "|    u2r|    52|\n",
      "| normal| 97278|\n",
      "|    r2l|  1126|\n",
      "|    dos|391458|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.groupBy('attack5').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```attack5``` 별로 공격의 특징을 분석해보자. 어떤 ```protocol```, ```src_bytes```, ```duration```이 어떤지 계산할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|protocol| count|\n",
      "+--------+------+\n",
      "|     tcp|190065|\n",
      "|     udp| 20354|\n",
      "|    icmp|283602|\n",
      "+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.groupBy(\"protocol\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+\n",
      "|attackB|protocol| count|\n",
      "+-------+--------+------+\n",
      "| normal|     udp| 19177|\n",
      "| normal|    icmp|  1288|\n",
      "| normal|     tcp| 76813|\n",
      "| attack|    icmp|282314|\n",
      "| attack|     tcp|113252|\n",
      "| attack|     udp|  1177|\n",
      "+-------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.groupBy('attackB','protocol').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+-----+\n",
      "|attackB|  icmp|   tcp|  udp|\n",
      "+-------+------+------+-----+\n",
      "| normal|  1288| 76813|19177|\n",
      "| attack|282314|113252| 1177|\n",
      "+-------+------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.groupBy('attackB').pivot('protocol').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+\n",
      "|attack5|              icmp|               tcp|               udp|\n",
      "+-------+------------------+------------------+------------------+\n",
      "|probing|10.700793650793651| 261454.6003016591|25.235897435897435|\n",
      "|    u2r|              null| 960.8979591836735|13.333333333333334|\n",
      "| normal| 91.47049689440993|1439.3120305156679| 98.01220211711947|\n",
      "|    r2l|              null|271972.57460035523|              null|\n",
      "|    dos| 936.2672084368129| 1090.303422435458|              28.0|\n",
      "+-------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.groupBy('attack5').pivot('protocol').avg('src_bytes').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|attack5|       avg(duration)|\n",
      "+-------+--------------------+\n",
      "|probing|   485.0299488677867|\n",
      "|    u2r|    80.9423076923077|\n",
      "| normal|  216.65732231336992|\n",
      "|    r2l|   559.7522202486679|\n",
      "|    dos|7.254929008986916E-4|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.groupBy('attack5').avg('duration').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-------+---+\n",
      "|attackB|icmp|    tcp|udp|\n",
      "+-------+----+-------+---+\n",
      "| normal|   0|5134218|516|\n",
      "| attack|   0|5155468| 74|\n",
      "+-------+----+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "myDf.groupBy('attackB').pivot('protocol').agg(F.max('dst_bytes')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "좀 더 세밀한 조건으로 ```duration>1000)```, ```dst_bytes==0```인 경우의 건수를 계산할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|protocol|count|\n",
      "+--------+-----+\n",
      "|     tcp|  139|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.select(\"protocol\", \"duration\", \"dst_bytes\")\\\n",
    "    .filter(_df.duration>1000)\\\n",
    "    .filter(_df.dst_bytes==0)\\\n",
    "    .groupBy(\"protocol\")\\\n",
    "    .count()\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### SQL\n",
    "\n",
    "SQL을 사용해보자. 위에 사용했던 ```_df```에서 임시 테이블 ```_tab```을 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df.registerTempTable(\"_tab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "tcp_interactions = spark.sql(\n",
    "\"\"\"\n",
    "    SELECT duration, dst_bytes FROM _tab\n",
    "    WHERE protocol = 'tcp' AND duration > 1000 AND dst_bytes = 0\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|duration|dst_bytes|\n",
      "+--------+---------+\n",
      "|    5057|        0|\n",
      "|    5059|        0|\n",
      "|    5051|        0|\n",
      "|    5056|        0|\n",
      "|    5051|        0|\n",
      "+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tcp_interactions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "tcp_interactions_out = tcp_interactions.rdd\\\n",
    "    .map(lambda p: \"Duration: {}, Dest. bytes: {}\".format(p.duration, p.dst_bytes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print'. Did you mean print(ti_out)? (<ipython-input-361-a9ffbd19a28f>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-361-a9ffbd19a28f>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    print ti_out\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m Missing parentheses in call to 'print'. Did you mean print(ti_out)?\n"
     ]
    }
   ],
   "source": [
    "for i,ti_out in enumerate(tcp_interactions_out.collect()):\n",
    "    if(i%10==0):\n",
    "        print ti_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 문제 S-2: Twitter JSON 데이터 읽기\n",
    "\n",
    "* [nok] 현재 디렉토리 _tweet.json\n",
    "    * src/ds_twitter_3.py로 변경 (ds_twitter_3.json으로 저장)\n",
    "\n",
    "\n",
    "\n",
    "* Twitter JSON을 읽을 경우\n",
    "\n",
    "구분 | 예\n",
    "-------|-------\n",
    "unicode를 사용하면 backslash | \"{\\\"created_at\\\":\\\"Sun Nov 13 00:05:19 +0000 2016\\\"\n",
    "보통 | {\"created_at\":\"Sun Nov 13 00:05:19 +0000 2016\"\n",
    "\n",
    "* allowBackslashEscapingAnyCharacter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 문제 S-3: 뉴욕에서 출생한 신생아 분석\n",
    "\n",
    "### 뉴욕에서 출생한 신생아가 년도별 성별에 차이가 있을까?\n",
    "\n",
    "뉴욕에서 2007년 출생한 유아의 기록이다.\n",
    "https://health.data.ny.gov/Health/Baby-Names-Beginning-2007/jxy9-yhdk\n",
    "\n",
    "Column Name | 설명\n",
    "-----|-----\n",
    "Year | Year data was collected.\n",
    "First Name | 이름\n",
    "County | Location where the baby’s mother resided as stated on their birth certificate.\n",
    "Sex | F= Female M= Male\n",
    "Count | Five (5) or more of the same baby name in a county outside of NYC; Ten (10) or more of the same baby name in a NYC borough.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 문제 S-4: 우버 택시의 운행기록 분석\n",
    "\n",
    "* 질문: 2015년 가장 많은 운행을 한 base는?\n",
    "https://github.com/tmcgrath/spark-with-python-course/blob/master/Spark-SQL-CSV-with-Python.ipynb\n",
    "\n",
    "* fivethirtyeight\n",
    "    * git clone https://github.com/fivethirtyeight/uber-tlc-foil-response.git\n",
    "        daily Uber trip statistics in January and February 2015\n",
    "\n",
    "dispatching_base_number | date | active_vehicles | trips\n",
    "----------|----------|----------|----------\n",
    "B02512 | 1/1/2015 | 190 | 1132\n",
    "B02765 | 1/1/2015 | 225 | 1765\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 문제 S-5: JDBC를 사용해서 데이터 읽기\n",
    "\n",
    "* jdbc를 연결하는 방식은 Java와 같이 'driver', 'url'을 설정하면 된다.\n",
    "* 여기서는 sqlite를 실습한다.\n",
    "\n",
    "* sqlite와 같이 Spark 패키지가 없는 경우, jar를 다운로드하고\n",
    "설정파일 conf/spark-defaults.conf에 'spark.driver.extraClassPath'를 추가한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.7 MongoDB Spark connector\n",
    "\n",
    "* Spark에서 MongoDB에 저장된 데이터를 읽어 온다.\n",
    "* 참조: pymongo-spark (Spark와 PyMongo를 사용하는 Python 라이브러리, 설치하려면 pip install pymongo-spark)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.7.1 설정\n",
    "\n",
    "* 참조 https://docs.mongodb.com/spark-connector/\n",
    "* 설정파일 conf/spark-defaults.conf 수정\n",
    "    * Spark 버전에 맞는 jar를 선택한다.\n",
    "    * MongoDB<3.2인 경우, spark.mongodb.input.partitioner가 필요하다.\n",
    "    * packages 여러 개를 넣을 경우에는 컴마로 분리한다.\n",
    "\n",
    "```python\n",
    "$vim conf/spark-defaults.conf \n",
    "spark.jars.packages=org.mongodb.spark:mongo-spark-connector_2.10:1.1.0\n",
    "spark.mongodb.input.partitioner=MongoPaginateBySizePartitioner\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print spark.conf.get('spark.jars.packages')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### S.7.2 uri\n",
    "\n",
    "* SparkSession에 uri를 설정할 수 있다. 연결에 필요한 ip, database, collection을 정의한다.\n",
    "\n",
    "```python\n",
    "spark = pyspark.sql.SparkSession.builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"myApp\")\\\n",
    "    .config(\"spark.mongodb.input.uri\", \"mongodb://127.0.0.1/myDB.ds_spark_df_mongo\") \\\n",
    "    .config(\"spark.mongodb.output.uri\", \"mongodb://127.0.0.1/myDB.ds_spark_df_mongo\") \\\n",
    "    .getOrCreate()\n",
    "```\n",
    "\n",
    "* 또는 실행시점에 설정할 수 있다 (아래 참조)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.7.3 MongoDB Python API\n",
    "\n",
    "* format은 \"com.mongodb.spark.sql.DefaultSource\"로 설정한다.\n",
    "* 'option'을 사용해서 실행시점에 Database, Colleciton 명을 설정할 수 있다.\n",
    "\n",
    "구분 | 명령어 예\n",
    "-----|-----\n",
    "쓰기 | DataFrame.write.format(\"com.mongodb.spark.sql.DefaultSource\")\\<br>.mode(\"overwrite\")\\<br>.option(\"uri\",\"mongodb://127.0.0.1/myDB.ds_spark_ml\")\\<br>.save()\n",
    "읽기 | spark.read.format(\"com.mongodb.spark.sql.DefaultSource\")\\<br>.option(\"uri\",\"mongodb://127.0.0.1/ds_twitter.seoul\")\\<br>.load()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### S.7.4 연습으로 쓰기, 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "people = spark.createDataFrame([(\"kim\",10),(\"lee\",20),(\"choi\",30),(\"park\",40)],[\"name\", \"age\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.write.format(\"com.mongodb.spark.sql.DefaultSource\")\\\n",
    "    .mode(\"append\")\\\n",
    "    .option(\"uri\",\"mongodb://127.0.0.1/myDB.ds_spark_ml\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\")\\\n",
    "    .option(\"uri\",\"mongodb://127.0.0.1/myDB.ds_spark_ml\")\\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df.select('name').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\")\\\n",
    "    .option(\"uri\",\"mongodb://127.0.0.1/ds_twitter.seoul\")\\\n",
    "    .load()\n",
    "df.select('text').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.8 spark-submit\n",
    "\n",
    "* spark-submit는 일괄실행 (self-contained app in quick-start 참조)\n",
    "\n",
    "* MongoDB를 사용하려면, spark-defaults.conf에 jar를 추가한다 (앞서 미리 설정하였다.)\n",
    "\n",
    "* spark-submit을 실행하기 전, 'conf/log4j.properties'를 수정 log level을 ERROR로 설정하였다.\n",
    "```python\n",
    "log4j.rootCategory=ERROR, console\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### S.8.1 간단한 작업\n",
    "\n",
    "* DataFrame 만들고, 출력하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/ds_spark_sql.py\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: UTF-8 -*-\n",
    "import pyspark\n",
    "\n",
    "def doIt():\n",
    "    d = [{'name': 'Alice', 'age': 1}]\n",
    "    print spark.createDataFrame(d).collect()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    myConf=pyspark.SparkConf()\n",
    "    spark = pyspark.sql.SparkSession.builder\\\n",
    "        .master(\"local\")\\\n",
    "        .appName(\"myApp\")\\\n",
    "        .config(conf=myConf)\\\n",
    "        .getOrCreate()\n",
    "    doIt()\n",
    "    spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "!/home/jsl/Downloads/spark-2.0.0-bin-hadoop2.7/bin/spark-submit src/ds_spark_sql.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.8.2 MongoDB\n",
    "\n",
    "* Database, Collection 읽기, 쓰기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile src/ds_spark_mongo.py\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: UTF-8 -*-\n",
    "import pyspark\n",
    "def doIt():\n",
    "    print \"---------RESULT-----------\"\n",
    "    print \"------mongodb write-------\"\n",
    "    myRdd = spark.sparkContext.parallelize([\n",
    "        (\"js\", 150),\n",
    "        (\"Gandalf\", 1000),\n",
    "        (\"Thorin\", 195),\n",
    "        (\"Balin\", 178),\n",
    "        (\"Kili\", 77),\n",
    "        (\"Dwalin\", 169),\n",
    "        (\"Oin\", 167),\n",
    "        (\"Gloin\", 158),\n",
    "        (\"Fili\", 82),\n",
    "        (\"Bombur\", None)\n",
    "    ])\n",
    "    myDf = spark.createDataFrame(myRdd, [\"name\", \"age\"])\n",
    "    print myDf\n",
    "    myDf.write.format(\"com.mongodb.spark.sql.DefaultSource\").mode(\"overwrite\").save()\n",
    "    print \"---------read-----------\"\n",
    "    df = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").load()\n",
    "    print df.printSchema()\n",
    "    df.registerTempTable(\"myTable\")\n",
    "    myTab = spark.sql(\"SELECT name, age FROM myTable WHERE age >= 100\")\n",
    "    myTab.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    myConf=pyspark.SparkConf()\n",
    "    spark = pyspark.sql.SparkSession.builder\\\n",
    "        .master(\"local\")\\\n",
    "        .appName(\"myApp\")\\\n",
    "        .config(\"spark.mongodb.input.uri\", \"mongodb://127.0.0.1/myDB.ds_spark_df_mongo\") \\\n",
    "        .config(\"spark.mongodb.output.uri\", \"mongodb://127.0.0.1/myDB.ds_spark_df_mongo\") \\\n",
    "        .getOrCreate()\n",
    "    doIt()\n",
    "    spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "!/home/jsl/Downloads/spark-2.0.0-bin-hadoop2.7/bin/spark-submit src/ds_spark_mongo.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 문제 S-6: MongoDB 저장된 열린데이터 읽어오는 spark-submit\n",
    "\n",
    "* MongoDB에 저장된 데이터 읽기\n",
    "\n",
    "구분 | 명\n",
    "-----|-----\n",
    "Database | ds_open_subwayPassengersDb\n",
    "Collection | db_open_subwayTable\n",
    "key | JSON 계층구조를 따라 읽는다. CardSubwayStatisticsService.row.RIDE_PASGR_NUM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
