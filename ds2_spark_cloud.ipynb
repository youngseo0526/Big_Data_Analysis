{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Cloud in Databricks\n",
    "\n",
    "Last updated 20200907MON1000 20180430MON1230\n",
    "\n",
    "## 목적\n",
    "Databricks Spark 사용할 수 있다.\n",
    "\n",
    "## 목차\n",
    "\n",
    "* 1. 노트북: 노트북 생성, 열기, 사용\n",
    "* 2. 파일시스템: fs, dbfs dbutils 명령어 (put, ls, ...), 라이브러리 설치\n",
    "* 3. 파일: 메뉴사용해서 올리기, dbutils를 사용하여 읽기, 쓰기, 삭제\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 소개\n",
    "\n",
    "Spark는 클라우드 플랫폼으로 제공되고 있어서, 자신의 컴퓨터에 직접 설치하지 않고 사용할 수 있다. Databricks, Google Dataproc, 마이크로소프트 Azure에서 제공하고 있다. 여기서는 데이터브릭스에서 온라인으로 제공하는 https://community.cloud.databricks.com 를 설명한다.\n",
    "Databricks Community Edition은 **Amazon Web Services**에서 실행되지만, 사용자에게 비용이 부과되지는 않는다. 용량이 더 필요하거나, 실제 제품을 만드려면 community판에서 업그레이드해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 노트북\n",
    "\n",
    "### Workspace\n",
    "로그인하면 **Workspace**가 주어진다. Workspace는 자신의 작업공간으로 **루트 폴더**와 같다. Notebook을 저장하거나 라이브러리를 설치하거나 데이터를 저장할 수 있다. 또한 일반적으로 자신의 컴퓨터에서 할 수 있는 복사, 이동 등을 할 수 있다.\n",
    "\n",
    "### 클러스터 생성\n",
    "Notebook을 사용하려면 클러스터를 가동해야 한다.\n",
    "맨 좌측 메뉴에서 **Clusters**를 선택하고, 생성된 클러스터가 있는지 확인하자. 처음 사용하는 것이라면 당연히 없겠다.\n",
    "**+ Create Cluster** 버튼을 누르고:\n",
    "- Cluster Name에는 적당한 이름, 자신의 영문 이니셜을 입력해도 된다.\n",
    "- Databricks Runtime Version은 원하는 버전을 골라서 선택하면 되지만, 현재 보여지는 것을 그냥 두어도 된다.\n",
    "\n",
    "* 주의: 무료 사용 (Community Edition)은 2시간 아무 작업도 하지 않으면 클러스터가 종료된다 (your cluster will automatically terminate after an idle period of two hours.)\n",
    "\n",
    "### 노트북 생성\n",
    "노트북을 사용하려면, 맨 좌측 메뉴에서 **Workspace** 아이콘을 클릭하면 우측으로 열리는 메뉴 **아래 화살표**를 누른다.\n",
    "처음 노트북을 생성하려면, **Workspace** 아이콘을 클릭하면 열리는 메뉴판에서 자신의 로그인 이메일 옆의 **화살표 'v'**를 누르면 드롭다운 메뉴가 나타난다. Create > Notebook를 선택하면 생성할 수 있다.\n",
    "- Name은 적당한 이름, 가급적 영어와 숫자로 구성하여 입력한다. 확장자 ipynb는 입력하지 않는다 (예: s1_rdd)\n",
    "- Default Language는 Python으로 하자.\n",
    "- Cluster는 방금 생성한 클러스터를 고른다.\n",
    "\n",
    "### 노트북 열기\n",
    "생성할 때와 같이 **Workspace** 아이콘을 클릭하면 열리는 메뉴판에서 자신의 로그인 이메일, 이번에는 아래에 생성했던 노트북 파일을 클릭한다.\n",
    "열리면 맨 위에 클러스터를 Detached되어 있다면, 생성해 놓았던 클러스터를 선택하면 된다.\n",
    "\n",
    "### 노트북 사용\n",
    "\n",
    "#### Spark 사용\n",
    "보통 Spark 인스턴스를 만드는 것과 같이 생성해서 사용할 수 있다.\n",
    "또는 Databricks에서 미리 친절하게 생성해서 제공하는 아래와 같은 Spark 변수가 있으니, 그냥 사용해도 된다.\n",
    "\n",
    "미리 생성된 변수들 | 클래스\n",
    "-----|-----\n",
    "spark | SparkSession, 모든 컨텍스트의 통합 세션\n",
    "sc | SparkContext\n",
    "sqlContext | SQLContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### shell 명령어\n",
    "노트북 shell 명령어는 우리가 보통 Notebook에서 사용하는 것과 크게 다르지 않다.\n",
    "```%```를 붙여서 사용하면 된다.\n",
    "\n",
    "셀 명령어 | 설명\n",
    "-----|-----\n",
    "%md | markdown을 쓸 경우\n",
    "%sh | shell\n",
    "%fs | DBFS Databricks file system\n",
    "%scala, %python, %r, and %sql | 다른 언어 지원\n",
    "\n",
    "### 노트북 가져나가기\n",
    "\n",
    "상단 메뉴에서 File > Export > IPython Notebook을 클릭하면, 현재 작성중인 파일이 로컬에 저장된다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. File System\n",
    "\n",
    "* 1) 내 계정의 로컬파일은 file:/로 시작한다. 보통 리눅스와 같아서, 사용자 계정 /home/ubuntu/를 제공한다.\n",
    "* 2) 분산파일 (dbfs:/)이 있다. dbfs 약어는 Databricks File System의 줄임말로 자체적으로 제공하는 파일 시스템이다.\n",
    "\n",
    "파일관련 명령어는 리눅스와 거의 동일하다. 도움말은 dbutils.fs.help() 명령으로 볼 수 있다.\n",
    "\n",
    "### 2.1 cli\n",
    "github 오픈소스 Databricks command-line interface (CLI) 설치해서 사용한다.\n",
    "리눅스에서 사용하는 명령어와 비슷하다.\n",
    "\n",
    "```\n",
    "dbfs ls\n",
    "dbfs cp ./apple.txt dbfs:/apple.txt\n",
    "dbfs cp -r ./banana dbfs:/banana\n",
    "```\n",
    "\n",
    "### 2.2 %fs\n",
    "\n",
    "셀명령어 ```%fs```를 사용해서 해당 셀에 다음과 같이 파일을 읽거나, 복사하거나 할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%fs ls /\n",
    "\n",
    "%fs ls /FileStore/tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 dbutils\n",
    "\n",
    "DBFS Utils 사용 예: 디렉토리 생성, put, head, rm 등 명령어를 사용할 수 있다.\n",
    "* dbfs:/는 dbfs경로를 접근할 경우 사용한다.\n",
    "* file:/는 자신의 databricks 계정으로 할당이 된 로컬 파일을 접근할 경우 사용한다.\n",
    "* %fs로 줄여쓸 수 있다.\n",
    "* 도움말 dbutils.fs.help()\n",
    "\n",
    "```\n",
    "dbutils.fs.mkdirs(\"/foobar/\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dbutils.fs.ls('dbfs:/FileStore/tables')  # list b'dbfs:/FileStore/tables'\n",
    "\n",
    "dbutils.fs.ls(\"/data/\")\n",
    "\n",
    "dbutils.fs.ls(\"/\") 또는 'dbfs:' 넣어서 dbutils.fs.ls(\"dbfs:/\")\n",
    "\n",
    "dbutils.fs.ls(\"file:/home/ubuntu/databricks\") # 'file:' lists local files (not mine!)\n",
    "\n",
    "dbfs cp ./apple.txt dbfs:/apple.txt # Put local file ./apple.txt to dbfs:/apple.txt\n",
    "\n",
    "dbfs cp /apple.txt ./apple.txt # Get dbfs:/apple.txt and save to local file ./apple.txt\n",
    "\n",
    "dbfs cp -r ./banana /banana # Recursively put local dir ./banana to dbfs:/banana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 라이브러리 설치\n",
    "\n",
    "웬만한 라이브러리는 설치되어 있다.\n",
    "다음이 설치되어 있는지 라이브러를 불러와 보자.\n",
    "\n",
    "```\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "```\n",
    "\n",
    "#### 메뉴에서 라이브러리 설치\n",
    "\n",
    "Databriks Runtime version 5.1이상에서 library설치 기능이 보완되었다.\n",
    "**Workspace** 아이콘을 누른 후, Create, Library를 클릭한다.\n",
    "Python 라이브러리를 설치하려면 PyPI를 누르고, 라이브러리 명을 타이핑하고 Create를 누르면 설치가 된다.\n",
    "PyPI에서 simplejson을 설치하고 테스트 해보자.\n",
    "\n",
    "```\n",
    "import simplejson\n",
    "```\n",
    "\n",
    "#### dbutils 명령어로 라이브러리 설치\n",
    "\n",
    "또는 그냥 노트북 셀에서 dbutils 명령어로 설치할 수 있다.\n",
    "\n",
    "* 1) 처음에 'scipy'이 설치되었는지 확인하자. 버전을 확인하면 제공하는 버전은 '1.4.1'\n",
    "\n",
    "```\n",
    "%python\n",
    "import pkg_resources\n",
    "pkg_resources.get_distribution('scipy').version\n",
    "\n",
    "Out[1]: '1.4.1'\n",
    "```\n",
    "\n",
    "* 2) 위 '1.4.1'버전은 기본으로 제공되는 버전이고, 내가 사용하는 노트북에서 설치된 라이브러리를 확인해보자.\n",
    "지금은 처음이니 아무 것도 없을 것이다.\n",
    "\n",
    "```\n",
    "dbutils.library.list()\n",
    "\n",
    "Out[2]: []\n",
    "```\n",
    "\n",
    "* 3) 설치하려는 버전 'scipy' '1.5.0'을 설치하고, 재시작 restartPython()하면 1.5.0이 설치된 것을 확인할 수 있다.\n",
    "\n",
    "```\n",
    "dbutils.library.installPyPI('scipy','1.5.0')\n",
    "dbutils.library.restartPython()\n",
    "dbutils.library.list()\n",
    "\n",
    "Out[3]: ['scipy==1.5.0']\n",
    "```\n",
    "\n",
    "* 4) 설치된 scipy의 버전이 맞는지 import scipy해서 확인해보자.\n",
    "\n",
    "```\n",
    "%python\n",
    "import scipy\n",
    "import pkg_resources\n",
    "pkg_resources.get_distribution('scipy').version\n",
    "\n",
    "Out[1]: '1.5.0'\n",
    "```\n",
    "\n",
    "#### 해당 노트북으로 제한된 라이브러리\n",
    "\n",
    "pip 명령어를 사용한다.\n",
    "```python\n",
    "%pip install matplotlib\n",
    "%pip uninstall -y matplotlib\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 데이터\n",
    "\n",
    "데이터베이스는 테이블로 구성되며, 테이블은 구조적 데이터로 구성된다.\n",
    "테이블은 크게 나누면:\n",
    "* 전역 테이블 Global tables은 모든 클러스터에서 사용할 수 있는 파일이고\n",
    "* 지역 테이블 Local tables은 한 클러스터에서만 사용할 수 있는 파일을 말한다.\n",
    "테이블은 파일을 올리는 메뉴를 이용하거나, 프로그램으로 파일을 올리면 만들어진다.\n",
    "\n",
    "## 3.1 메뉴를 사용해서 파일 올리기\n",
    "\n",
    "좌측 메뉴에서 **Data** 아이콘을 클릭하면 메뉴가 열린다. 우측 상단에 ```Add Data```를 하면 ```Create New Table```이 뜬다.\n",
    "\n",
    "* 1) 아래 Upload File, S3, DBFS, Other Data Sources메뉴가 있다.\n",
    "DBFS에 파일을 올리거나, 현재 올려진 파일 목록을 볼 수 있다.\n",
    "\n",
    "* 2) 또는 /FileStore/tables/ 아래에 'Select' 버튼으로 선택하거나, 마우스 Drop으로 파일을 올릴 수 있다.\n",
    "그러면 ```/FileStore/tables/<filename>-<random-number>.<file-type>``` 명칭으로 생성된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 dbutils를 사용하여 읽고, 쓰거나, 삭제\n",
    "\n",
    "1) fs.put() 또는 2) 보통 파일 쓰는 방식으로 쓴다.\n",
    "\n",
    "```\n",
    "jstxt=\"\"\"Wikipedia\n",
    "Apache Spark is an open source cluster computing framework.\n",
    "아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다.\n",
    "Apache Spark Apache Spark Apache Spark Apache Spark\n",
    "아파치 스파크 아파치 스파크 아파치 스파크 아파치 스파크\n",
    "Originally developed at the University of California, Berkeley's AMPLab,\n",
    "the Spark codebase was later donated to the Apache Software Foundation,\n",
    "which has maintained it since.\n",
    "Spark provides an interface for programming entire clusters with\n",
    "implicit data parallelism and fault-tolerance.\"\"\"\n",
    "\n",
    "dbutils.fs.put(\"/data/ds_spark_wiki.txt\", jstxt)\n",
    "```\n",
    "\n",
    "방금 쓰인 파일을 확인할 수 있다.\n",
    "```\n",
    "dbutils.fs.ls(\"/data/\")\n",
    "```\n",
    "\n",
    "파일을 삭제할 때는 rm 명령어를 사용한다.\n",
    "\n",
    "```\n",
    "dbutils.fs.rm('/data/ds_spark_wiki.txt')\n",
    "```\n",
    "\n",
    "파일을 읽고 쓸 때는 로컬파일과 분별하기 위해 앞에 '/dbfs'를 붙여주어야 한다.\n",
    "```\n",
    "dbutils.fs.ls(\"/FileStore/tables/\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 프로그램으로 파일 생성\n",
    "\n",
    "spark 또는 SQL 명령어로 테이블을 생성할 수 있다.\n",
    "아래 명령어는 DataFrame을 테이블로 생성하고 있다.\n",
    "주의할 점은 파일이 생성되는 폴더가 /usr/hive/warehouse 디렉토리 안에 default 데이터베이스의 테이블로 생성된다.\n",
    "그 디렉토리로 가서 찾아보자.\n",
    "그 디렉토리는 'Create New Table' > DBFS > user > hive > warehouse를 클릭하면 거기서 찾을 수 있다.\n",
    "```python\n",
    "df=spark.read.csv('/FileStore/tables/tmp.txt')\n",
    "df.write.saveAsTable(\"tmpDf\")\n",
    "dbutils.fs.ls('/user/hive/warehouse')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### local file\n",
    "\n",
    "자신이 만든 영역, 즉 로컬파일을 읽을 수 있다.\n",
    "/dbfs/라고 한다. dbfs:/라고 하지 않는다.\n",
    "아래 명령어는 파일을 찾을 수 없다고 ```FileNotFoundError``` 오류가 발생하고 있다. 현재 버전에서는 Python에서 파일을 읽을 때 경로에 문제가 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"/dbfs/data/ds_spark_wiki.txt\", \"r\") as f_read:\n",
    "  for line in f_read:\n",
    "    print line"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
